{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "52980aaae5424035bbb7887060469d91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aac4700733bb4a9889bf3938438abcb7",
              "IPY_MODEL_5eef4ab1c4a142c683180bc8247d04e8"
            ],
            "layout": "IPY_MODEL_25abe19eb71e4e3ca0eb7c3407d3c3f2"
          }
        },
        "aac4700733bb4a9889bf3938438abcb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b8a447367c0416ca54d46ffc09a1114",
            "placeholder": "​",
            "style": "IPY_MODEL_a0c85e8d7f0d4dc794bd1f6da0680ffc",
            "value": "0.001 MB of 0.020 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "5eef4ab1c4a142c683180bc8247d04e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7c664d4182c4a359dadbd3eb8690eb1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7524e69f4bee4e7ca97e4a683d4006cd",
            "value": 0.03542414509729832
          }
        },
        "25abe19eb71e4e3ca0eb7c3407d3c3f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b8a447367c0416ca54d46ffc09a1114": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0c85e8d7f0d4dc794bd1f6da0680ffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7c664d4182c4a359dadbd3eb8690eb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7524e69f4bee4e7ca97e4a683d4006cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8620b6163c56445ca1d5132b6961f5eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_446f76996f0a4f6a92f3dbfe55e236ef",
              "IPY_MODEL_304c0e54dd874822b5bd2448d08f0eca"
            ],
            "layout": "IPY_MODEL_07ff86770ec547559a5866332c22dfc4"
          }
        },
        "446f76996f0a4f6a92f3dbfe55e236ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32e125cf25004649a1de15ee0bc1f373",
            "placeholder": "​",
            "style": "IPY_MODEL_47d4ec9325e147089341c2406ae371f5",
            "value": "0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "304c0e54dd874822b5bd2448d08f0eca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c0518d98f95415b9dafea13089da73d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62617cf2e0344885a4a2b0a832664155",
            "value": 1
          }
        },
        "07ff86770ec547559a5866332c22dfc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32e125cf25004649a1de15ee0bc1f373": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47d4ec9325e147089341c2406ae371f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c0518d98f95415b9dafea13089da73d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62617cf2e0344885a4a2b0a832664155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "528e67742d5443b989b39506b837797f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0135e7827aed48ec8a71bb40706eaa54",
              "IPY_MODEL_218349f8ee504c2ba623e989688cf6a2"
            ],
            "layout": "IPY_MODEL_a9c2e43fa04a4177bc0e3d1ea1672a35"
          }
        },
        "0135e7827aed48ec8a71bb40706eaa54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e02982aa2c3e45a3a2c48f328ba0d7dc",
            "placeholder": "​",
            "style": "IPY_MODEL_f19a9eca41964c3e98b4c84a0c0368a5",
            "value": "0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "218349f8ee504c2ba623e989688cf6a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81ae052ab12246fd8bcf78bf2462e053",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd0158a235284fd780904c1b1de1a530",
            "value": 1
          }
        },
        "a9c2e43fa04a4177bc0e3d1ea1672a35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e02982aa2c3e45a3a2c48f328ba0d7dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f19a9eca41964c3e98b4c84a0c0368a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81ae052ab12246fd8bcf78bf2462e053": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd0158a235284fd780904c1b1de1a530": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3db9fdbc3f2642e19a2873b532c38b97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_78472df39aca4073bb8470a50b281f56",
              "IPY_MODEL_1e4641640ae24fbaa25bad7b0bcc75c2"
            ],
            "layout": "IPY_MODEL_170c6c1677ca4089bdf940a1bc401e61"
          }
        },
        "78472df39aca4073bb8470a50b281f56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1f38496bd844f9a90c546f9716a0516",
            "placeholder": "​",
            "style": "IPY_MODEL_91a7efb2c93a4d4aae743cfbf12ef25f",
            "value": "0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "1e4641640ae24fbaa25bad7b0bcc75c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f98d1572bf6d4836b69f81e9e0f36d39",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6dc470a9b444f279db138b595d312e2",
            "value": 1
          }
        },
        "170c6c1677ca4089bdf940a1bc401e61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1f38496bd844f9a90c546f9716a0516": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91a7efb2c93a4d4aae743cfbf12ef25f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f98d1572bf6d4836b69f81e9e0f36d39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6dc470a9b444f279db138b595d312e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd6fe0822e3140619ab0b764e3036f90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7bad6c2d5ef14853b90722c06619c259",
              "IPY_MODEL_b344a200c4fb43d494cad8f07ab7435e"
            ],
            "layout": "IPY_MODEL_53d40e506baf49e1960e3ff62acb08f5"
          }
        },
        "7bad6c2d5ef14853b90722c06619c259": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df417aba6c964d9e8f10ef32da63e312",
            "placeholder": "​",
            "style": "IPY_MODEL_76ad62dc78624a6f877ff2f039eef6e8",
            "value": "0.001 MB of 0.018 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "b344a200c4fb43d494cad8f07ab7435e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ae25e27052040d6ae4c495b63884e94",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e14ac8c3d4b74a99a571a16750b1ef7d",
            "value": 0.038816334991708126
          }
        },
        "53d40e506baf49e1960e3ff62acb08f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df417aba6c964d9e8f10ef32da63e312": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76ad62dc78624a6f877ff2f039eef6e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ae25e27052040d6ae4c495b63884e94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e14ac8c3d4b74a99a571a16750b1ef7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91a7fd04776047fdadaf43a2035d41bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7084213d2e584175944c5c58f9106d51",
              "IPY_MODEL_20b8b00663ba497981cf547caddf9aca"
            ],
            "layout": "IPY_MODEL_f587296a53e1411cbc7861e742fad411"
          }
        },
        "7084213d2e584175944c5c58f9106d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d113ccc2c969425e96ae393facecba9c",
            "placeholder": "​",
            "style": "IPY_MODEL_4820eb2db866495a9b6733e4f4c22129",
            "value": "0.019 MB of 0.019 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "20b8b00663ba497981cf547caddf9aca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b148d23d8b549d8bae2d4aeb379a6c7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbacabf125fe438ba3d39e9fed4a5741",
            "value": 1
          }
        },
        "f587296a53e1411cbc7861e742fad411": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d113ccc2c969425e96ae393facecba9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4820eb2db866495a9b6733e4f4c22129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b148d23d8b549d8bae2d4aeb379a6c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbacabf125fe438ba3d39e9fed4a5741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f2d07d782b0946e8899663778ffdfa86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0fcca7e8a424310bcc1b1f02362fdf9",
              "IPY_MODEL_b6f5a379432f470c9ee9104bea69219d"
            ],
            "layout": "IPY_MODEL_c02b275c9eb24697a2d53c227449cb30"
          }
        },
        "c0fcca7e8a424310bcc1b1f02362fdf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63c97284fc4e44efbb20161fcf8e7158",
            "placeholder": "​",
            "style": "IPY_MODEL_34cb159f32af4950bd8e8b1828e48ec6",
            "value": "0.001 MB of 0.018 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "b6f5a379432f470c9ee9104bea69219d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6129ec9b9294e1ca760ad6f21f9a3b9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab89fb32062a4b2a83f27c4d18621432",
            "value": 0.038858090254390965
          }
        },
        "c02b275c9eb24697a2d53c227449cb30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63c97284fc4e44efbb20161fcf8e7158": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34cb159f32af4950bd8e8b1828e48ec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6129ec9b9294e1ca760ad6f21f9a3b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab89fb32062a4b2a83f27c4d18621432": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "571f1088186d4c91a12d03fdad4897a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3cd332a37284323a261356d5bdcfabf",
              "IPY_MODEL_7da1d7fcfd7141dd947a35684262f0a4"
            ],
            "layout": "IPY_MODEL_2d6164c531a445c8b2548df7e407620e"
          }
        },
        "b3cd332a37284323a261356d5bdcfabf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_120a72d33943498fa7d7a8bcb6d7bac3",
            "placeholder": "​",
            "style": "IPY_MODEL_31b4328a8ddc4ec6ad47e5db0bf45155",
            "value": "0.001 MB of 0.018 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "7da1d7fcfd7141dd947a35684262f0a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66c980c6cf4742f698e01f5d682c6881",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f5a8e10ea1049e2927808671cba2308",
            "value": 0.0387400434467777
          }
        },
        "2d6164c531a445c8b2548df7e407620e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "120a72d33943498fa7d7a8bcb6d7bac3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31b4328a8ddc4ec6ad47e5db0bf45155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66c980c6cf4742f698e01f5d682c6881": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f5a8e10ea1049e2927808671cba2308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55fbe2834d3047c19951ab154bd640d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db858be869ae4e70bb22c03dca08ea4b",
              "IPY_MODEL_13906cd6ac6142e9a9a2d8866f5e14fa"
            ],
            "layout": "IPY_MODEL_0fdb1d18d62846b68b11a020aa952fe8"
          }
        },
        "db858be869ae4e70bb22c03dca08ea4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae89578db6ec49ba8687e6cca1a95986",
            "placeholder": "​",
            "style": "IPY_MODEL_dc2e6f1ee9694dcc9107487c75358b5e",
            "value": "0.001 MB of 0.018 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "13906cd6ac6142e9a9a2d8866f5e14fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3471f0eb3e7d4414b1761d091ff82f75",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a037d0e20bef4d9baff26cdcaa99bd5f",
            "value": 0.03905843141339444
          }
        },
        "0fdb1d18d62846b68b11a020aa952fe8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae89578db6ec49ba8687e6cca1a95986": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc2e6f1ee9694dcc9107487c75358b5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3471f0eb3e7d4414b1761d091ff82f75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a037d0e20bef4d9baff26cdcaa99bd5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e802090af4c464998709f9dadc1af9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fcfeeba10343484195fd8ecb4884e758",
              "IPY_MODEL_cf2b68dcdf41438e8572c657d5284d82"
            ],
            "layout": "IPY_MODEL_70fb174019e546b0a2defb84f8ba832c"
          }
        },
        "fcfeeba10343484195fd8ecb4884e758": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b75fe3aa80d40baa82e206523169089",
            "placeholder": "​",
            "style": "IPY_MODEL_7289ee46fa2c447f8263cbb24a1d2a57",
            "value": "0.001 MB of 0.018 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "cf2b68dcdf41438e8572c657d5284d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_569dba25d3d94bc0a0e1e394db9f10d7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c51befecf9b437baa4441cc987ea953",
            "value": 0.03926605504587156
          }
        },
        "70fb174019e546b0a2defb84f8ba832c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b75fe3aa80d40baa82e206523169089": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7289ee46fa2c447f8263cbb24a1d2a57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "569dba25d3d94bc0a0e1e394db9f10d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c51befecf9b437baa4441cc987ea953": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b595e40c328c4b9394e564d1cdd59d26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_21b4e4544a3042e8b61ab665b400999e",
              "IPY_MODEL_6ad3a1f2bce54d2ea5cfd3f5cdc5a373"
            ],
            "layout": "IPY_MODEL_5f06b3475c644bbe875c47cd2160ce77"
          }
        },
        "21b4e4544a3042e8b61ab665b400999e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1aed698964c4be080f707eacd16371b",
            "placeholder": "​",
            "style": "IPY_MODEL_1c6c61e2010343e792d20639aa52038b",
            "value": "0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "6ad3a1f2bce54d2ea5cfd3f5cdc5a373": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6eaff9334f31497395a4cd0691f41678",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5e017dbc39f4408691e9bc36d58215d1",
            "value": 1
          }
        },
        "5f06b3475c644bbe875c47cd2160ce77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1aed698964c4be080f707eacd16371b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c6c61e2010343e792d20639aa52038b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6eaff9334f31497395a4cd0691f41678": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e017dbc39f4408691e9bc36d58215d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d0ac5511d1fb440c8db26d168a8f90ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_80cea503623a4e9382093908ad0bb255",
              "IPY_MODEL_4c75820afac94978ab2e8aee0d0a9c81"
            ],
            "layout": "IPY_MODEL_44c2e7a070ef4bcea1213eafe283b7cb"
          }
        },
        "80cea503623a4e9382093908ad0bb255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d4aea74c62848509289faa9e35a9748",
            "placeholder": "​",
            "style": "IPY_MODEL_e7b1e843d2864984b0dcb9226d1c4c96",
            "value": "0.001 MB of 0.009 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "4c75820afac94978ab2e8aee0d0a9c81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b8fc7c6968945929c4631c9a00edac0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c8340d78e8154c31a197c7b9bdbbdf7e",
            "value": 0.08290957329206279
          }
        },
        "44c2e7a070ef4bcea1213eafe283b7cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d4aea74c62848509289faa9e35a9748": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7b1e843d2864984b0dcb9226d1c4c96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b8fc7c6968945929c4631c9a00edac0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8340d78e8154c31a197c7b9bdbbdf7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        " # Vitaly Marin version 2 with early stopping upon reaching covergence close to -100 on this game within 5% or so \n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiXUT3LCMMfZ",
        "outputId": "be807b2b-7d50-4a04-ed58-9ef785f14a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# path to project"
      ],
      "metadata": {
        "id": "jgzGLvSiarHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = 'drive/MyDrive/CS7648_group7_project/'"
      ],
      "metadata": {
        "id": "4LTRT3XRaeP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/IRL_DQfD\n",
        "# !pwd"
      ],
      "metadata": {
        "id": "U7lFHAaw3EWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d76d3611-6e6e-45d4-ab72-a4464e5fe5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/IRL_DQfD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and installations"
      ],
      "metadata": {
        "id": "pj__74Qp2z7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDgg0D3Z_rhH",
        "outputId": "dcf950f8-ba4f-444e-c910-47c3ce1a850c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.9 MB 7.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 168 kB 81.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 71.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 168 kB 79.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 80.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 82.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 79.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 78.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 158 kB 77.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 82.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 74.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 82.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 86.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 81.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 86.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 84.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 156 kB 85.1 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()\n",
        "#6667acf0131f4abd55b0c07404f6f71f48cf2379"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "nJH7cYIn_w3G",
        "outputId": "c23e8b00-dbd0-465d-917e-f871840fbdae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDuJNWLy62x6",
        "outputId": "195f5f89-3779-4a61-c1bd-2df48cc610cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "# from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "HCJZhSQQ8UVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[atari,accept-rom-license]==0.21.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7WSUOG2jLAe",
        "outputId": "89fbfa3e-a8c1-4fa2-d9a5-3e0a16f457ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym[accept-rom-license,atari]==0.21.0\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.21.0) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.21.0) (1.5.0)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.1\n",
            "  Downloading ale_py-0.7.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 53.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from ale-py~=0.7.1->gym[accept-rom-license,atari]==0.21.0) (5.10.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from ale-py~=0.7.1->gym[accept-rom-license,atari]==0.21.0) (4.13.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (4.64.1)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.10.0->ale-py~=0.7.1->gym[accept-rom-license,atari]==0.21.0) (3.11.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (2022.9.24)\n",
            "Building wheels for collected packages: gym, AutoROM.accept-rom-license\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616822 sha256=91df20d97866540a2201f063f6dafb9c9323da6739b3cb8e1f5b54885c7e6dd5\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/6d/b3/a3a6e10704795c9b9000f1ab2dc480dfe7bed42f5972806e73\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441028 sha256=0b335aebe17c54b8c9859f00a88432d808b793ab32d8ab6ff77be4c2724c5bab\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/08/c5/28b973078691a3f8baf99fcaec1ed8f0e05ef6e54d2390212c\n",
            "Successfully built gym AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, gym, ale-py\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2 gym-0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.gymlibrary.dev/environments/classic_control/\n",
        "import gym\n",
        "env = gym.make('Acrobot-v1')"
      ],
      "metadata": {
        "id": "ml85GZ5rGbSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Network"
      ],
      "metadata": {
        "id": "comShCA4271p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Set\n",
        "# Making tf as torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self,input, output):\n",
        "    super(Net, self).__init__()\n",
        "    self.fc1 = nn.Linear(input, 64)\n",
        "    self.fc2 = nn.Linear(64, 64*2)\n",
        "    self.fc3 = nn.Linear(64*2, output)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "class DQN_Torch:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        \n",
        "        #define the state size\n",
        "        self.state_size = state_size\n",
        "        \n",
        "        #define the action size\n",
        "        self.action_size = action_size\n",
        "        \n",
        "        #define the replay buffer\n",
        "        self.replay_buffer = deque(maxlen=10000)\n",
        "\n",
        "        #define the demo buffer\n",
        "        self.demo_buffer = deque(maxlen=10000)\n",
        "        \n",
        "        #define the discount factor\n",
        "        self.gamma = 0.9  \n",
        "        \n",
        "        #define the epsilon value\n",
        "        self.epsilon = 0.01\n",
        "        \n",
        "        #define the update rate at which we want to update the target network\n",
        "        self.update_rate = 20\n",
        "        \n",
        "        #define the main network\n",
        "        self.main_network = Net(self.state_size, self.action_size)\n",
        "        \n",
        "        self.loss_fn = nn.MSELoss()\n",
        "        self.loss_supervised_fn = nn.L1Loss()\n",
        "\n",
        "        self.optim = torch.optim.Adam(self.main_network.parameters(),lr=0.001)\n",
        "      \n",
        "        #define the target network\n",
        "        self.target_network = Net(self.state_size, self.action_size)\n",
        "        self.target_network.requires_grad = False\n",
        "\n",
        "        # self.device = torch.device('cpu')\n",
        "        if torch.cuda.is_available():\n",
        "          print(\"Using CUDA...\")\n",
        "          self.device = torch.device('cuda')\n",
        "          self.main_network = self.main_network.to(device)\n",
        "          self.target_network = self.target_network.to(device)\n",
        "        else:\n",
        "          print(\"Using CPU...\")\n",
        "          self.device = torch.device('cpu')\n",
        "        \n",
        "        #copy the weights of the main network to the target network\n",
        "        self.target_network.load_state_dict(self.main_network.state_dict())\n",
        "\n",
        "    #We learned that we train DQN by randomly sampling a minibatch of transitions from the\n",
        "    #replay buffer. So, we define a function called store_transition which stores the transition information\n",
        "    #into the replay buffer\n",
        "\n",
        "    def store_transistion(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
        "        \n",
        "    # Save expert demonstrations\n",
        "    def store_demonstrations(self, state, action, reward, next_state, done):\n",
        "        self.demo_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    #We learned that in DQN, to take care of exploration-exploitation trade off, we select action\n",
        "    #using the epsilon-greedy policy. So, now we define the function called epsilon_greedy\n",
        "    #for selecting action using the epsilon-greedy policy.\n",
        "    \n",
        "    def epsilon_greedy(self, state):\n",
        "        if random.uniform(0,1) < self.epsilon:\n",
        "            return np.random.randint(self.action_size)\n",
        "        \n",
        "        Q_values = self.main_network(torch.from_numpy(state).type(torch.float).to(device))\n",
        "        \n",
        "        # print(Q_values, torch.argmax(Q_values[0]).item())\n",
        "        return torch.argmax(Q_values).item()\n",
        "\n",
        "    # Pretrain using expert demonstrations\n",
        "    def pre_train(self):\n",
        "      print('Pre-training ...')\n",
        "\n",
        "      ## Loading the pre-trained network\n",
        "      # pretrained_state_dict = pickle.load(open(PATH+'project/Sai_folder/code/experiment_runtime_log/acrobot2_pre_trained_network_400_100_256', 'rb'))\n",
        "      # self.main_network.load_state_dict(pretrained_state_dict)\n",
        "      # self.target_network.load_state_dict(pretrained_state_dict)\n",
        "\n",
        "      ## Pre-training the network\n",
        "      for i in range(200): ## PARAM_CHECK: how many steps for pretraining?\n",
        "          self.train(batch_size = 256, pretrain=True)\n",
        "          if i % 10 == 0 and i > 0:\n",
        "              get_demonstrations_from_trained_dqn(self.main_network.state_dict(), store=False)\n",
        "              print('{}th step of pre-train finish ...'.format(i))\n",
        "\n",
        "      print('All pre-train finish.')\n",
        "\n",
        "    \n",
        "    # Supervised Loss function for the demo buffer data\n",
        "\n",
        "    def loss_supervised(self, X, y):\n",
        "        X_max, _ = torch.max(X, axis=1)\n",
        "        # print(X_max, y)\n",
        "        return self.loss_supervised_fn(X_max, y)\n",
        "\n",
        "\n",
        "    #train the network\n",
        "    def train(self, batch_size = 32, pretrain = False, sample_demo = False, demo_sample_percent = 0.25, supervised_weight = 1.0):\n",
        "        \n",
        "        data_buffer = []\n",
        "        demo_buffer = []\n",
        "\n",
        "        # # data_buffer = self.demo_buffer if pretrain else self.replay_buffer\n",
        "        if pretrain:\n",
        "          #sample a mini batch of transition from the demo buffer if pretrain\n",
        "          demo_buffer = random.sample(self.demo_buffer, batch_size)\n",
        "        elif sample_demo:\n",
        "          ## Add some of the expert_demos to the replay_buffer\n",
        "          demo_sample_size = int(batch_size * demo_sample_percent)\n",
        "          demo_buffer = random.sample(self.demo_buffer, demo_sample_size)\n",
        "          data_buffer = random.sample(self.replay_buffer, batch_size - demo_sample_size)\n",
        "          # print(f'Demos: {len(self.demo_buffer)}, Data: {len(data_buffer)}') \n",
        "        else:\n",
        "          ## Don't include any expert demonstrations during training\n",
        "          data_buffer = random.sample(self.replay_buffer, batch_size)\n",
        "          \n",
        "        # minibatch = random.sample(self.replay_buffer, batch_size)\n",
        "        # minibatch = random.sample(data_buffer, batch_size)\n",
        "\n",
        "        minibatch = []\n",
        "        if(len(data_buffer)!=0):\n",
        "          minibatch.append(data_buffer)\n",
        "        if(len(demo_buffer)!= 0):\n",
        "          minibatch.append(demo_buffer)\n",
        "        minibatch = np.concatenate(minibatch, axis=0)\n",
        "\n",
        "        X = [] # State \n",
        "        y = [] # Expected Q values of the input state\n",
        "        \n",
        "        #compute the Q value using the target network \n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            if not done:\n",
        "                target_Q = (reward + self.gamma * torch.max(self.target_network(torch.from_numpy(next_state).type(torch.float).to(device))).detach().cpu().numpy())\n",
        "            else:\n",
        "                target_Q = reward\n",
        "                \n",
        "            #compute the Q value using the main network\n",
        "            state = torch.from_numpy(state).type(torch.float).to(self.device) \n",
        "            Q_values = self.main_network(state)\n",
        "\n",
        "            # print(Q_values)\n",
        "            Q_values[action] = target_Q\n",
        "            # print(Q_values)\n",
        "            X.append(state)\n",
        "            y.append(Q_values)\n",
        "        \n",
        "        X = torch.squeeze(torch.stack(X, 0), 1)\n",
        "        y = torch.squeeze(torch.stack(y, 0), 1)\n",
        "        # X = torch.from_numpy(np.array(X).astype('float32')).type(torch.float).to(self.device)\n",
        "        # y = torch.from_numpy(np.array(y).astype('float32')).type(torch.float).to(self.device)\n",
        "\n",
        "        y_pred = self.main_network(X)\n",
        "\n",
        "        X_supervised = [] # Q_value of the state with masks\n",
        "        y_supervised = [] # Actual Q_value of the selected action\n",
        "\n",
        "        for state, action, reward, next_state, done in demo_buffer:\n",
        "            #compute the Q value using the main network\n",
        "            state = torch.from_numpy(state).type(torch.float).to(self.device) \n",
        "            Q_values = self.main_network(state)\n",
        "\n",
        "            mask_action = 0.8 * torch.ones(self.action_size).type(torch.float).to(self.device)\n",
        "            mask_action[action] = 0\n",
        "\n",
        "            X_supervised.append(Q_values + mask_action)\n",
        "            y_supervised.append(Q_values[action])\n",
        "        \n",
        "        if(len(demo_buffer) > 0):\n",
        "          # print(X_supervised, y_supervised)\n",
        "          X_supervised = torch.squeeze(torch.stack(X_supervised, 0), 1)\n",
        "          y_supervised = torch.stack(y_supervised, 0).detach()\n",
        "\n",
        "        if(len(demo_buffer) > 0):\n",
        "          loss = self.loss_fn(y_pred, y) + supervised_weight * self.loss_supervised(X_supervised, y_supervised) # TODO: lambda is the weighting parameter for the supervised loss. Value = ?\n",
        "        else:\n",
        "          loss = self.loss_fn(y_pred, y)\n",
        "\n",
        "        self.optim.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optim.step()\n",
        "            \n",
        "    #update the target network weights by copying from the main network\n",
        "    def update_target_network(self):\n",
        "        self.target_network.load_state_dict(self.main_network.state_dict())"
      ],
      "metadata": {
        "id": "Z_U62Sx_Yifj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "SLJScBpm4w06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 200\n",
        "num_timesteps = 1000\n",
        "batch_size = 64\n",
        "num_screens = 4\n",
        "\n",
        "#Game parameters\n",
        "state_size = 6\n",
        "action_size = 3\n",
        "\n",
        "max_reward_goal = -100*1.05\n"
      ],
      "metadata": {
        "id": "HJpiEk73ADvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get expert demonstrations"
      ],
      "metadata": {
        "id": "weSuqwbM2eyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_demonstrations_from_trained_dqn(trained_state_dict, store=False):\n",
        "  trained_dqn = DQN_Torch(state_size, action_size)\n",
        "  trained_dqn.target_network.load_state_dict(trained_state_dict)\n",
        "  trained_dqn.main_network.load_state_dict(trained_state_dict)\n",
        "  # new_action = trained_dqn.epsilon_greedy()\n",
        "  done = False\n",
        "  avg_reward = 0\n",
        "\n",
        "  for i in range(100):\n",
        "      \n",
        "      #set return to 0\n",
        "      Return = 0\n",
        "      \n",
        "      #preprocess the game screen\n",
        "      state = env.reset()\n",
        "\n",
        "      #for each step in the episode\n",
        "      for t in range(1000):\n",
        "\n",
        "          action = trained_dqn.epsilon_greedy(state)\n",
        "\n",
        "          #perform the selected action\n",
        "          next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "          #store the transition information\n",
        "          trained_dqn.store_transistion(state, action, reward, next_state, done)\n",
        "\n",
        "          #update current state to next state\n",
        "          state = next_state\n",
        "\n",
        "          #update the return\n",
        "          Return += reward\n",
        "\n",
        "          if done:\n",
        "            # print(f\"Iteration {i}: Time Steps Completed: {t}\")\n",
        "            avg_reward += Return\n",
        "            break\n",
        "\n",
        "  print(f\"Total Avg Reward = {avg_reward/100}\")\n",
        "  # print(trained_dqn.replay_buffer)\n",
        "  if store:\n",
        "    pickle.dump(trained_dqn.replay_buffer, open('acrobot2_expert_demonstrations', 'wb'))"
      ],
      "metadata": {
        "id": "Ge5wKqdDQr4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trained_state_dict = pickle.load(open(PATH+'project/Sai_folder/code/model_weights/model_weights_400.pkl', 'rb'))\n",
        "# get_demonstrations_from_trained_dqn(trained_state_dict, store=True)"
      ],
      "metadata": {
        "id": "INAzS4d6TbSf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd609ef7-592a-4235-eab6-692d783d087b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Avg Reward = 148.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DDQN"
      ],
      "metadata": {
        "id": "6nfGdFoc2kZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_DDQN(dqn):\n",
        "\n",
        "  done = False\n",
        "  time_step = 0\n",
        "  ep_reward = []\n",
        "  ep_steps = []\n",
        "\n",
        "  wandb.init(\n",
        "      entity=\"irl_team7\",\n",
        "      # Set the project where this run will be logged\n",
        "      project=\"DQfD\", \n",
        "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "      name = f\"acrobot2_ddqn_{num_episodes}_{batch_size}\",\n",
        "      # name=f\"cartpole_{anneal_type}_{decay_rate}\", \n",
        "      # Track hyperparameters and run metadata\n",
        "      config={\n",
        "          \"environment\": \"Acrobot\",\n",
        "          \"network\": \"DDQN\",\n",
        "          \"num_episodes\": num_episodes,\n",
        "          \"num_timesteps\": num_timesteps,\n",
        "          \"batch_size\": batch_size,\n",
        "          \"num_screens\": num_screens,\n",
        "          \"state_size\": state_size,\n",
        "          \"action_size\": action_size\n",
        "          # \"random_seed\": seed_val\n",
        "      })\n",
        "\n",
        "  converged_model_true = False \n",
        "\n",
        "  #for each episode\n",
        "  for i in range(num_episodes):\n",
        "      \n",
        "      #set return to 0\n",
        "      Return = 0\n",
        "      \n",
        "      #preprocess the game screen\n",
        "      state = env.reset()\n",
        "\n",
        "      #for each step in the episode\n",
        "      for t in range(num_timesteps):\n",
        "          #render the environment\n",
        "          # env.render(render_mode = \"human\")\n",
        "          \n",
        "          #update the time step\n",
        "          time_step += 1\n",
        "          \n",
        "          #update the target network\n",
        "          if time_step % dqn.update_rate == 0:\n",
        "              dqn.update_target_network()\n",
        "\n",
        "          #select the action\n",
        "          action = dqn.epsilon_greedy(state)\n",
        "\n",
        "          #perform the selected action\n",
        "          next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "          #store the transition information\n",
        "          dqn.store_transistion(state, action, reward, next_state, done)\n",
        "\n",
        "          #update current state to next state\n",
        "          state = next_state\n",
        "\n",
        "          #update the return\n",
        "          Return += reward\n",
        "\n",
        "          # if t%1000 == 0:\n",
        "          #   print(f\"Episode: {i}  TimeStep: {t}  Cum Reward: {Return}\")\n",
        "\n",
        "          #if the episode is done then print the return\n",
        "          if done:\n",
        "              print(f\"Episode: {i} Time Step: {t} Cumulative Rewards: {Return}\")\n",
        "              ep_reward.append(Return)\n",
        "              ep_steps.append(t)\n",
        "              break\n",
        "              \n",
        "          #if the number of transistions in the replay buffer is greater than batch size\n",
        "          #then train the network\n",
        "          if len(dqn.replay_buffer) > batch_size:\n",
        "              dqn.train(batch_size)\n",
        "\n",
        "      wandb.log({\"rewards\": Return, \"time_step\": t})\n",
        "      if(Return >= max_reward_goal):\n",
        "         converged_model_true = True \n",
        "\n",
        "      if (converged_model_true or (i+1)%50 == 0):\n",
        "          pickle.dump(dqn.main_network.state_dict(), open(f'acrobot2_ddqn_weights_{i+1}', 'wb')) # save model every 50 epochs \n",
        "          if(converged_model_true ):\n",
        "              break\n",
        "\n",
        "  wandb.finish()\n",
        "\n",
        "  return ep_reward"
      ],
      "metadata": {
        "id": "o_cwlPrDex-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DQfD"
      ],
      "metadata": {
        "id": "_O0shl7V2nGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_DQfD(dqn, anneal_type = \"constant\", initial_sample_percent = 0, anneal_threshold = None, decay_rate = 0.9, expo_const = 0.5, seed_val=1):\n",
        "  ## anneal_type = [\"constant\", \"linear\", \"exponential\", \"threshold\"]\n",
        "\n",
        "  ## Pre-train the network on the expert demonstration\n",
        "\n",
        "  # get the demonstrations (Assuming there is a file)\n",
        "  # expert_demos = pickle.load(open('expert_demonstrations', 'rb'))\n",
        "  random.seed(seed_val)\n",
        "  np.random.seed(seed_val)\n",
        "\n",
        "  expert_demos = pickle.load(open('acrobot_expert_demonstrations', 'rb'))\n",
        "\n",
        "\n",
        "  dqn.demo_buffer = expert_demos\n",
        "\n",
        "  print(f\"Number of <state,action> pairs in the demonstrator data = {len(expert_demos)}\")\n",
        "\n",
        "  if anneal_type == \"constant\":\n",
        "    run_name = f\"acrobot_constant_{initial_sample_percent}_{seed_val}\"\n",
        "  elif anneal_type == \"linear\":\n",
        "    run_name = f\"acrobot_linear_{initial_sample_percent}_{decay_rate}_{seed_val}\"\n",
        "  elif anneal_type == \"exponential\":\n",
        "    run_name = f\"acrobot_exponential_{initial_sample_percent}_{expo_const}_{seed_val}\"\n",
        "  elif anneal_type == \"threshold\":\n",
        "    run_name = f\"acrobot_threshold_{initial_sample_percent}_{anneal_threshold}_{seed_val}\"\n",
        "\n",
        "  wandb.init(\n",
        "      entity=\"irl_team7\",\n",
        "      # Set the project where this run will be logged\n",
        "      project=\"DQfD\", \n",
        "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "      name = run_name,\n",
        "      # name=f\"cartpole_{anneal_type}_{decay_rate}\", \n",
        "      # Track hyperparameters and run metadata\n",
        "      config={\n",
        "          \"environment\": \"Acrobot\",\n",
        "          \"num_episodes\": num_episodes,\n",
        "          \"num_timesteps\": num_timesteps,\n",
        "          \"batch_size\": batch_size,\n",
        "          \"num_screens\": num_screens,\n",
        "          \"state_size\": state_size,\n",
        "          \"action_size\": action_size,\n",
        "          \"initial_sample_percent\": initial_sample_percent, \n",
        "          \"anneal_type\": anneal_type,\n",
        "          \"decay_rate\": decay_rate,\n",
        "          \"expo_const\": expo_const,\n",
        "          \"anneal_threshold\": anneal_threshold,\n",
        "          \"random_seed\": seed_val\n",
        "      })\n",
        "\n",
        "\n",
        "  ## Pre-train the network\n",
        "  dqn.pre_train()\n",
        "\n",
        "  done = False\n",
        "  time_step = 0\n",
        "  ep_reward = []\n",
        "  ep_steps = []\n",
        "\n",
        "\n",
        "  # if anneal_type == \"constant\":\n",
        "  #   demo_sample_percent = initial_sample_percent\n",
        "  # else:\n",
        "  #   demo_sample_percent = 1    ## Start with all expert demonstrations and gradually decrease\n",
        "\n",
        "\n",
        "  #for each episode\n",
        "  for i in range(num_episodes):\n",
        "      \n",
        "      #set return to 0\n",
        "      Return = 0\n",
        "      \n",
        "      #preprocess the game screen\n",
        "      state = env.reset()\n",
        "\n",
        "      if anneal_type == \"constant\":\n",
        "        demo_sample_percent = initial_sample_percent\n",
        "      elif anneal_type == \"linear\":\n",
        "        # demo_sample_percent = decay_rate * demo_sample_percent\n",
        "        demo_sample_percent = max(0, initial_sample_percent - decay_rate * i)\n",
        "      elif anneal_type == \"exponential\":\n",
        "        demo_sample_percent = initial_sample_percent * np.exp(-expo_const * i)\n",
        "      elif anneal_type == \"threshold\":\n",
        "        demo_sample_percent = initial_sample_percent\n",
        "        if len(ep_reward) > 10 and sum(ep_reward[-10:])/10 > anneal_threshold:\n",
        "          demo_sample_percent = 0.0\n",
        "\n",
        "      print(f'Annealing: {anneal_type}, Demo Sampling Percent: {demo_sample_percent}')\n",
        "\n",
        "      #for each step in the episode\n",
        "      for t in range(num_timesteps):\n",
        "          #render the environment\n",
        "          # env.render(render_mode = \"human\")\n",
        "          \n",
        "          #update the time step\n",
        "          time_step += 1\n",
        "          \n",
        "          #update the target network\n",
        "          if time_step % dqn.update_rate == 0:\n",
        "              dqn.update_target_network()\n",
        "\n",
        "          #select the action\n",
        "          action = dqn.epsilon_greedy(state)\n",
        "\n",
        "          #perform the selected action\n",
        "          next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "          #store the transition information\n",
        "          dqn.store_transistion(state, action, reward, next_state, done)\n",
        "\n",
        "          #update current state to next state\n",
        "          state = next_state\n",
        "\n",
        "          #update the return\n",
        "          Return += reward\n",
        "\n",
        "          # if t%1000 == 0:\n",
        "          #   print(f\"Episode: {i}  TimeStep: {t}  Cum Reward: {Return}\")\n",
        "\n",
        "          #if the episode is done then print the return\n",
        "          if done:\n",
        "              print(f\"Episode: {i} Time Step: {t} Cumulative Rewards: {Return}\")\n",
        "              ep_reward.append(Return)\n",
        "              ep_steps.append(t)\n",
        "              break\n",
        "              \n",
        "          #if the number of transistions in the replay buffer is greater than batch size\n",
        "          #then train the network\n",
        "          if len(dqn.replay_buffer) > batch_size:\n",
        "              dqn.train(batch_size, sample_demo = True, supervised_weight = 0.1, demo_sample_percent = demo_sample_percent)\n",
        "    \n",
        "      if(len(ep_reward) > 5 and np.mean(ep_reward[-5:0])==500):\n",
        "        break\n",
        "\n",
        "      wandb.log({\"rewards\": Return, \"time_step\": t})\n",
        "  \n",
        "  wandb.finish()\n",
        "\n",
        "  pickle.dump(ep_reward, open(run_name, 'wb'))\n",
        "\n",
        "  return ep_reward"
      ],
      "metadata": {
        "id": "LEULjGvcfUC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display"
      ],
      "metadata": {
        "id": "-JOB1BXd48K8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_rewards(ep_reward):\n",
        "  plt.plot(np.arange(len(ep_reward)), ep_reward)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "m0zWw_CNVC43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training DDQN"
      ],
      "metadata": {
        "id": "z61rS4pTTo-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dqn = DQN_Torch(state_size, action_size)\n",
        "run_DDQN(dqn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d0ac5511d1fb440c8db26d168a8f90ee",
            "80cea503623a4e9382093908ad0bb255",
            "4c75820afac94978ab2e8aee0d0a9c81",
            "44c2e7a070ef4bcea1213eafe283b7cb",
            "3d4aea74c62848509289faa9e35a9748",
            "e7b1e843d2864984b0dcb9226d1c4c96",
            "5b8fc7c6968945929c4631c9a00edac0",
            "c8340d78e8154c31a197c7b9bdbbdf7e"
          ]
        },
        "id": "Go3Q0R2oTuwd",
        "outputId": "5e1fe176-cdaa-4b5a-e440-0a75d34700b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkmittal37\u001b[0m (\u001b[33mirl_team7\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/IRL_DQfD/wandb/run-20221211_071358-2z7u7q18</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/irl_team7/DQfD/runs/2z7u7q18\" target=\"_blank\">acrobot_ddqn_400_64</a></strong> to <a href=\"https://wandb.ai/irl_team7/DQfD\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0 Time Step: 499 Cumulative Rewards: -500.0\n",
            "Episode: 1 Time Step: 499 Cumulative Rewards: -500.0\n",
            "Episode: 2 Time Step: 499 Cumulative Rewards: -500.0\n",
            "Episode: 3 Time Step: 133 Cumulative Rewards: -133.0\n",
            "Episode: 4 Time Step: 125 Cumulative Rewards: -125.0\n",
            "Episode: 5 Time Step: 140 Cumulative Rewards: -140.0\n",
            "Episode: 6 Time Step: 295 Cumulative Rewards: -295.0\n",
            "Episode: 7 Time Step: 304 Cumulative Rewards: -304.0\n",
            "Episode: 8 Time Step: 440 Cumulative Rewards: -440.0\n",
            "Episode: 9 Time Step: 499 Cumulative Rewards: -500.0\n",
            "Episode: 10 Time Step: 499 Cumulative Rewards: -500.0\n",
            "Episode: 11 Time Step: 310 Cumulative Rewards: -310.0\n",
            "Episode: 12 Time Step: 454 Cumulative Rewards: -454.0\n",
            "Episode: 13 Time Step: 279 Cumulative Rewards: -279.0\n",
            "Episode: 14 Time Step: 248 Cumulative Rewards: -248.0\n",
            "Episode: 15 Time Step: 301 Cumulative Rewards: -301.0\n",
            "Episode: 16 Time Step: 259 Cumulative Rewards: -259.0\n",
            "Episode: 17 Time Step: 454 Cumulative Rewards: -454.0\n",
            "Episode: 18 Time Step: 367 Cumulative Rewards: -367.0\n",
            "Episode: 19 Time Step: 181 Cumulative Rewards: -181.0\n",
            "Episode: 20 Time Step: 271 Cumulative Rewards: -271.0\n",
            "Episode: 21 Time Step: 499 Cumulative Rewards: -500.0\n",
            "Episode: 22 Time Step: 227 Cumulative Rewards: -227.0\n",
            "Episode: 23 Time Step: 391 Cumulative Rewards: -391.0\n",
            "Episode: 24 Time Step: 195 Cumulative Rewards: -195.0\n",
            "Episode: 25 Time Step: 359 Cumulative Rewards: -359.0\n",
            "Episode: 26 Time Step: 302 Cumulative Rewards: -302.0\n",
            "Episode: 27 Time Step: 499 Cumulative Rewards: -500.0\n",
            "Episode: 28 Time Step: 216 Cumulative Rewards: -216.0\n",
            "Episode: 29 Time Step: 274 Cumulative Rewards: -274.0\n",
            "Episode: 30 Time Step: 323 Cumulative Rewards: -323.0\n",
            "Episode: 31 Time Step: 140 Cumulative Rewards: -140.0\n",
            "Episode: 32 Time Step: 194 Cumulative Rewards: -194.0\n",
            "Episode: 33 Time Step: 198 Cumulative Rewards: -198.0\n",
            "Episode: 34 Time Step: 179 Cumulative Rewards: -179.0\n",
            "Episode: 35 Time Step: 197 Cumulative Rewards: -197.0\n",
            "Episode: 36 Time Step: 148 Cumulative Rewards: -148.0\n",
            "Episode: 37 Time Step: 214 Cumulative Rewards: -214.0\n",
            "Episode: 38 Time Step: 155 Cumulative Rewards: -155.0\n",
            "Episode: 39 Time Step: 164 Cumulative Rewards: -164.0\n",
            "Episode: 40 Time Step: 143 Cumulative Rewards: -143.0\n",
            "Episode: 41 Time Step: 197 Cumulative Rewards: -197.0\n",
            "Episode: 42 Time Step: 468 Cumulative Rewards: -468.0\n",
            "Episode: 43 Time Step: 344 Cumulative Rewards: -344.0\n",
            "Episode: 44 Time Step: 154 Cumulative Rewards: -154.0\n",
            "Episode: 45 Time Step: 242 Cumulative Rewards: -242.0\n",
            "Episode: 46 Time Step: 210 Cumulative Rewards: -210.0\n",
            "Episode: 47 Time Step: 162 Cumulative Rewards: -162.0\n",
            "Episode: 48 Time Step: 205 Cumulative Rewards: -205.0\n",
            "Episode: 49 Time Step: 228 Cumulative Rewards: -228.0\n",
            "Episode: 50 Time Step: 238 Cumulative Rewards: -238.0\n",
            "Episode: 51 Time Step: 207 Cumulative Rewards: -207.0\n",
            "Episode: 52 Time Step: 182 Cumulative Rewards: -182.0\n",
            "Episode: 53 Time Step: 246 Cumulative Rewards: -246.0\n",
            "Episode: 54 Time Step: 442 Cumulative Rewards: -442.0\n",
            "Episode: 55 Time Step: 249 Cumulative Rewards: -249.0\n",
            "Episode: 56 Time Step: 211 Cumulative Rewards: -211.0\n",
            "Episode: 57 Time Step: 263 Cumulative Rewards: -263.0\n",
            "Episode: 58 Time Step: 460 Cumulative Rewards: -460.0\n",
            "Episode: 59 Time Step: 117 Cumulative Rewards: -117.0\n",
            "Episode: 60 Time Step: 146 Cumulative Rewards: -146.0\n",
            "Episode: 61 Time Step: 131 Cumulative Rewards: -131.0\n",
            "Episode: 62 Time Step: 316 Cumulative Rewards: -316.0\n",
            "Episode: 63 Time Step: 187 Cumulative Rewards: -187.0\n",
            "Episode: 64 Time Step: 256 Cumulative Rewards: -256.0\n",
            "Episode: 65 Time Step: 368 Cumulative Rewards: -368.0\n",
            "Episode: 66 Time Step: 204 Cumulative Rewards: -204.0\n",
            "Episode: 67 Time Step: 396 Cumulative Rewards: -396.0\n",
            "Episode: 68 Time Step: 380 Cumulative Rewards: -380.0\n",
            "Episode: 69 Time Step: 229 Cumulative Rewards: -229.0\n",
            "Episode: 70 Time Step: 267 Cumulative Rewards: -267.0\n",
            "Episode: 71 Time Step: 171 Cumulative Rewards: -171.0\n",
            "Episode: 72 Time Step: 252 Cumulative Rewards: -252.0\n",
            "Episode: 73 Time Step: 259 Cumulative Rewards: -259.0\n",
            "Episode: 74 Time Step: 197 Cumulative Rewards: -197.0\n",
            "Episode: 75 Time Step: 340 Cumulative Rewards: -340.0\n",
            "Episode: 76 Time Step: 189 Cumulative Rewards: -189.0\n",
            "Episode: 77 Time Step: 304 Cumulative Rewards: -304.0\n",
            "Episode: 78 Time Step: 266 Cumulative Rewards: -266.0\n",
            "Episode: 79 Time Step: 244 Cumulative Rewards: -244.0\n",
            "Episode: 80 Time Step: 123 Cumulative Rewards: -123.0\n",
            "Episode: 81 Time Step: 179 Cumulative Rewards: -179.0\n",
            "Episode: 82 Time Step: 188 Cumulative Rewards: -188.0\n",
            "Episode: 83 Time Step: 260 Cumulative Rewards: -260.0\n",
            "Episode: 84 Time Step: 342 Cumulative Rewards: -342.0\n",
            "Episode: 85 Time Step: 273 Cumulative Rewards: -273.0\n",
            "Episode: 86 Time Step: 179 Cumulative Rewards: -179.0\n",
            "Episode: 87 Time Step: 295 Cumulative Rewards: -295.0\n",
            "Episode: 88 Time Step: 169 Cumulative Rewards: -169.0\n",
            "Episode: 89 Time Step: 192 Cumulative Rewards: -192.0\n",
            "Episode: 90 Time Step: 133 Cumulative Rewards: -133.0\n",
            "Episode: 91 Time Step: 196 Cumulative Rewards: -196.0\n",
            "Episode: 92 Time Step: 418 Cumulative Rewards: -418.0\n",
            "Episode: 93 Time Step: 255 Cumulative Rewards: -255.0\n",
            "Episode: 94 Time Step: 333 Cumulative Rewards: -333.0\n",
            "Episode: 95 Time Step: 227 Cumulative Rewards: -227.0\n",
            "Episode: 96 Time Step: 229 Cumulative Rewards: -229.0\n",
            "Episode: 97 Time Step: 233 Cumulative Rewards: -233.0\n",
            "Episode: 98 Time Step: 264 Cumulative Rewards: -264.0\n",
            "Episode: 99 Time Step: 407 Cumulative Rewards: -407.0\n",
            "Episode: 100 Time Step: 233 Cumulative Rewards: -233.0\n",
            "Episode: 101 Time Step: 350 Cumulative Rewards: -350.0\n",
            "Episode: 102 Time Step: 147 Cumulative Rewards: -147.0\n",
            "Episode: 103 Time Step: 297 Cumulative Rewards: -297.0\n",
            "Episode: 104 Time Step: 499 Cumulative Rewards: -500.0\n",
            "Episode: 105 Time Step: 286 Cumulative Rewards: -286.0\n",
            "Episode: 106 Time Step: 499 Cumulative Rewards: -500.0\n",
            "Episode: 107 Time Step: 216 Cumulative Rewards: -216.0\n",
            "Episode: 108 Time Step: 173 Cumulative Rewards: -173.0\n",
            "Episode: 109 Time Step: 424 Cumulative Rewards: -424.0\n",
            "Episode: 110 Time Step: 185 Cumulative Rewards: -185.0\n",
            "Episode: 111 Time Step: 281 Cumulative Rewards: -281.0\n",
            "Episode: 112 Time Step: 189 Cumulative Rewards: -189.0\n",
            "Episode: 113 Time Step: 267 Cumulative Rewards: -267.0\n",
            "Episode: 114 Time Step: 290 Cumulative Rewards: -290.0\n",
            "Episode: 115 Time Step: 370 Cumulative Rewards: -370.0\n",
            "Episode: 116 Time Step: 273 Cumulative Rewards: -273.0\n",
            "Episode: 117 Time Step: 162 Cumulative Rewards: -162.0\n",
            "Episode: 118 Time Step: 130 Cumulative Rewards: -130.0\n",
            "Episode: 119 Time Step: 254 Cumulative Rewards: -254.0\n",
            "Episode: 120 Time Step: 232 Cumulative Rewards: -232.0\n",
            "Episode: 121 Time Step: 236 Cumulative Rewards: -236.0\n",
            "Episode: 122 Time Step: 253 Cumulative Rewards: -253.0\n",
            "Episode: 123 Time Step: 189 Cumulative Rewards: -189.0\n",
            "Episode: 124 Time Step: 174 Cumulative Rewards: -174.0\n",
            "Episode: 125 Time Step: 154 Cumulative Rewards: -154.0\n",
            "Episode: 126 Time Step: 138 Cumulative Rewards: -138.0\n",
            "Episode: 127 Time Step: 219 Cumulative Rewards: -219.0\n",
            "Episode: 128 Time Step: 163 Cumulative Rewards: -163.0\n",
            "Episode: 129 Time Step: 210 Cumulative Rewards: -210.0\n",
            "Episode: 130 Time Step: 240 Cumulative Rewards: -240.0\n",
            "Episode: 131 Time Step: 268 Cumulative Rewards: -268.0\n",
            "Episode: 132 Time Step: 142 Cumulative Rewards: -142.0\n",
            "Episode: 133 Time Step: 204 Cumulative Rewards: -204.0\n",
            "Episode: 134 Time Step: 129 Cumulative Rewards: -129.0\n",
            "Episode: 135 Time Step: 184 Cumulative Rewards: -184.0\n",
            "Episode: 136 Time Step: 142 Cumulative Rewards: -142.0\n",
            "Episode: 137 Time Step: 201 Cumulative Rewards: -201.0\n",
            "Episode: 138 Time Step: 272 Cumulative Rewards: -272.0\n",
            "Episode: 139 Time Step: 228 Cumulative Rewards: -228.0\n",
            "Episode: 140 Time Step: 262 Cumulative Rewards: -262.0\n",
            "Episode: 141 Time Step: 364 Cumulative Rewards: -364.0\n",
            "Episode: 142 Time Step: 144 Cumulative Rewards: -144.0\n",
            "Episode: 143 Time Step: 104 Cumulative Rewards: -104.0\n",
            "Episode: 144 Time Step: 119 Cumulative Rewards: -119.0\n",
            "Episode: 145 Time Step: 157 Cumulative Rewards: -157.0\n",
            "Episode: 146 Time Step: 165 Cumulative Rewards: -165.0\n",
            "Episode: 147 Time Step: 133 Cumulative Rewards: -133.0\n",
            "Episode: 148 Time Step: 142 Cumulative Rewards: -142.0\n",
            "Episode: 149 Time Step: 190 Cumulative Rewards: -190.0\n",
            "Episode: 150 Time Step: 127 Cumulative Rewards: -127.0\n",
            "Episode: 151 Time Step: 273 Cumulative Rewards: -273.0\n",
            "Episode: 152 Time Step: 210 Cumulative Rewards: -210.0\n",
            "Episode: 153 Time Step: 142 Cumulative Rewards: -142.0\n",
            "Episode: 154 Time Step: 246 Cumulative Rewards: -246.0\n",
            "Episode: 155 Time Step: 206 Cumulative Rewards: -206.0\n",
            "Episode: 156 Time Step: 158 Cumulative Rewards: -158.0\n",
            "Episode: 157 Time Step: 249 Cumulative Rewards: -249.0\n",
            "Episode: 158 Time Step: 378 Cumulative Rewards: -378.0\n",
            "Episode: 159 Time Step: 253 Cumulative Rewards: -253.0\n",
            "Episode: 160 Time Step: 218 Cumulative Rewards: -218.0\n",
            "Episode: 161 Time Step: 174 Cumulative Rewards: -174.0\n",
            "Episode: 162 Time Step: 166 Cumulative Rewards: -166.0\n",
            "Episode: 163 Time Step: 269 Cumulative Rewards: -269.0\n",
            "Episode: 164 Time Step: 249 Cumulative Rewards: -249.0\n",
            "Episode: 165 Time Step: 227 Cumulative Rewards: -227.0\n",
            "Episode: 166 Time Step: 166 Cumulative Rewards: -166.0\n",
            "Episode: 167 Time Step: 180 Cumulative Rewards: -180.0\n",
            "Episode: 168 Time Step: 242 Cumulative Rewards: -242.0\n",
            "Episode: 169 Time Step: 129 Cumulative Rewards: -129.0\n",
            "Episode: 170 Time Step: 369 Cumulative Rewards: -369.0\n",
            "Episode: 171 Time Step: 375 Cumulative Rewards: -375.0\n",
            "Episode: 172 Time Step: 183 Cumulative Rewards: -183.0\n",
            "Episode: 173 Time Step: 173 Cumulative Rewards: -173.0\n",
            "Episode: 174 Time Step: 267 Cumulative Rewards: -267.0\n",
            "Episode: 175 Time Step: 196 Cumulative Rewards: -196.0\n",
            "Episode: 176 Time Step: 173 Cumulative Rewards: -173.0\n",
            "Episode: 177 Time Step: 240 Cumulative Rewards: -240.0\n",
            "Episode: 178 Time Step: 251 Cumulative Rewards: -251.0\n",
            "Episode: 179 Time Step: 137 Cumulative Rewards: -137.0\n",
            "Episode: 180 Time Step: 151 Cumulative Rewards: -151.0\n",
            "Episode: 181 Time Step: 141 Cumulative Rewards: -141.0\n",
            "Episode: 182 Time Step: 248 Cumulative Rewards: -248.0\n",
            "Episode: 183 Time Step: 289 Cumulative Rewards: -289.0\n",
            "Episode: 184 Time Step: 353 Cumulative Rewards: -353.0\n",
            "Episode: 185 Time Step: 178 Cumulative Rewards: -178.0\n",
            "Episode: 186 Time Step: 193 Cumulative Rewards: -193.0\n",
            "Episode: 187 Time Step: 288 Cumulative Rewards: -288.0\n",
            "Episode: 188 Time Step: 180 Cumulative Rewards: -180.0\n",
            "Episode: 189 Time Step: 126 Cumulative Rewards: -126.0\n",
            "Episode: 190 Time Step: 177 Cumulative Rewards: -177.0\n",
            "Episode: 191 Time Step: 219 Cumulative Rewards: -219.0\n",
            "Episode: 192 Time Step: 128 Cumulative Rewards: -128.0\n",
            "Episode: 193 Time Step: 200 Cumulative Rewards: -200.0\n",
            "Episode: 194 Time Step: 160 Cumulative Rewards: -160.0\n",
            "Episode: 195 Time Step: 393 Cumulative Rewards: -393.0\n",
            "Episode: 196 Time Step: 232 Cumulative Rewards: -232.0\n",
            "Episode: 197 Time Step: 121 Cumulative Rewards: -121.0\n",
            "Episode: 198 Time Step: 252 Cumulative Rewards: -252.0\n",
            "Episode: 199 Time Step: 415 Cumulative Rewards: -415.0\n",
            "Episode: 200 Time Step: 196 Cumulative Rewards: -196.0\n",
            "Episode: 201 Time Step: 203 Cumulative Rewards: -203.0\n",
            "Episode: 202 Time Step: 146 Cumulative Rewards: -146.0\n",
            "Episode: 203 Time Step: 282 Cumulative Rewards: -282.0\n",
            "Episode: 204 Time Step: 475 Cumulative Rewards: -475.0\n",
            "Episode: 205 Time Step: 190 Cumulative Rewards: -190.0\n",
            "Episode: 206 Time Step: 305 Cumulative Rewards: -305.0\n",
            "Episode: 207 Time Step: 340 Cumulative Rewards: -340.0\n",
            "Episode: 208 Time Step: 157 Cumulative Rewards: -157.0\n",
            "Episode: 209 Time Step: 244 Cumulative Rewards: -244.0\n",
            "Episode: 210 Time Step: 209 Cumulative Rewards: -209.0\n",
            "Episode: 211 Time Step: 310 Cumulative Rewards: -310.0\n",
            "Episode: 212 Time Step: 176 Cumulative Rewards: -176.0\n",
            "Episode: 213 Time Step: 205 Cumulative Rewards: -205.0\n",
            "Episode: 214 Time Step: 149 Cumulative Rewards: -149.0\n",
            "Episode: 215 Time Step: 233 Cumulative Rewards: -233.0\n",
            "Episode: 216 Time Step: 197 Cumulative Rewards: -197.0\n",
            "Episode: 217 Time Step: 211 Cumulative Rewards: -211.0\n",
            "Episode: 218 Time Step: 220 Cumulative Rewards: -220.0\n",
            "Episode: 219 Time Step: 240 Cumulative Rewards: -240.0\n",
            "Episode: 220 Time Step: 193 Cumulative Rewards: -193.0\n",
            "Episode: 221 Time Step: 383 Cumulative Rewards: -383.0\n",
            "Episode: 222 Time Step: 278 Cumulative Rewards: -278.0\n",
            "Episode: 223 Time Step: 357 Cumulative Rewards: -357.0\n",
            "Episode: 224 Time Step: 296 Cumulative Rewards: -296.0\n",
            "Episode: 225 Time Step: 203 Cumulative Rewards: -203.0\n",
            "Episode: 226 Time Step: 266 Cumulative Rewards: -266.0\n",
            "Episode: 227 Time Step: 191 Cumulative Rewards: -191.0\n",
            "Episode: 228 Time Step: 180 Cumulative Rewards: -180.0\n",
            "Episode: 229 Time Step: 167 Cumulative Rewards: -167.0\n",
            "Episode: 230 Time Step: 309 Cumulative Rewards: -309.0\n",
            "Episode: 231 Time Step: 410 Cumulative Rewards: -410.0\n",
            "Episode: 232 Time Step: 279 Cumulative Rewards: -279.0\n",
            "Episode: 233 Time Step: 108 Cumulative Rewards: -108.0\n",
            "Episode: 234 Time Step: 170 Cumulative Rewards: -170.0\n",
            "Episode: 235 Time Step: 198 Cumulative Rewards: -198.0\n",
            "Episode: 236 Time Step: 298 Cumulative Rewards: -298.0\n",
            "Episode: 237 Time Step: 228 Cumulative Rewards: -228.0\n",
            "Episode: 238 Time Step: 318 Cumulative Rewards: -318.0\n",
            "Episode: 239 Time Step: 325 Cumulative Rewards: -325.0\n",
            "Episode: 240 Time Step: 163 Cumulative Rewards: -163.0\n",
            "Episode: 241 Time Step: 209 Cumulative Rewards: -209.0\n",
            "Episode: 242 Time Step: 351 Cumulative Rewards: -351.0\n",
            "Episode: 243 Time Step: 200 Cumulative Rewards: -200.0\n",
            "Episode: 244 Time Step: 321 Cumulative Rewards: -321.0\n",
            "Episode: 245 Time Step: 134 Cumulative Rewards: -134.0\n",
            "Episode: 246 Time Step: 466 Cumulative Rewards: -466.0\n",
            "Episode: 247 Time Step: 211 Cumulative Rewards: -211.0\n",
            "Episode: 248 Time Step: 348 Cumulative Rewards: -348.0\n",
            "Episode: 249 Time Step: 175 Cumulative Rewards: -175.0\n",
            "Episode: 250 Time Step: 141 Cumulative Rewards: -141.0\n",
            "Episode: 251 Time Step: 283 Cumulative Rewards: -283.0\n",
            "Episode: 252 Time Step: 190 Cumulative Rewards: -190.0\n",
            "Episode: 253 Time Step: 238 Cumulative Rewards: -238.0\n",
            "Episode: 254 Time Step: 312 Cumulative Rewards: -312.0\n",
            "Episode: 255 Time Step: 347 Cumulative Rewards: -347.0\n",
            "Episode: 256 Time Step: 499 Cumulative Rewards: -500.0\n",
            "Episode: 257 Time Step: 458 Cumulative Rewards: -458.0\n",
            "Episode: 258 Time Step: 247 Cumulative Rewards: -247.0\n",
            "Episode: 259 Time Step: 130 Cumulative Rewards: -130.0\n",
            "Episode: 260 Time Step: 234 Cumulative Rewards: -234.0\n",
            "Episode: 261 Time Step: 173 Cumulative Rewards: -173.0\n",
            "Episode: 262 Time Step: 366 Cumulative Rewards: -366.0\n",
            "Episode: 263 Time Step: 351 Cumulative Rewards: -351.0\n",
            "Episode: 264 Time Step: 233 Cumulative Rewards: -233.0\n",
            "Episode: 265 Time Step: 328 Cumulative Rewards: -328.0\n",
            "Episode: 266 Time Step: 120 Cumulative Rewards: -120.0\n",
            "Episode: 267 Time Step: 151 Cumulative Rewards: -151.0\n",
            "Episode: 268 Time Step: 177 Cumulative Rewards: -177.0\n",
            "Episode: 269 Time Step: 379 Cumulative Rewards: -379.0\n",
            "Episode: 270 Time Step: 151 Cumulative Rewards: -151.0\n",
            "Episode: 271 Time Step: 137 Cumulative Rewards: -137.0\n",
            "Episode: 272 Time Step: 183 Cumulative Rewards: -183.0\n",
            "Episode: 273 Time Step: 210 Cumulative Rewards: -210.0\n",
            "Episode: 274 Time Step: 205 Cumulative Rewards: -205.0\n",
            "Episode: 275 Time Step: 212 Cumulative Rewards: -212.0\n",
            "Episode: 276 Time Step: 208 Cumulative Rewards: -208.0\n",
            "Episode: 277 Time Step: 318 Cumulative Rewards: -318.0\n",
            "Episode: 278 Time Step: 225 Cumulative Rewards: -225.0\n",
            "Episode: 279 Time Step: 211 Cumulative Rewards: -211.0\n",
            "Episode: 280 Time Step: 247 Cumulative Rewards: -247.0\n",
            "Episode: 281 Time Step: 369 Cumulative Rewards: -369.0\n",
            "Episode: 282 Time Step: 321 Cumulative Rewards: -321.0\n",
            "Episode: 283 Time Step: 156 Cumulative Rewards: -156.0\n",
            "Episode: 284 Time Step: 270 Cumulative Rewards: -270.0\n",
            "Episode: 285 Time Step: 313 Cumulative Rewards: -313.0\n",
            "Episode: 286 Time Step: 267 Cumulative Rewards: -267.0\n",
            "Episode: 287 Time Step: 268 Cumulative Rewards: -268.0\n",
            "Episode: 288 Time Step: 212 Cumulative Rewards: -212.0\n",
            "Episode: 289 Time Step: 185 Cumulative Rewards: -185.0\n",
            "Episode: 290 Time Step: 135 Cumulative Rewards: -135.0\n",
            "Episode: 291 Time Step: 499 Cumulative Rewards: -500.0\n",
            "Episode: 292 Time Step: 263 Cumulative Rewards: -263.0\n",
            "Episode: 293 Time Step: 202 Cumulative Rewards: -202.0\n",
            "Episode: 294 Time Step: 281 Cumulative Rewards: -281.0\n",
            "Episode: 295 Time Step: 499 Cumulative Rewards: -500.0\n",
            "Episode: 296 Time Step: 236 Cumulative Rewards: -236.0\n",
            "Episode: 297 Time Step: 220 Cumulative Rewards: -220.0\n",
            "Episode: 298 Time Step: 296 Cumulative Rewards: -296.0\n",
            "Episode: 299 Time Step: 237 Cumulative Rewards: -237.0\n",
            "Episode: 300 Time Step: 269 Cumulative Rewards: -269.0\n",
            "Episode: 301 Time Step: 276 Cumulative Rewards: -276.0\n",
            "Episode: 302 Time Step: 286 Cumulative Rewards: -286.0\n",
            "Episode: 303 Time Step: 208 Cumulative Rewards: -208.0\n",
            "Episode: 304 Time Step: 180 Cumulative Rewards: -180.0\n",
            "Episode: 305 Time Step: 230 Cumulative Rewards: -230.0\n",
            "Episode: 306 Time Step: 437 Cumulative Rewards: -437.0\n",
            "Episode: 307 Time Step: 404 Cumulative Rewards: -404.0\n",
            "Episode: 308 Time Step: 201 Cumulative Rewards: -201.0\n",
            "Episode: 309 Time Step: 138 Cumulative Rewards: -138.0\n",
            "Episode: 310 Time Step: 145 Cumulative Rewards: -145.0\n",
            "Episode: 311 Time Step: 156 Cumulative Rewards: -156.0\n",
            "Episode: 312 Time Step: 287 Cumulative Rewards: -287.0\n",
            "Episode: 313 Time Step: 253 Cumulative Rewards: -253.0\n",
            "Episode: 314 Time Step: 144 Cumulative Rewards: -144.0\n",
            "Episode: 315 Time Step: 341 Cumulative Rewards: -341.0\n",
            "Episode: 316 Time Step: 244 Cumulative Rewards: -244.0\n",
            "Episode: 317 Time Step: 219 Cumulative Rewards: -219.0\n",
            "Episode: 318 Time Step: 127 Cumulative Rewards: -127.0\n",
            "Episode: 319 Time Step: 499 Cumulative Rewards: -500.0\n",
            "Episode: 320 Time Step: 123 Cumulative Rewards: -123.0\n",
            "Episode: 321 Time Step: 349 Cumulative Rewards: -349.0\n",
            "Episode: 322 Time Step: 199 Cumulative Rewards: -199.0\n",
            "Episode: 323 Time Step: 238 Cumulative Rewards: -238.0\n",
            "Episode: 324 Time Step: 284 Cumulative Rewards: -284.0\n",
            "Episode: 325 Time Step: 154 Cumulative Rewards: -154.0\n",
            "Episode: 326 Time Step: 182 Cumulative Rewards: -182.0\n",
            "Episode: 327 Time Step: 195 Cumulative Rewards: -195.0\n",
            "Episode: 328 Time Step: 373 Cumulative Rewards: -373.0\n",
            "Episode: 329 Time Step: 172 Cumulative Rewards: -172.0\n",
            "Episode: 330 Time Step: 189 Cumulative Rewards: -189.0\n",
            "Episode: 331 Time Step: 138 Cumulative Rewards: -138.0\n",
            "Episode: 332 Time Step: 202 Cumulative Rewards: -202.0\n",
            "Episode: 333 Time Step: 273 Cumulative Rewards: -273.0\n",
            "Episode: 334 Time Step: 253 Cumulative Rewards: -253.0\n",
            "Episode: 335 Time Step: 190 Cumulative Rewards: -190.0\n",
            "Episode: 336 Time Step: 206 Cumulative Rewards: -206.0\n",
            "Episode: 337 Time Step: 209 Cumulative Rewards: -209.0\n",
            "Episode: 338 Time Step: 194 Cumulative Rewards: -194.0\n",
            "Episode: 339 Time Step: 157 Cumulative Rewards: -157.0\n",
            "Episode: 340 Time Step: 147 Cumulative Rewards: -147.0\n",
            "Episode: 341 Time Step: 220 Cumulative Rewards: -220.0\n",
            "Episode: 342 Time Step: 215 Cumulative Rewards: -215.0\n",
            "Episode: 343 Time Step: 194 Cumulative Rewards: -194.0\n",
            "Episode: 344 Time Step: 172 Cumulative Rewards: -172.0\n",
            "Episode: 345 Time Step: 262 Cumulative Rewards: -262.0\n",
            "Episode: 346 Time Step: 175 Cumulative Rewards: -175.0\n",
            "Episode: 347 Time Step: 167 Cumulative Rewards: -167.0\n",
            "Episode: 348 Time Step: 262 Cumulative Rewards: -262.0\n",
            "Episode: 349 Time Step: 198 Cumulative Rewards: -198.0\n",
            "Episode: 350 Time Step: 326 Cumulative Rewards: -326.0\n",
            "Episode: 351 Time Step: 185 Cumulative Rewards: -185.0\n",
            "Episode: 352 Time Step: 233 Cumulative Rewards: -233.0\n",
            "Episode: 353 Time Step: 375 Cumulative Rewards: -375.0\n",
            "Episode: 354 Time Step: 465 Cumulative Rewards: -465.0\n",
            "Episode: 355 Time Step: 246 Cumulative Rewards: -246.0\n",
            "Episode: 356 Time Step: 373 Cumulative Rewards: -373.0\n",
            "Episode: 357 Time Step: 329 Cumulative Rewards: -329.0\n",
            "Episode: 358 Time Step: 151 Cumulative Rewards: -151.0\n",
            "Episode: 359 Time Step: 241 Cumulative Rewards: -241.0\n",
            "Episode: 360 Time Step: 468 Cumulative Rewards: -468.0\n",
            "Episode: 361 Time Step: 239 Cumulative Rewards: -239.0\n",
            "Episode: 362 Time Step: 255 Cumulative Rewards: -255.0\n",
            "Episode: 363 Time Step: 306 Cumulative Rewards: -306.0\n",
            "Episode: 364 Time Step: 330 Cumulative Rewards: -330.0\n",
            "Episode: 365 Time Step: 259 Cumulative Rewards: -259.0\n",
            "Episode: 366 Time Step: 379 Cumulative Rewards: -379.0\n",
            "Episode: 367 Time Step: 312 Cumulative Rewards: -312.0\n",
            "Episode: 368 Time Step: 212 Cumulative Rewards: -212.0\n",
            "Episode: 369 Time Step: 403 Cumulative Rewards: -403.0\n",
            "Episode: 370 Time Step: 313 Cumulative Rewards: -313.0\n",
            "Episode: 371 Time Step: 499 Cumulative Rewards: -500.0\n",
            "Episode: 372 Time Step: 202 Cumulative Rewards: -202.0\n",
            "Episode: 373 Time Step: 161 Cumulative Rewards: -161.0\n",
            "Episode: 374 Time Step: 291 Cumulative Rewards: -291.0\n",
            "Episode: 375 Time Step: 284 Cumulative Rewards: -284.0\n",
            "Episode: 376 Time Step: 227 Cumulative Rewards: -227.0\n",
            "Episode: 377 Time Step: 267 Cumulative Rewards: -267.0\n",
            "Episode: 378 Time Step: 381 Cumulative Rewards: -381.0\n",
            "Episode: 379 Time Step: 140 Cumulative Rewards: -140.0\n",
            "Episode: 380 Time Step: 169 Cumulative Rewards: -169.0\n",
            "Episode: 381 Time Step: 167 Cumulative Rewards: -167.0\n",
            "Episode: 382 Time Step: 263 Cumulative Rewards: -263.0\n",
            "Episode: 383 Time Step: 193 Cumulative Rewards: -193.0\n",
            "Episode: 384 Time Step: 164 Cumulative Rewards: -164.0\n",
            "Episode: 385 Time Step: 237 Cumulative Rewards: -237.0\n",
            "Episode: 386 Time Step: 365 Cumulative Rewards: -365.0\n",
            "Episode: 387 Time Step: 320 Cumulative Rewards: -320.0\n",
            "Episode: 388 Time Step: 189 Cumulative Rewards: -189.0\n",
            "Episode: 389 Time Step: 252 Cumulative Rewards: -252.0\n",
            "Episode: 390 Time Step: 203 Cumulative Rewards: -203.0\n",
            "Episode: 391 Time Step: 173 Cumulative Rewards: -173.0\n",
            "Episode: 392 Time Step: 144 Cumulative Rewards: -144.0\n",
            "Episode: 393 Time Step: 194 Cumulative Rewards: -194.0\n",
            "Episode: 394 Time Step: 170 Cumulative Rewards: -170.0\n",
            "Episode: 395 Time Step: 301 Cumulative Rewards: -301.0\n",
            "Episode: 396 Time Step: 235 Cumulative Rewards: -235.0\n",
            "Episode: 397 Time Step: 325 Cumulative Rewards: -325.0\n",
            "Episode: 398 Time Step: 203 Cumulative Rewards: -203.0\n",
            "Episode: 399 Time Step: 222 Cumulative Rewards: -222.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.009 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.082909…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0ac5511d1fb440c8db26d168a8f90ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>▇▄▃▇▄▆▇▇▅▆▅▅▆▇█▆▅▆▅▃▅▆▄▆▆▄▇▆▅▁▃▄▆▆▇▅▄▅▄▆</td></tr><tr><td>time_step</td><td>▂▅▆▂▅▃▂▂▄▃▄▄▃▂▁▃▄▃▄▆▄▃▅▃▃▅▂▃▄█▆▅▃▃▂▄▅▄▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>-222.0</td></tr><tr><td>time_step</td><td>222</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">acrobot_ddqn_400_64</strong>: <a href=\"https://wandb.ai/irl_team7/DQfD/runs/2z7u7q18\" target=\"_blank\">https://wandb.ai/irl_team7/DQfD/runs/2z7u7q18</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221211_071358-2z7u7q18/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -133.0,\n",
              " -125.0,\n",
              " -140.0,\n",
              " -295.0,\n",
              " -304.0,\n",
              " -440.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -310.0,\n",
              " -454.0,\n",
              " -279.0,\n",
              " -248.0,\n",
              " -301.0,\n",
              " -259.0,\n",
              " -454.0,\n",
              " -367.0,\n",
              " -181.0,\n",
              " -271.0,\n",
              " -500.0,\n",
              " -227.0,\n",
              " -391.0,\n",
              " -195.0,\n",
              " -359.0,\n",
              " -302.0,\n",
              " -500.0,\n",
              " -216.0,\n",
              " -274.0,\n",
              " -323.0,\n",
              " -140.0,\n",
              " -194.0,\n",
              " -198.0,\n",
              " -179.0,\n",
              " -197.0,\n",
              " -148.0,\n",
              " -214.0,\n",
              " -155.0,\n",
              " -164.0,\n",
              " -143.0,\n",
              " -197.0,\n",
              " -468.0,\n",
              " -344.0,\n",
              " -154.0,\n",
              " -242.0,\n",
              " -210.0,\n",
              " -162.0,\n",
              " -205.0,\n",
              " -228.0,\n",
              " -238.0,\n",
              " -207.0,\n",
              " -182.0,\n",
              " -246.0,\n",
              " -442.0,\n",
              " -249.0,\n",
              " -211.0,\n",
              " -263.0,\n",
              " -460.0,\n",
              " -117.0,\n",
              " -146.0,\n",
              " -131.0,\n",
              " -316.0,\n",
              " -187.0,\n",
              " -256.0,\n",
              " -368.0,\n",
              " -204.0,\n",
              " -396.0,\n",
              " -380.0,\n",
              " -229.0,\n",
              " -267.0,\n",
              " -171.0,\n",
              " -252.0,\n",
              " -259.0,\n",
              " -197.0,\n",
              " -340.0,\n",
              " -189.0,\n",
              " -304.0,\n",
              " -266.0,\n",
              " -244.0,\n",
              " -123.0,\n",
              " -179.0,\n",
              " -188.0,\n",
              " -260.0,\n",
              " -342.0,\n",
              " -273.0,\n",
              " -179.0,\n",
              " -295.0,\n",
              " -169.0,\n",
              " -192.0,\n",
              " -133.0,\n",
              " -196.0,\n",
              " -418.0,\n",
              " -255.0,\n",
              " -333.0,\n",
              " -227.0,\n",
              " -229.0,\n",
              " -233.0,\n",
              " -264.0,\n",
              " -407.0,\n",
              " -233.0,\n",
              " -350.0,\n",
              " -147.0,\n",
              " -297.0,\n",
              " -500.0,\n",
              " -286.0,\n",
              " -500.0,\n",
              " -216.0,\n",
              " -173.0,\n",
              " -424.0,\n",
              " -185.0,\n",
              " -281.0,\n",
              " -189.0,\n",
              " -267.0,\n",
              " -290.0,\n",
              " -370.0,\n",
              " -273.0,\n",
              " -162.0,\n",
              " -130.0,\n",
              " -254.0,\n",
              " -232.0,\n",
              " -236.0,\n",
              " -253.0,\n",
              " -189.0,\n",
              " -174.0,\n",
              " -154.0,\n",
              " -138.0,\n",
              " -219.0,\n",
              " -163.0,\n",
              " -210.0,\n",
              " -240.0,\n",
              " -268.0,\n",
              " -142.0,\n",
              " -204.0,\n",
              " -129.0,\n",
              " -184.0,\n",
              " -142.0,\n",
              " -201.0,\n",
              " -272.0,\n",
              " -228.0,\n",
              " -262.0,\n",
              " -364.0,\n",
              " -144.0,\n",
              " -104.0,\n",
              " -119.0,\n",
              " -157.0,\n",
              " -165.0,\n",
              " -133.0,\n",
              " -142.0,\n",
              " -190.0,\n",
              " -127.0,\n",
              " -273.0,\n",
              " -210.0,\n",
              " -142.0,\n",
              " -246.0,\n",
              " -206.0,\n",
              " -158.0,\n",
              " -249.0,\n",
              " -378.0,\n",
              " -253.0,\n",
              " -218.0,\n",
              " -174.0,\n",
              " -166.0,\n",
              " -269.0,\n",
              " -249.0,\n",
              " -227.0,\n",
              " -166.0,\n",
              " -180.0,\n",
              " -242.0,\n",
              " -129.0,\n",
              " -369.0,\n",
              " -375.0,\n",
              " -183.0,\n",
              " -173.0,\n",
              " -267.0,\n",
              " -196.0,\n",
              " -173.0,\n",
              " -240.0,\n",
              " -251.0,\n",
              " -137.0,\n",
              " -151.0,\n",
              " -141.0,\n",
              " -248.0,\n",
              " -289.0,\n",
              " -353.0,\n",
              " -178.0,\n",
              " -193.0,\n",
              " -288.0,\n",
              " -180.0,\n",
              " -126.0,\n",
              " -177.0,\n",
              " -219.0,\n",
              " -128.0,\n",
              " -200.0,\n",
              " -160.0,\n",
              " -393.0,\n",
              " -232.0,\n",
              " -121.0,\n",
              " -252.0,\n",
              " -415.0,\n",
              " -196.0,\n",
              " -203.0,\n",
              " -146.0,\n",
              " -282.0,\n",
              " -475.0,\n",
              " -190.0,\n",
              " -305.0,\n",
              " -340.0,\n",
              " -157.0,\n",
              " -244.0,\n",
              " -209.0,\n",
              " -310.0,\n",
              " -176.0,\n",
              " -205.0,\n",
              " -149.0,\n",
              " -233.0,\n",
              " -197.0,\n",
              " -211.0,\n",
              " -220.0,\n",
              " -240.0,\n",
              " -193.0,\n",
              " -383.0,\n",
              " -278.0,\n",
              " -357.0,\n",
              " -296.0,\n",
              " -203.0,\n",
              " -266.0,\n",
              " -191.0,\n",
              " -180.0,\n",
              " -167.0,\n",
              " -309.0,\n",
              " -410.0,\n",
              " -279.0,\n",
              " -108.0,\n",
              " -170.0,\n",
              " -198.0,\n",
              " -298.0,\n",
              " -228.0,\n",
              " -318.0,\n",
              " -325.0,\n",
              " -163.0,\n",
              " -209.0,\n",
              " -351.0,\n",
              " -200.0,\n",
              " -321.0,\n",
              " -134.0,\n",
              " -466.0,\n",
              " -211.0,\n",
              " -348.0,\n",
              " -175.0,\n",
              " -141.0,\n",
              " -283.0,\n",
              " -190.0,\n",
              " -238.0,\n",
              " -312.0,\n",
              " -347.0,\n",
              " -500.0,\n",
              " -458.0,\n",
              " -247.0,\n",
              " -130.0,\n",
              " -234.0,\n",
              " -173.0,\n",
              " -366.0,\n",
              " -351.0,\n",
              " -233.0,\n",
              " -328.0,\n",
              " -120.0,\n",
              " -151.0,\n",
              " -177.0,\n",
              " -379.0,\n",
              " -151.0,\n",
              " -137.0,\n",
              " -183.0,\n",
              " -210.0,\n",
              " -205.0,\n",
              " -212.0,\n",
              " -208.0,\n",
              " -318.0,\n",
              " -225.0,\n",
              " -211.0,\n",
              " -247.0,\n",
              " -369.0,\n",
              " -321.0,\n",
              " -156.0,\n",
              " -270.0,\n",
              " -313.0,\n",
              " -267.0,\n",
              " -268.0,\n",
              " -212.0,\n",
              " -185.0,\n",
              " -135.0,\n",
              " -500.0,\n",
              " -263.0,\n",
              " -202.0,\n",
              " -281.0,\n",
              " -500.0,\n",
              " -236.0,\n",
              " -220.0,\n",
              " -296.0,\n",
              " -237.0,\n",
              " -269.0,\n",
              " -276.0,\n",
              " -286.0,\n",
              " -208.0,\n",
              " -180.0,\n",
              " -230.0,\n",
              " -437.0,\n",
              " -404.0,\n",
              " -201.0,\n",
              " -138.0,\n",
              " -145.0,\n",
              " -156.0,\n",
              " -287.0,\n",
              " -253.0,\n",
              " -144.0,\n",
              " -341.0,\n",
              " -244.0,\n",
              " -219.0,\n",
              " -127.0,\n",
              " -500.0,\n",
              " -123.0,\n",
              " -349.0,\n",
              " -199.0,\n",
              " -238.0,\n",
              " -284.0,\n",
              " -154.0,\n",
              " -182.0,\n",
              " -195.0,\n",
              " -373.0,\n",
              " -172.0,\n",
              " -189.0,\n",
              " -138.0,\n",
              " -202.0,\n",
              " -273.0,\n",
              " -253.0,\n",
              " -190.0,\n",
              " -206.0,\n",
              " -209.0,\n",
              " -194.0,\n",
              " -157.0,\n",
              " -147.0,\n",
              " -220.0,\n",
              " -215.0,\n",
              " -194.0,\n",
              " -172.0,\n",
              " -262.0,\n",
              " -175.0,\n",
              " -167.0,\n",
              " -262.0,\n",
              " -198.0,\n",
              " -326.0,\n",
              " -185.0,\n",
              " -233.0,\n",
              " -375.0,\n",
              " -465.0,\n",
              " -246.0,\n",
              " -373.0,\n",
              " -329.0,\n",
              " -151.0,\n",
              " -241.0,\n",
              " -468.0,\n",
              " -239.0,\n",
              " -255.0,\n",
              " -306.0,\n",
              " -330.0,\n",
              " -259.0,\n",
              " -379.0,\n",
              " -312.0,\n",
              " -212.0,\n",
              " -403.0,\n",
              " -313.0,\n",
              " -500.0,\n",
              " -202.0,\n",
              " -161.0,\n",
              " -291.0,\n",
              " -284.0,\n",
              " -227.0,\n",
              " -267.0,\n",
              " -381.0,\n",
              " -140.0,\n",
              " -169.0,\n",
              " -167.0,\n",
              " -263.0,\n",
              " -193.0,\n",
              " -164.0,\n",
              " -237.0,\n",
              " -365.0,\n",
              " -320.0,\n",
              " -189.0,\n",
              " -252.0,\n",
              " -203.0,\n",
              " -173.0,\n",
              " -144.0,\n",
              " -194.0,\n",
              " -170.0,\n",
              " -301.0,\n",
              " -235.0,\n",
              " -325.0,\n",
              " -203.0,\n",
              " -222.0]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying different annealing"
      ],
      "metadata": {
        "id": "Ou56t9D35Opy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dqn = DQN_Torch(state_size, action_size)\n",
        "\n",
        "# expert_demos = pickle.load(open(PATH+'project/Sai_folder/code/demo_data/expert_demonstrations', 'rb'))\n",
        "# dqn.demo_buffer = expert_demos\n",
        "\n",
        "# dqn.pre_train()\n",
        "\n",
        "# # DDQN at the end of 400 iterations, 100 epochs with batch size 256 for pre training\n",
        "# pickle.dump(dqn.main_network.state_dict(), open(PATH+'project/Sai_folder/code/experiment_runtime_log/pre_trained_network_400_100_256', 'wb'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXxN4-VA791S",
        "outputId": "40b2ff46-60d6-478a-b533-3c22db6be62f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-training ...\n",
            "Total Avg Reward = 9.34\n",
            "10th step of pre-train finish ...\n",
            "Total Avg Reward = 12.3\n",
            "20th step of pre-train finish ...\n",
            "Total Avg Reward = 19.44\n",
            "30th step of pre-train finish ...\n",
            "Total Avg Reward = 22.06\n",
            "40th step of pre-train finish ...\n",
            "Total Avg Reward = 29.1\n",
            "50th step of pre-train finish ...\n",
            "Total Avg Reward = 34.03\n",
            "60th step of pre-train finish ...\n",
            "Total Avg Reward = 35.62\n",
            "70th step of pre-train finish ...\n",
            "Total Avg Reward = 40.36\n",
            "80th step of pre-train finish ...\n",
            "Total Avg Reward = 46.64\n",
            "90th step of pre-train finish ...\n",
            "All pre-train finish.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for k in [1]: ## k is the seed value\n",
        "  # Annealing Type - Exponential\n",
        "  for sample_rate in [0.25, 0.5]:\n",
        "    for ec in [0.001, 0.01, 0.1]:\n",
        "      run_DQfD(DQN_Torch(state_size, action_size), anneal_type=\"exponential\", initial_sample_percent = float(sample_rate), expo_const = float(ec), seed_val = int(k))\n",
        "\n",
        "  # Annealing Type - Constant\n",
        "  for sample_rate in [0, 0.25, 0.5, 0.75, 1.0]:\n",
        "    run_DQfD(DQN_Torch(state_size, action_size), anneal_type=\"constant\", initial_sample_percent = float(sample_rate), seed_val = int(k))\n",
        "\n",
        "  # Annealing Type - Linear\n",
        "  for sample_rate in [0.25, 0.5]:\n",
        "    for dr in [0.01, 0.005, 0.0025]:\n",
        "      run_DQfD(DQN_Torch(state_size, action_size), anneal_type=\"linear\", initial_sample_percent = float(sample_rate), decay_rate = float(dr), seed_val = int(k))\n",
        "\n",
        "  # Annealing Type - Threshold\n",
        "  for sample_rate in [0.25, 0.5]:\n",
        "    for reward_threshold in [100, 200, 300]:\n",
        "      run_DQfD(DQN_Torch(state_size, action_size), anneal_type=\"threshold\", initial_sample_percent = float(sample_rate), anneal_threshold = float(reward_threshold), seed_val = int(k))"
      ],
      "metadata": {
        "id": "xYEJNotW8Aqi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "52980aaae5424035bbb7887060469d91",
            "aac4700733bb4a9889bf3938438abcb7",
            "5eef4ab1c4a142c683180bc8247d04e8",
            "25abe19eb71e4e3ca0eb7c3407d3c3f2",
            "7b8a447367c0416ca54d46ffc09a1114",
            "a0c85e8d7f0d4dc794bd1f6da0680ffc",
            "f7c664d4182c4a359dadbd3eb8690eb1",
            "7524e69f4bee4e7ca97e4a683d4006cd",
            "8620b6163c56445ca1d5132b6961f5eb",
            "446f76996f0a4f6a92f3dbfe55e236ef",
            "304c0e54dd874822b5bd2448d08f0eca",
            "07ff86770ec547559a5866332c22dfc4",
            "32e125cf25004649a1de15ee0bc1f373",
            "47d4ec9325e147089341c2406ae371f5",
            "9c0518d98f95415b9dafea13089da73d",
            "62617cf2e0344885a4a2b0a832664155",
            "528e67742d5443b989b39506b837797f",
            "0135e7827aed48ec8a71bb40706eaa54",
            "218349f8ee504c2ba623e989688cf6a2",
            "a9c2e43fa04a4177bc0e3d1ea1672a35",
            "e02982aa2c3e45a3a2c48f328ba0d7dc",
            "f19a9eca41964c3e98b4c84a0c0368a5",
            "81ae052ab12246fd8bcf78bf2462e053",
            "dd0158a235284fd780904c1b1de1a530",
            "3db9fdbc3f2642e19a2873b532c38b97",
            "78472df39aca4073bb8470a50b281f56",
            "1e4641640ae24fbaa25bad7b0bcc75c2",
            "170c6c1677ca4089bdf940a1bc401e61",
            "f1f38496bd844f9a90c546f9716a0516",
            "91a7efb2c93a4d4aae743cfbf12ef25f",
            "f98d1572bf6d4836b69f81e9e0f36d39",
            "a6dc470a9b444f279db138b595d312e2",
            "cd6fe0822e3140619ab0b764e3036f90",
            "7bad6c2d5ef14853b90722c06619c259",
            "b344a200c4fb43d494cad8f07ab7435e",
            "53d40e506baf49e1960e3ff62acb08f5",
            "df417aba6c964d9e8f10ef32da63e312",
            "76ad62dc78624a6f877ff2f039eef6e8",
            "6ae25e27052040d6ae4c495b63884e94",
            "e14ac8c3d4b74a99a571a16750b1ef7d",
            "91a7fd04776047fdadaf43a2035d41bf",
            "7084213d2e584175944c5c58f9106d51",
            "20b8b00663ba497981cf547caddf9aca",
            "f587296a53e1411cbc7861e742fad411",
            "d113ccc2c969425e96ae393facecba9c",
            "4820eb2db866495a9b6733e4f4c22129",
            "6b148d23d8b549d8bae2d4aeb379a6c7",
            "bbacabf125fe438ba3d39e9fed4a5741",
            "f2d07d782b0946e8899663778ffdfa86",
            "c0fcca7e8a424310bcc1b1f02362fdf9",
            "b6f5a379432f470c9ee9104bea69219d",
            "c02b275c9eb24697a2d53c227449cb30",
            "63c97284fc4e44efbb20161fcf8e7158",
            "34cb159f32af4950bd8e8b1828e48ec6",
            "a6129ec9b9294e1ca760ad6f21f9a3b9",
            "ab89fb32062a4b2a83f27c4d18621432",
            "571f1088186d4c91a12d03fdad4897a9",
            "b3cd332a37284323a261356d5bdcfabf",
            "7da1d7fcfd7141dd947a35684262f0a4",
            "2d6164c531a445c8b2548df7e407620e",
            "120a72d33943498fa7d7a8bcb6d7bac3",
            "31b4328a8ddc4ec6ad47e5db0bf45155",
            "66c980c6cf4742f698e01f5d682c6881",
            "0f5a8e10ea1049e2927808671cba2308",
            "55fbe2834d3047c19951ab154bd640d6",
            "db858be869ae4e70bb22c03dca08ea4b",
            "13906cd6ac6142e9a9a2d8866f5e14fa",
            "0fdb1d18d62846b68b11a020aa952fe8",
            "ae89578db6ec49ba8687e6cca1a95986",
            "dc2e6f1ee9694dcc9107487c75358b5e",
            "3471f0eb3e7d4414b1761d091ff82f75",
            "a037d0e20bef4d9baff26cdcaa99bd5f",
            "2e802090af4c464998709f9dadc1af9e",
            "fcfeeba10343484195fd8ecb4884e758",
            "cf2b68dcdf41438e8572c657d5284d82",
            "70fb174019e546b0a2defb84f8ba832c",
            "5b75fe3aa80d40baa82e206523169089",
            "7289ee46fa2c447f8263cbb24a1d2a57",
            "569dba25d3d94bc0a0e1e394db9f10d7",
            "0c51befecf9b437baa4441cc987ea953",
            "b595e40c328c4b9394e564d1cdd59d26",
            "21b4e4544a3042e8b61ab665b400999e",
            "6ad3a1f2bce54d2ea5cfd3f5cdc5a373",
            "5f06b3475c644bbe875c47cd2160ce77",
            "a1aed698964c4be080f707eacd16371b",
            "1c6c61e2010343e792d20639aa52038b",
            "6eaff9334f31497395a4cd0691f41678",
            "5e017dbc39f4408691e9bc36d58215d1"
          ]
        },
        "outputId": "d7d54d67-04ac-4575-dae8-4a60932ad38c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msaiprasath3344\u001b[0m (\u001b[33mirl_team7\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of <state,action> pairs in the demonstrator data = 10000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221210_161212-2ofww3to</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/irl_team7/DQfD/runs/2ofww3to\" target=\"_blank\">cartpole_exponential_0.25_0.001_1</a></strong> to <a href=\"https://wandb.ai/irl_team7/DQfD\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-training ...\n",
            "All pre-train finish.\n",
            "Annealing: exponential, Demo Sampling Percent: 0.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0 Time Step: 98 Cumulative Rewards: 99.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24975012495834376\n",
            "Episode: 1 Time Step: 48 Cumulative Rewards: 49.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24950049966683327\n",
            "Episode: 2 Time Step: 37 Cumulative Rewards: 38.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24925112387584325\n",
            "Episode: 3 Time Step: 34 Cumulative Rewards: 35.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24900199733599787\n",
            "Episode: 4 Time Step: 45 Cumulative Rewards: 46.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24875311979817058\n",
            "Episode: 5 Time Step: 47 Cumulative Rewards: 48.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24850449101348382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 6 Time Step: 63 Cumulative Rewards: 64.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24825611073330878\n",
            "Episode: 7 Time Step: 23 Cumulative Rewards: 24.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24800797870926516\n",
            "Episode: 8 Time Step: 29 Cumulative Rewards: 30.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2477600946932209\n",
            "Episode: 9 Time Step: 35 Cumulative Rewards: 36.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24751245843729203\n",
            "Episode: 10 Time Step: 25 Cumulative Rewards: 26.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24726506969384218\n",
            "Episode: 11 Time Step: 42 Cumulative Rewards: 43.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24701792821548263\n",
            "Episode: 12 Time Step: 35 Cumulative Rewards: 36.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2467710337550719\n",
            "Episode: 13 Time Step: 30 Cumulative Rewards: 31.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24652438606571547\n",
            "Episode: 14 Time Step: 27 Cumulative Rewards: 28.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24627798490076566\n",
            "Episode: 15 Time Step: 60 Cumulative Rewards: 61.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24603183001382128\n",
            "Episode: 16 Time Step: 57 Cumulative Rewards: 58.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2457859211587274\n",
            "Episode: 17 Time Step: 27 Cumulative Rewards: 28.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2455402580895752\n",
            "Episode: 18 Time Step: 41 Cumulative Rewards: 42.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2452948405607015\n",
            "Episode: 19 Time Step: 47 Cumulative Rewards: 48.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2450496683266888\n",
            "Episode: 20 Time Step: 38 Cumulative Rewards: 39.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2448047411423649\n",
            "Episode: 21 Time Step: 35 Cumulative Rewards: 36.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2445600587628025\n",
            "Episode: 22 Time Step: 32 Cumulative Rewards: 33.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24431562094331927\n",
            "Episode: 23 Time Step: 59 Cumulative Rewards: 60.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24407142743947732\n",
            "Episode: 24 Time Step: 41 Cumulative Rewards: 42.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24382747800708315\n",
            "Episode: 25 Time Step: 39 Cumulative Rewards: 40.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24358377240218734\n",
            "Episode: 26 Time Step: 54 Cumulative Rewards: 55.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2433403103810842\n",
            "Episode: 27 Time Step: 90 Cumulative Rewards: 91.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2430970917003117\n",
            "Episode: 28 Time Step: 56 Cumulative Rewards: 57.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2428541161166512\n",
            "Episode: 29 Time Step: 49 Cumulative Rewards: 50.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24261138338712704\n",
            "Episode: 30 Time Step: 49 Cumulative Rewards: 50.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24236889326900649\n",
            "Episode: 31 Time Step: 89 Cumulative Rewards: 90.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2421266455197994\n",
            "Episode: 32 Time Step: 74 Cumulative Rewards: 75.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.241884639897258\n",
            "Episode: 33 Time Step: 57 Cumulative Rewards: 58.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24164287615937666\n",
            "Episode: 34 Time Step: 64 Cumulative Rewards: 65.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24140135406439162\n",
            "Episode: 35 Time Step: 106 Cumulative Rewards: 107.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24116007337078077\n",
            "Episode: 36 Time Step: 82 Cumulative Rewards: 83.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24091903383726337\n",
            "Episode: 37 Time Step: 77 Cumulative Rewards: 78.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24067823522279988\n",
            "Episode: 38 Time Step: 154 Cumulative Rewards: 155.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24043767728659168\n",
            "Episode: 39 Time Step: 195 Cumulative Rewards: 196.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2401973597880808\n",
            "Episode: 40 Time Step: 148 Cumulative Rewards: 149.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23995728248694972\n",
            "Episode: 41 Time Step: 149 Cumulative Rewards: 150.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23971744514312113\n",
            "Episode: 42 Time Step: 356 Cumulative Rewards: 357.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23947784751675766\n",
            "Episode: 43 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23923848936826167\n",
            "Episode: 44 Time Step: 207 Cumulative Rewards: 208.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.238999370458275\n",
            "Episode: 45 Time Step: 190 Cumulative Rewards: 191.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23876049054767867\n",
            "Episode: 46 Time Step: 239 Cumulative Rewards: 240.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23852184939759277\n",
            "Episode: 47 Time Step: 180 Cumulative Rewards: 181.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23828344676937618\n",
            "Episode: 48 Time Step: 239 Cumulative Rewards: 240.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23804528242462622\n",
            "Episode: 49 Time Step: 219 Cumulative Rewards: 220.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2378073561251785\n",
            "Episode: 50 Time Step: 155 Cumulative Rewards: 156.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23756966763310675\n",
            "Episode: 51 Time Step: 241 Cumulative Rewards: 242.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23733221671072238\n",
            "Episode: 52 Time Step: 207 Cumulative Rewards: 208.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23709500312057455\n",
            "Episode: 53 Time Step: 199 Cumulative Rewards: 200.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23685802662544958\n",
            "Episode: 54 Time Step: 164 Cumulative Rewards: 165.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23662128698837095\n",
            "Episode: 55 Time Step: 182 Cumulative Rewards: 183.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23638478397259907\n",
            "Episode: 56 Time Step: 147 Cumulative Rewards: 148.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23614851734163084\n",
            "Episode: 57 Time Step: 182 Cumulative Rewards: 183.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23591248685919963\n",
            "Episode: 58 Time Step: 171 Cumulative Rewards: 172.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23567669228927493\n",
            "Episode: 59 Time Step: 200 Cumulative Rewards: 201.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23544113339606218\n",
            "Episode: 60 Time Step: 157 Cumulative Rewards: 158.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23520580994400242\n",
            "Episode: 61 Time Step: 208 Cumulative Rewards: 209.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23497072169777222\n",
            "Episode: 62 Time Step: 215 Cumulative Rewards: 216.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2347358684222833\n",
            "Episode: 63 Time Step: 178 Cumulative Rewards: 179.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23450124988268237\n",
            "Episode: 64 Time Step: 210 Cumulative Rewards: 211.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23426686584435086\n",
            "Episode: 65 Time Step: 219 Cumulative Rewards: 220.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2340327160729047\n",
            "Episode: 66 Time Step: 263 Cumulative Rewards: 264.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23379880033419415\n",
            "Episode: 67 Time Step: 202 Cumulative Rewards: 203.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23356511839430338\n",
            "Episode: 68 Time Step: 210 Cumulative Rewards: 211.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2333316700195505\n",
            "Episode: 69 Time Step: 248 Cumulative Rewards: 249.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23309845497648707\n",
            "Episode: 70 Time Step: 215 Cumulative Rewards: 216.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23286547303189803\n",
            "Episode: 71 Time Step: 162 Cumulative Rewards: 163.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23263272395280143\n",
            "Episode: 72 Time Step: 190 Cumulative Rewards: 191.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2324002075064482\n",
            "Episode: 73 Time Step: 383 Cumulative Rewards: 384.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2321679234603218\n",
            "Episode: 74 Time Step: 160 Cumulative Rewards: 161.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23193587158213821\n",
            "Episode: 75 Time Step: 196 Cumulative Rewards: 197.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23170405163984556\n",
            "Episode: 76 Time Step: 317 Cumulative Rewards: 318.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23147246340162383\n",
            "Episode: 77 Time Step: 185 Cumulative Rewards: 186.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23124110663588482\n",
            "Episode: 78 Time Step: 239 Cumulative Rewards: 240.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2310099811112717\n",
            "Episode: 79 Time Step: 185 Cumulative Rewards: 186.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23077908659665894\n",
            "Episode: 80 Time Step: 193 Cumulative Rewards: 194.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.230548422861152\n",
            "Episode: 81 Time Step: 233 Cumulative Rewards: 234.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23031798967408718\n",
            "Episode: 82 Time Step: 186 Cumulative Rewards: 187.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23008778680503117\n",
            "Episode: 83 Time Step: 201 Cumulative Rewards: 202.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22985781402378117\n",
            "Episode: 84 Time Step: 219 Cumulative Rewards: 220.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22962807110036434\n",
            "Episode: 85 Time Step: 205 Cumulative Rewards: 206.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22939855780503773\n",
            "Episode: 86 Time Step: 257 Cumulative Rewards: 258.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22916927390828806\n",
            "Episode: 87 Time Step: 195 Cumulative Rewards: 196.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2289402191808314\n",
            "Episode: 88 Time Step: 303 Cumulative Rewards: 304.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.228711393393613\n",
            "Episode: 89 Time Step: 209 Cumulative Rewards: 210.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22848279631780705\n",
            "Episode: 90 Time Step: 283 Cumulative Rewards: 284.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22825442772481644\n",
            "Episode: 91 Time Step: 234 Cumulative Rewards: 235.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2280262873862726\n",
            "Episode: 92 Time Step: 375 Cumulative Rewards: 376.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22779837507403514\n",
            "Episode: 93 Time Step: 255 Cumulative Rewards: 256.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22757069056019175\n",
            "Episode: 94 Time Step: 295 Cumulative Rewards: 296.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22734323361705785\n",
            "Episode: 95 Time Step: 226 Cumulative Rewards: 227.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22711600401717655\n",
            "Episode: 96 Time Step: 224 Cumulative Rewards: 225.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22688900153331817\n",
            "Episode: 97 Time Step: 362 Cumulative Rewards: 363.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22666222593848023\n",
            "Episode: 98 Time Step: 245 Cumulative Rewards: 246.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22643567700588713\n",
            "Episode: 99 Time Step: 447 Cumulative Rewards: 448.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>▂▁▁▁▁▁▂▁▁▁▁▂▁▁▂▃▃█▃▄▃▄▃▃▃▃▅▄▃▆▅▄▄▄▄▅▄▄▄▇</td></tr><tr><td>time_step</td><td>▂▁▁▁▁▁▂▁▁▁▁▂▁▁▂▃▃█▃▄▃▄▃▃▃▃▅▄▃▆▅▄▄▄▄▅▄▄▄▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>448.0</td></tr><tr><td>time_step</td><td>447</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">cartpole_exponential_0.25_0.001_1</strong>: <a href=\"https://wandb.ai/irl_team7/DQfD/runs/2ofww3to\" target=\"_blank\">https://wandb.ai/irl_team7/DQfD/runs/2ofww3to</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221210_161212-2ofww3to/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA...\n",
            "Number of <state,action> pairs in the demonstrator data = 10000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221210_162046-1ugo4033</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/irl_team7/DQfD/runs/1ugo4033\" target=\"_blank\">cartpole_exponential_0.25_0.01_1</a></strong> to <a href=\"https://wandb.ai/irl_team7/DQfD\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-training ...\n",
            "All pre-train finish.\n",
            "Annealing: exponential, Demo Sampling Percent: 0.25\n",
            "Episode: 0 Time Step: 37 Cumulative Rewards: 38.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24751245843729203\n",
            "Episode: 1 Time Step: 42 Cumulative Rewards: 43.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2450496683266888\n",
            "Episode: 2 Time Step: 36 Cumulative Rewards: 37.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24261138338712704\n",
            "Episode: 3 Time Step: 80 Cumulative Rewards: 81.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2401973597880808\n",
            "Episode: 4 Time Step: 35 Cumulative Rewards: 36.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2378073561251785\n",
            "Episode: 5 Time Step: 29 Cumulative Rewards: 30.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23544113339606218\n",
            "Episode: 6 Time Step: 40 Cumulative Rewards: 41.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23309845497648707\n",
            "Episode: 7 Time Step: 30 Cumulative Rewards: 31.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23077908659665894\n",
            "Episode: 8 Time Step: 41 Cumulative Rewards: 42.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22848279631780705\n",
            "Episode: 9 Time Step: 26 Cumulative Rewards: 27.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22620935450898988\n",
            "Episode: 10 Time Step: 33 Cumulative Rewards: 34.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22395853382413206\n",
            "Episode: 11 Time Step: 35 Cumulative Rewards: 36.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22173010917928937\n",
            "Episode: 12 Time Step: 58 Cumulative Rewards: 59.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.21952385773014033\n",
            "Episode: 13 Time Step: 34 Cumulative Rewards: 35.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.21733955884970146\n",
            "Episode: 14 Time Step: 57 Cumulative Rewards: 58.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.21517699410626445\n",
            "Episode: 15 Time Step: 84 Cumulative Rewards: 85.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.21303594724155284\n",
            "Episode: 16 Time Step: 80 Cumulative Rewards: 81.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.21091620414909593\n",
            "Episode: 17 Time Step: 39 Cumulative Rewards: 40.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.208817552852818\n",
            "Episode: 18 Time Step: 38 Cumulative Rewards: 39.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.20673978348584057\n",
            "Episode: 19 Time Step: 32 Cumulative Rewards: 33.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.20468268826949546\n",
            "Episode: 20 Time Step: 48 Cumulative Rewards: 49.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.20264606149254677\n",
            "Episode: 21 Time Step: 102 Cumulative Rewards: 103.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.20062969949061962\n",
            "Episode: 22 Time Step: 84 Cumulative Rewards: 85.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.1986334006258335\n",
            "Episode: 23 Time Step: 47 Cumulative Rewards: 48.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.19665696526663837\n",
            "Episode: 24 Time Step: 49 Cumulative Rewards: 50.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.19470019576785122\n",
            "Episode: 25 Time Step: 92 Cumulative Rewards: 93.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.19276289645089156\n",
            "Episode: 26 Time Step: 48 Cumulative Rewards: 49.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.1908448735842133\n",
            "Episode: 27 Time Step: 53 Cumulative Rewards: 54.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.18894593536393137\n",
            "Episode: 28 Time Step: 94 Cumulative Rewards: 95.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.18706589189464132\n",
            "Episode: 29 Time Step: 97 Cumulative Rewards: 98.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.18520455517042947\n",
            "Episode: 30 Time Step: 68 Cumulative Rewards: 69.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.1833617390560723\n",
            "Episode: 31 Time Step: 132 Cumulative Rewards: 133.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.18153725926842273\n",
            "Episode: 32 Time Step: 174 Cumulative Rewards: 175.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.17973093335798154\n",
            "Episode: 33 Time Step: 58 Cumulative Rewards: 59.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.1779425806906524\n",
            "Episode: 34 Time Step: 164 Cumulative Rewards: 165.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.17617202242967836\n",
            "Episode: 35 Time Step: 9 Cumulative Rewards: 10.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.17441908151775776\n",
            "Episode: 36 Time Step: 8 Cumulative Rewards: 9.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.17268358265933867\n",
            "Episode: 37 Time Step: 8 Cumulative Rewards: 9.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.17096535230308896\n",
            "Episode: 38 Time Step: 9 Cumulative Rewards: 10.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.16926421862454116\n",
            "Episode: 39 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.16758001150890983\n",
            "Episode: 40 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.16591256253407985\n",
            "Episode: 41 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.1642617049537642\n",
            "Episode: 42 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.16262727368082913\n",
            "Episode: 43 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.16100910527078535\n",
            "Episode: 44 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.15940703790544333\n",
            "Episode: 45 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.1578209113767315\n",
            "Episode: 46 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.1562505670706752\n",
            "Episode: 47 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.1546958479515352\n",
            "Episode: 48 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.15315659854610403\n",
            "Episode: 49 Time Step: 264 Cumulative Rewards: 265.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.15163266492815836\n",
            "Episode: 50 Time Step: 399 Cumulative Rewards: 400.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.15012389470306647\n",
            "Episode: 51 Time Step: 217 Cumulative Rewards: 218.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.1486301369925486\n",
            "Episode: 52 Time Step: 330 Cumulative Rewards: 331.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.1471512424195888\n",
            "Episode: 53 Time Step: 371 Cumulative Rewards: 372.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.1456870630934974\n",
            "Episode: 54 Time Step: 257 Cumulative Rewards: 258.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.14423745259512166\n",
            "Episode: 55 Time Step: 171 Cumulative Rewards: 172.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.14280226596220372\n",
            "Episode: 56 Time Step: 281 Cumulative Rewards: 282.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.14138135967488427\n",
            "Episode: 57 Time Step: 304 Cumulative Rewards: 305.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.1399745916413505\n",
            "Episode: 58 Time Step: 292 Cumulative Rewards: 293.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.13858182118362677\n",
            "Episode: 59 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.13720290902350663\n",
            "Episode: 60 Time Step: 282 Cumulative Rewards: 283.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.13583771726862495\n",
            "Episode: 61 Time Step: 86 Cumulative Rewards: 87.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.13448610939866862\n",
            "Episode: 62 Time Step: 274 Cumulative Rewards: 275.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.1331479502517243\n",
            "Episode: 63 Time Step: 250 Cumulative Rewards: 251.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.13182310601076214\n",
            "Episode: 64 Time Step: 294 Cumulative Rewards: 295.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.130511444190254\n",
            "Episode: 65 Time Step: 354 Cumulative Rewards: 355.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.1292128336229248\n",
            "Episode: 66 Time Step: 265 Cumulative Rewards: 266.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.1279271444466356\n",
            "Episode: 67 Time Step: 260 Cumulative Rewards: 261.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.1266542480913974\n",
            "Episode: 68 Time Step: 291 Cumulative Rewards: 292.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.12539401726651386\n",
            "Episode: 69 Time Step: 281 Cumulative Rewards: 282.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.12414632594785237\n",
            "Episode: 70 Time Step: 470 Cumulative Rewards: 471.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.12291104936524128\n",
            "Episode: 71 Time Step: 322 Cumulative Rewards: 323.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.12168806398999292\n",
            "Episode: 72 Time Step: 111 Cumulative Rewards: 112.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.12047724752255061\n",
            "Episode: 73 Time Step: 338 Cumulative Rewards: 339.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.1192784788802586\n",
            "Episode: 74 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.11809163818525367\n",
            "Episode: 75 Time Step: 331 Cumulative Rewards: 332.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.11691660675247731\n",
            "Episode: 76 Time Step: 326 Cumulative Rewards: 327.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.11575326707780702\n",
            "Episode: 77 Time Step: 378 Cumulative Rewards: 379.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.11460150282630588\n",
            "Episode: 78 Time Step: 324 Cumulative Rewards: 325.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.11346119882058896\n",
            "Episode: 79 Time Step: 351 Cumulative Rewards: 352.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.11233224102930539\n",
            "Episode: 80 Time Step: 390 Cumulative Rewards: 391.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.11121451655573528\n",
            "Episode: 81 Time Step: 354 Cumulative Rewards: 355.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.11010791362649981\n",
            "Episode: 82 Time Step: 312 Cumulative Rewards: 313.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.10901232158038389\n",
            "Episode: 83 Time Step: 284 Cumulative Rewards: 285.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.10792763085726993\n",
            "Episode: 84 Time Step: 60 Cumulative Rewards: 61.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.10685373298718168\n",
            "Episode: 85 Time Step: 352 Cumulative Rewards: 353.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.1057905205794372\n",
            "Episode: 86 Time Step: 306 Cumulative Rewards: 307.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.10473788731190975\n",
            "Episode: 87 Time Step: 257 Cumulative Rewards: 258.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.10369572792039534\n",
            "Episode: 88 Time Step: 195 Cumulative Rewards: 196.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.10266393818808638\n",
            "Episode: 89 Time Step: 388 Cumulative Rewards: 389.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.10164241493514978\n",
            "Episode: 90 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.10063105600840899\n",
            "Episode: 91 Time Step: 372 Cumulative Rewards: 373.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.09962976027112853\n",
            "Episode: 92 Time Step: 425 Cumulative Rewards: 426.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.09863842759290027\n",
            "Episode: 93 Time Step: 257 Cumulative Rewards: 258.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.09765695883963027\n",
            "Episode: 94 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0966852558636253\n",
            "Episode: 95 Time Step: 215 Cumulative Rewards: 216.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.09572322149377802\n",
            "Episode: 96 Time Step: 253 Cumulative Rewards: 254.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.09477075952584971\n",
            "Episode: 97 Time Step: 303 Cumulative Rewards: 304.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.09382777471284989\n",
            "Episode: 98 Time Step: 317 Cumulative Rewards: 318.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.09289417275551143\n",
            "Episode: 99 Time Step: 342 Cumulative Rewards: 343.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52980aaae5424035bbb7887060469d91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>▁▁▁▁▁▂▂▁▂▂▂▂▂▂▁▁████▇▆▃▅▅▄▅▅▅▆▆▅▆▅▅▄▆▅▄▆</td></tr><tr><td>time_step</td><td>▁▁▁▁▁▂▂▁▂▂▂▂▂▂▁▁████▇▆▃▅▅▄▅▅▅▆▆▅▆▅▅▄▆▅▄▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>343.0</td></tr><tr><td>time_step</td><td>342</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">cartpole_exponential_0.25_0.01_1</strong>: <a href=\"https://wandb.ai/irl_team7/DQfD/runs/1ugo4033\" target=\"_blank\">https://wandb.ai/irl_team7/DQfD/runs/1ugo4033</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221210_162046-1ugo4033/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA...\n",
            "Number of <state,action> pairs in the demonstrator data = 10000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221210_163217-b65hxmtu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/irl_team7/DQfD/runs/b65hxmtu\" target=\"_blank\">cartpole_exponential_0.25_0.1_1</a></strong> to <a href=\"https://wandb.ai/irl_team7/DQfD\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-training ...\n",
            "All pre-train finish.\n",
            "Annealing: exponential, Demo Sampling Percent: 0.25\n",
            "Episode: 0 Time Step: 48 Cumulative Rewards: 49.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22620935450898988\n",
            "Episode: 1 Time Step: 27 Cumulative Rewards: 28.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.20468268826949546\n",
            "Episode: 2 Time Step: 35 Cumulative Rewards: 36.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.18520455517042947\n",
            "Episode: 3 Time Step: 65 Cumulative Rewards: 66.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.16758001150890983\n",
            "Episode: 4 Time Step: 45 Cumulative Rewards: 46.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.15163266492815836\n",
            "Episode: 5 Time Step: 36 Cumulative Rewards: 37.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.1372029090235066\n",
            "Episode: 6 Time Step: 21 Cumulative Rewards: 22.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.12414632594785237\n",
            "Episode: 7 Time Step: 25 Cumulative Rewards: 26.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.11233224102930539\n",
            "Episode: 8 Time Step: 22 Cumulative Rewards: 23.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.10164241493514978\n",
            "Episode: 9 Time Step: 28 Cumulative Rewards: 29.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.09196986029286058\n",
            "Episode: 10 Time Step: 36 Cumulative Rewards: 37.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.08321777092451989\n",
            "Episode: 11 Time Step: 32 Cumulative Rewards: 33.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0752985529780505\n",
            "Episode: 12 Time Step: 24 Cumulative Rewards: 25.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.06813294825850315\n",
            "Episode: 13 Time Step: 21 Cumulative Rewards: 22.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.06164924098540161\n",
            "Episode: 14 Time Step: 25 Cumulative Rewards: 26.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.055782540037107455\n",
            "Episode: 15 Time Step: 20 Cumulative Rewards: 21.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.050474129498663846\n",
            "Episode: 16 Time Step: 20 Cumulative Rewards: 21.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.04567088101318365\n",
            "Episode: 17 Time Step: 19 Cumulative Rewards: 20.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.04132472205539663\n",
            "Episode: 18 Time Step: 35 Cumulative Rewards: 36.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.03739215480565876\n",
            "Episode: 19 Time Step: 22 Cumulative Rewards: 23.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.033833820809153176\n",
            "Episode: 20 Time Step: 35 Cumulative Rewards: 36.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.030614107063245476\n",
            "Episode: 21 Time Step: 24 Cumulative Rewards: 25.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.027700789590583468\n",
            "Episode: 22 Time Step: 26 Cumulative Rewards: 27.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.025064710930700927\n",
            "Episode: 23 Time Step: 22 Cumulative Rewards: 23.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.022679488322353118\n",
            "Episode: 24 Time Step: 19 Cumulative Rewards: 20.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0205212496559747\n",
            "Episode: 25 Time Step: 24 Cumulative Rewards: 25.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.01856839455358347\n",
            "Episode: 26 Time Step: 29 Cumulative Rewards: 30.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.01680137818493744\n",
            "Episode: 27 Time Step: 55 Cumulative Rewards: 56.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.015202515656304488\n",
            "Episode: 28 Time Step: 61 Cumulative Rewards: 62.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.013755805014101802\n",
            "Episode: 29 Time Step: 33 Cumulative Rewards: 34.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.012446767091965986\n",
            "Episode: 30 Time Step: 44 Cumulative Rewards: 45.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.01126230059838945\n",
            "Episode: 31 Time Step: 34 Cumulative Rewards: 35.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.010190550994591553\n",
            "Episode: 32 Time Step: 50 Cumulative Rewards: 51.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.009220791850309999\n",
            "Episode: 33 Time Step: 78 Cumulative Rewards: 79.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.008343317490081516\n",
            "Episode: 34 Time Step: 71 Cumulative Rewards: 72.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.007549345855579625\n",
            "Episode: 35 Time Step: 44 Cumulative Rewards: 45.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.00683093061182314\n",
            "Episode: 36 Time Step: 132 Cumulative Rewards: 133.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.006180881617584847\n",
            "Episode: 37 Time Step: 49 Cumulative Rewards: 50.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.005592692964041398\n",
            "Episode: 38 Time Step: 139 Cumulative Rewards: 140.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.005060477861451095\n",
            "Episode: 39 Time Step: 119 Cumulative Rewards: 120.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.004578909722183545\n",
            "Episode: 40 Time Step: 230 Cumulative Rewards: 231.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.004143168850440309\n",
            "Episode: 41 Time Step: 249 Cumulative Rewards: 250.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.003748894205119426\n",
            "Episode: 42 Time Step: 232 Cumulative Rewards: 233.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0033921397530502335\n",
            "Episode: 43 Time Step: 225 Cumulative Rewards: 226.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.003069334975767109\n",
            "Episode: 44 Time Step: 461 Cumulative Rewards: 462.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0027772491345605765\n",
            "Episode: 45 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.002512958936158394\n",
            "Episode: 46 Time Step: 225 Cumulative Rewards: 226.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.002273819275423954\n",
            "Episode: 47 Time Step: 184 Cumulative Rewards: 185.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.002057436762255006\n",
            "Episode: 48 Time Step: 192 Cumulative Rewards: 193.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0018616457677310845\n",
            "Episode: 49 Time Step: 190 Cumulative Rewards: 191.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0016844867497713668\n",
            "Episode: 50 Time Step: 153 Cumulative Rewards: 154.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0015241866413789082\n",
            "Episode: 51 Time Step: 164 Cumulative Rewards: 165.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0013791411051901929\n",
            "Episode: 52 Time Step: 171 Cumulative Rewards: 172.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0012478984767275532\n",
            "Episode: 53 Time Step: 195 Cumulative Rewards: 196.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0011291452356531665\n",
            "Episode: 54 Time Step: 248 Cumulative Rewards: 249.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0010216928596160166\n",
            "Episode: 55 Time Step: 168 Cumulative Rewards: 169.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0009244659291207322\n",
            "Episode: 56 Time Step: 171 Cumulative Rewards: 172.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.000836491364367818\n",
            "Episode: 57 Time Step: 168 Cumulative Rewards: 169.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0007568886863439532\n",
            "Episode: 58 Time Step: 169 Cumulative Rewards: 170.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0006848612046920921\n",
            "Episode: 59 Time Step: 169 Cumulative Rewards: 170.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0006196880441665896\n",
            "Episode: 60 Time Step: 166 Cumulative Rewards: 167.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0005607169298714503\n",
            "Episode: 61 Time Step: 180 Cumulative Rewards: 181.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0005073576590739335\n",
            "Episode: 62 Time Step: 178 Cumulative Rewards: 179.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0004590761942572264\n",
            "Episode: 63 Time Step: 197 Cumulative Rewards: 198.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0004153893182934835\n",
            "Episode: 64 Time Step: 220 Cumulative Rewards: 221.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0003758597982443931\n",
            "Episode: 65 Time Step: 182 Cumulative Rewards: 183.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0003400920093869732\n",
            "Episode: 66 Time Step: 155 Cumulative Rewards: 156.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.00030772797566837025\n",
            "Episode: 67 Time Step: 192 Cumulative Rewards: 193.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0002784437869612006\n",
            "Episode: 68 Time Step: 165 Cumulative Rewards: 166.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0002519463572621276\n",
            "Episode: 69 Time Step: 179 Cumulative Rewards: 180.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.00022797049138862906\n",
            "Episode: 70 Time Step: 166 Cumulative Rewards: 167.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.00020627623081647596\n",
            "Episode: 71 Time Step: 150 Cumulative Rewards: 151.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0001866464520941698\n",
            "Episode: 72 Time Step: 164 Cumulative Rewards: 165.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.00016888469379846093\n",
            "Episode: 73 Time Step: 168 Cumulative Rewards: 169.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.00015281319028239308\n",
            "Episode: 74 Time Step: 160 Cumulative Rewards: 161.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0001382710925369584\n",
            "Episode: 75 Time Step: 166 Cumulative Rewards: 167.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0001251128583601526\n",
            "Episode: 76 Time Step: 129 Cumulative Rewards: 130.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.00011320679572169924\n",
            "Episode: 77 Time Step: 149 Cumulative Rewards: 150.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.00010243374474494661\n",
            "Episode: 78 Time Step: 182 Cumulative Rewards: 183.0\n",
            "Annealing: exponential, Demo Sampling Percent: 9.268588511477206e-05\n",
            "Episode: 79 Time Step: 139 Cumulative Rewards: 140.0\n",
            "Annealing: exponential, Demo Sampling Percent: 8.386565697562796e-05\n",
            "Episode: 80 Time Step: 143 Cumulative Rewards: 144.0\n",
            "Annealing: exponential, Demo Sampling Percent: 7.58847845197167e-05\n",
            "Episode: 81 Time Step: 146 Cumulative Rewards: 147.0\n",
            "Annealing: exponential, Demo Sampling Percent: 6.866339249303551e-05\n",
            "Episode: 82 Time Step: 147 Cumulative Rewards: 148.0\n",
            "Annealing: exponential, Demo Sampling Percent: 6.212920677698796e-05\n",
            "Episode: 83 Time Step: 175 Cumulative Rewards: 176.0\n",
            "Annealing: exponential, Demo Sampling Percent: 5.621683104471205e-05\n",
            "Episode: 84 Time Step: 161 Cumulative Rewards: 162.0\n",
            "Annealing: exponential, Demo Sampling Percent: 5.086709225266104e-05\n",
            "Episode: 85 Time Step: 146 Cumulative Rewards: 147.0\n",
            "Annealing: exponential, Demo Sampling Percent: 4.60264484168948e-05\n",
            "Episode: 86 Time Step: 134 Cumulative Rewards: 135.0\n",
            "Annealing: exponential, Demo Sampling Percent: 4.164645274690831e-05\n",
            "Episode: 87 Time Step: 151 Cumulative Rewards: 152.0\n",
            "Annealing: exponential, Demo Sampling Percent: 3.7683268773869126e-05\n",
            "Episode: 88 Time Step: 156 Cumulative Rewards: 157.0\n",
            "Annealing: exponential, Demo Sampling Percent: 3.409723162050285e-05\n",
            "Episode: 89 Time Step: 129 Cumulative Rewards: 130.0\n",
            "Annealing: exponential, Demo Sampling Percent: 3.085245102166989e-05\n",
            "Episode: 90 Time Step: 154 Cumulative Rewards: 155.0\n",
            "Annealing: exponential, Demo Sampling Percent: 2.7916452122528694e-05\n",
            "Episode: 91 Time Step: 176 Cumulative Rewards: 177.0\n",
            "Annealing: exponential, Demo Sampling Percent: 2.525985045927331e-05\n",
            "Episode: 92 Time Step: 164 Cumulative Rewards: 165.0\n",
            "Annealing: exponential, Demo Sampling Percent: 2.2856057869543317e-05\n",
            "Episode: 93 Time Step: 205 Cumulative Rewards: 206.0\n",
            "Annealing: exponential, Demo Sampling Percent: 2.0681016389158057e-05\n",
            "Episode: 94 Time Step: 242 Cumulative Rewards: 243.0\n",
            "Annealing: exponential, Demo Sampling Percent: 1.871295747192515e-05\n",
            "Episode: 95 Time Step: 186 Cumulative Rewards: 187.0\n",
            "Annealing: exponential, Demo Sampling Percent: 1.6932184122713444e-05\n",
            "Episode: 96 Time Step: 146 Cumulative Rewards: 147.0\n",
            "Annealing: exponential, Demo Sampling Percent: 1.5320873763305506e-05\n",
            "Episode: 97 Time Step: 194 Cumulative Rewards: 195.0\n",
            "Annealing: exponential, Demo Sampling Percent: 1.3862899858044236e-05\n",
            "Episode: 98 Time Step: 149 Cumulative Rewards: 150.0\n",
            "Annealing: exponential, Demo Sampling Percent: 1.254367051404382e-05\n",
            "Episode: 99 Time Step: 190 Cumulative Rewards: 191.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▃▄▄█▄▃▄▃▃▃▄▃▃▃▃▃▃▃▃▃▃▃▄▃▃</td></tr><tr><td>time_step</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▃▄▄█▄▃▄▃▃▃▄▃▃▃▃▃▃▃▃▃▃▃▄▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>191.0</td></tr><tr><td>time_step</td><td>190</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">cartpole_exponential_0.25_0.1_1</strong>: <a href=\"https://wandb.ai/irl_team7/DQfD/runs/b65hxmtu\" target=\"_blank\">https://wandb.ai/irl_team7/DQfD/runs/b65hxmtu</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221210_163217-b65hxmtu/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA...\n",
            "Number of <state,action> pairs in the demonstrator data = 10000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221210_163819-23of3vps</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/irl_team7/DQfD/runs/23of3vps\" target=\"_blank\">cartpole_exponential_0.5_0.001_1</a></strong> to <a href=\"https://wandb.ai/irl_team7/DQfD\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-training ...\n",
            "All pre-train finish.\n",
            "Annealing: exponential, Demo Sampling Percent: 0.5\n",
            "Episode: 0 Time Step: 36 Cumulative Rewards: 37.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4995002499166875\n",
            "Episode: 1 Time Step: 61 Cumulative Rewards: 62.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.49900099933366654\n",
            "Episode: 2 Time Step: 53 Cumulative Rewards: 54.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4985022477516865\n",
            "Episode: 3 Time Step: 57 Cumulative Rewards: 58.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.49800399467199574\n",
            "Episode: 4 Time Step: 32 Cumulative Rewards: 33.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.49750623959634116\n",
            "Episode: 5 Time Step: 32 Cumulative Rewards: 33.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.49700898202696764\n",
            "Episode: 6 Time Step: 32 Cumulative Rewards: 33.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.49651222146661755\n",
            "Episode: 7 Time Step: 31 Cumulative Rewards: 32.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.49601595741853033\n",
            "Episode: 8 Time Step: 21 Cumulative Rewards: 22.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4955201893864418\n",
            "Episode: 9 Time Step: 21 Cumulative Rewards: 22.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.49502491687458405\n",
            "Episode: 10 Time Step: 23 Cumulative Rewards: 24.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.49453013938768436\n",
            "Episode: 11 Time Step: 27 Cumulative Rewards: 28.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.49403585643096526\n",
            "Episode: 12 Time Step: 30 Cumulative Rewards: 31.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4935420675101438\n",
            "Episode: 13 Time Step: 52 Cumulative Rewards: 53.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.49304877213143095\n",
            "Episode: 14 Time Step: 23 Cumulative Rewards: 24.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4925559698015313\n",
            "Episode: 15 Time Step: 25 Cumulative Rewards: 26.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.49206366002764257\n",
            "Episode: 16 Time Step: 27 Cumulative Rewards: 28.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4915718423174548\n",
            "Episode: 17 Time Step: 25 Cumulative Rewards: 26.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4910805161791504\n",
            "Episode: 18 Time Step: 43 Cumulative Rewards: 44.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.490589681121403\n",
            "Episode: 19 Time Step: 27 Cumulative Rewards: 28.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4900993366533776\n",
            "Episode: 20 Time Step: 52 Cumulative Rewards: 53.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4896094822847298\n",
            "Episode: 21 Time Step: 67 Cumulative Rewards: 68.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.489120117525605\n",
            "Episode: 22 Time Step: 22 Cumulative Rewards: 23.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.48863124188663853\n",
            "Episode: 23 Time Step: 29 Cumulative Rewards: 30.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.48814285487895465\n",
            "Episode: 24 Time Step: 56 Cumulative Rewards: 57.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4876549560141663\n",
            "Episode: 25 Time Step: 141 Cumulative Rewards: 142.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4871675448043747\n",
            "Episode: 26 Time Step: 54 Cumulative Rewards: 55.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4866806207621684\n",
            "Episode: 27 Time Step: 76 Cumulative Rewards: 77.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4861941834006234\n",
            "Episode: 28 Time Step: 36 Cumulative Rewards: 37.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4857082322333024\n",
            "Episode: 29 Time Step: 49 Cumulative Rewards: 50.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4852227667742541\n",
            "Episode: 30 Time Step: 33 Cumulative Rewards: 34.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.48473778653801297\n",
            "Episode: 31 Time Step: 31 Cumulative Rewards: 32.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4842532910395988\n",
            "Episode: 32 Time Step: 49 Cumulative Rewards: 50.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.483769279794516\n",
            "Episode: 33 Time Step: 58 Cumulative Rewards: 59.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4832857523187533\n",
            "Episode: 34 Time Step: 51 Cumulative Rewards: 52.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.48280270812878323\n",
            "Episode: 35 Time Step: 58 Cumulative Rewards: 59.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.48232014674156154\n",
            "Episode: 36 Time Step: 190 Cumulative Rewards: 191.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.48183806767452675\n",
            "Episode: 37 Time Step: 39 Cumulative Rewards: 40.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.48135647044559976\n",
            "Episode: 38 Time Step: 39 Cumulative Rewards: 40.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.48087535457318337\n",
            "Episode: 39 Time Step: 32 Cumulative Rewards: 33.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4803947195761616\n",
            "Episode: 40 Time Step: 42 Cumulative Rewards: 43.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.47991456497389945\n",
            "Episode: 41 Time Step: 29 Cumulative Rewards: 30.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.47943489028624225\n",
            "Episode: 42 Time Step: 39 Cumulative Rewards: 40.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4789556950335153\n",
            "Episode: 43 Time Step: 35 Cumulative Rewards: 36.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.47847697873652334\n",
            "Episode: 44 Time Step: 26 Cumulative Rewards: 27.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.47799874091655\n",
            "Episode: 45 Time Step: 18 Cumulative Rewards: 19.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.47752098109535734\n",
            "Episode: 46 Time Step: 25 Cumulative Rewards: 26.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.47704369879518554\n",
            "Episode: 47 Time Step: 76 Cumulative Rewards: 77.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.47656689353875237\n",
            "Episode: 48 Time Step: 42 Cumulative Rewards: 43.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.47609056484925244\n",
            "Episode: 49 Time Step: 89 Cumulative Rewards: 90.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.475614712250357\n",
            "Episode: 50 Time Step: 41 Cumulative Rewards: 42.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4751393352662135\n",
            "Episode: 51 Time Step: 64 Cumulative Rewards: 65.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.47466443342144476\n",
            "Episode: 52 Time Step: 62 Cumulative Rewards: 63.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4741900062411491\n",
            "Episode: 53 Time Step: 99 Cumulative Rewards: 100.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.47371605325089916\n",
            "Episode: 54 Time Step: 66 Cumulative Rewards: 67.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4732425739767419\n",
            "Episode: 55 Time Step: 78 Cumulative Rewards: 79.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.47276956794519814\n",
            "Episode: 56 Time Step: 94 Cumulative Rewards: 95.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4722970346832617\n",
            "Episode: 57 Time Step: 153 Cumulative Rewards: 154.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.47182497371839927\n",
            "Episode: 58 Time Step: 92 Cumulative Rewards: 93.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.47135338457854986\n",
            "Episode: 59 Time Step: 105 Cumulative Rewards: 106.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.47088226679212436\n",
            "Episode: 60 Time Step: 131 Cumulative Rewards: 132.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.47041161988800484\n",
            "Episode: 61 Time Step: 112 Cumulative Rewards: 113.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.46994144339554444\n",
            "Episode: 62 Time Step: 154 Cumulative Rewards: 155.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4694717368445666\n",
            "Episode: 63 Time Step: 158 Cumulative Rewards: 159.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.46900249976536473\n",
            "Episode: 64 Time Step: 175 Cumulative Rewards: 176.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4685337316887017\n",
            "Episode: 65 Time Step: 121 Cumulative Rewards: 122.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4680654321458094\n",
            "Episode: 66 Time Step: 116 Cumulative Rewards: 117.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4675976006683883\n",
            "Episode: 67 Time Step: 186 Cumulative Rewards: 187.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.46713023678860677\n",
            "Episode: 68 Time Step: 183 Cumulative Rewards: 184.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.466663340039101\n",
            "Episode: 69 Time Step: 205 Cumulative Rewards: 206.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.46619690995297414\n",
            "Episode: 70 Time Step: 161 Cumulative Rewards: 162.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.46573094606379606\n",
            "Episode: 71 Time Step: 163 Cumulative Rewards: 164.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.46526544790560287\n",
            "Episode: 72 Time Step: 242 Cumulative Rewards: 243.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4648004150128964\n",
            "Episode: 73 Time Step: 233 Cumulative Rewards: 234.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4643358469206436\n",
            "Episode: 74 Time Step: 163 Cumulative Rewards: 164.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.46387174316427643\n",
            "Episode: 75 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4634081032796911\n",
            "Episode: 76 Time Step: 209 Cumulative Rewards: 210.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.46294492680324767\n",
            "Episode: 77 Time Step: 162 Cumulative Rewards: 163.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.46248221327176964\n",
            "Episode: 78 Time Step: 246 Cumulative Rewards: 247.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4620199622225434\n",
            "Episode: 79 Time Step: 229 Cumulative Rewards: 230.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4615581731933179\n",
            "Episode: 80 Time Step: 275 Cumulative Rewards: 276.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.461096845722304\n",
            "Episode: 81 Time Step: 313 Cumulative Rewards: 314.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.46063597934817435\n",
            "Episode: 82 Time Step: 352 Cumulative Rewards: 353.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.46017557361006234\n",
            "Episode: 83 Time Step: 281 Cumulative Rewards: 282.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.45971562804756233\n",
            "Episode: 84 Time Step: 350 Cumulative Rewards: 351.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4592561422007287\n",
            "Episode: 85 Time Step: 207 Cumulative Rewards: 208.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.45879711561007547\n",
            "Episode: 86 Time Step: 316 Cumulative Rewards: 317.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4583385478165761\n",
            "Episode: 87 Time Step: 219 Cumulative Rewards: 220.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4578804383616628\n",
            "Episode: 88 Time Step: 275 Cumulative Rewards: 276.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.457422786787226\n",
            "Episode: 89 Time Step: 226 Cumulative Rewards: 227.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4569655926356141\n",
            "Episode: 90 Time Step: 362 Cumulative Rewards: 363.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4565088554496329\n",
            "Episode: 91 Time Step: 249 Cumulative Rewards: 250.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4560525747725452\n",
            "Episode: 92 Time Step: 325 Cumulative Rewards: 326.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4555967501480703\n",
            "Episode: 93 Time Step: 330 Cumulative Rewards: 331.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4551413811203835\n",
            "Episode: 94 Time Step: 329 Cumulative Rewards: 330.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4546864672341157\n",
            "Episode: 95 Time Step: 288 Cumulative Rewards: 289.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4542320080343531\n",
            "Episode: 96 Time Step: 242 Cumulative Rewards: 243.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.45377800306663635\n",
            "Episode: 97 Time Step: 248 Cumulative Rewards: 249.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.45332445187696047\n",
            "Episode: 98 Time Step: 310 Cumulative Rewards: 311.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.45287135401177425\n",
            "Episode: 99 Time Step: 219 Cumulative Rewards: 220.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8620b6163c56445ca1d5132b6961f5eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>▁▂▁▁▁▁▁▁▂▁▄▂▁▂▂▁▂▁▁▂▂▃▂▃▄▄▃▅▄▆▅▆█▇█▇▆█▆▆</td></tr><tr><td>time_step</td><td>▁▂▁▁▁▁▁▁▂▁▄▂▁▂▂▁▂▁▁▂▂▃▂▃▄▄▃▅▄▆▅▆█▇█▇▆█▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>220.0</td></tr><tr><td>time_step</td><td>219</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">cartpole_exponential_0.5_0.001_1</strong>: <a href=\"https://wandb.ai/irl_team7/DQfD/runs/23of3vps\" target=\"_blank\">https://wandb.ai/irl_team7/DQfD/runs/23of3vps</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221210_163819-23of3vps/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA...\n",
            "Number of <state,action> pairs in the demonstrator data = 10000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221210_164559-1mcx9oe6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/irl_team7/DQfD/runs/1mcx9oe6\" target=\"_blank\">cartpole_exponential_0.5_0.01_1</a></strong> to <a href=\"https://wandb.ai/irl_team7/DQfD\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-training ...\n",
            "All pre-train finish.\n",
            "Annealing: exponential, Demo Sampling Percent: 0.5\n",
            "Episode: 0 Time Step: 73 Cumulative Rewards: 74.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.49502491687458405\n",
            "Episode: 1 Time Step: 39 Cumulative Rewards: 40.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4900993366533776\n",
            "Episode: 2 Time Step: 52 Cumulative Rewards: 53.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4852227667742541\n",
            "Episode: 3 Time Step: 53 Cumulative Rewards: 54.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4803947195761616\n",
            "Episode: 4 Time Step: 32 Cumulative Rewards: 33.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.475614712250357\n",
            "Episode: 5 Time Step: 42 Cumulative Rewards: 43.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.47088226679212436\n",
            "Episode: 6 Time Step: 23 Cumulative Rewards: 24.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.46619690995297414\n",
            "Episode: 7 Time Step: 32 Cumulative Rewards: 33.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4615581731933179\n",
            "Episode: 8 Time Step: 25 Cumulative Rewards: 26.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4569655926356141\n",
            "Episode: 9 Time Step: 21 Cumulative Rewards: 22.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.45241870901797976\n",
            "Episode: 10 Time Step: 27 Cumulative Rewards: 28.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4479170676482641\n",
            "Episode: 11 Time Step: 26 Cumulative Rewards: 27.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.44346021835857874\n",
            "Episode: 12 Time Step: 33 Cumulative Rewards: 34.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.43904771546028065\n",
            "Episode: 13 Time Step: 26 Cumulative Rewards: 27.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.43467911769940293\n",
            "Episode: 14 Time Step: 27 Cumulative Rewards: 28.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4303539882125289\n",
            "Episode: 15 Time Step: 38 Cumulative Rewards: 39.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4260718944831057\n",
            "Episode: 16 Time Step: 42 Cumulative Rewards: 43.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.42183240829819185\n",
            "Episode: 17 Time Step: 35 Cumulative Rewards: 36.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.417635105705636\n",
            "Episode: 18 Time Step: 27 Cumulative Rewards: 28.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.41347956697168115\n",
            "Episode: 19 Time Step: 69 Cumulative Rewards: 70.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4093653765389909\n",
            "Episode: 20 Time Step: 51 Cumulative Rewards: 52.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.40529212298509354\n",
            "Episode: 21 Time Step: 27 Cumulative Rewards: 28.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.40125939898123925\n",
            "Episode: 22 Time Step: 35 Cumulative Rewards: 36.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.397266801251667\n",
            "Episode: 23 Time Step: 55 Cumulative Rewards: 56.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.39331393053327673\n",
            "Episode: 24 Time Step: 31 Cumulative Rewards: 32.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.38940039153570244\n",
            "Episode: 25 Time Step: 135 Cumulative Rewards: 136.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.3855257929017831\n",
            "Episode: 26 Time Step: 55 Cumulative Rewards: 56.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.3816897471684266\n",
            "Episode: 27 Time Step: 65 Cumulative Rewards: 66.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.37789187072786273\n",
            "Episode: 28 Time Step: 52 Cumulative Rewards: 53.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.37413178378928263\n",
            "Episode: 29 Time Step: 180 Cumulative Rewards: 181.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.37040911034085894\n",
            "Episode: 30 Time Step: 142 Cumulative Rewards: 143.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.3667234781121446\n",
            "Episode: 31 Time Step: 104 Cumulative Rewards: 105.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.36307451853684547\n",
            "Episode: 32 Time Step: 76 Cumulative Rewards: 77.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.3594618667159631\n",
            "Episode: 33 Time Step: 113 Cumulative Rewards: 114.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.3558851613813048\n",
            "Episode: 34 Time Step: 291 Cumulative Rewards: 292.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.3523440448593567\n",
            "Episode: 35 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.3488381630355155\n",
            "Episode: 36 Time Step: 146 Cumulative Rewards: 147.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.34536716531867734\n",
            "Episode: 37 Time Step: 310 Cumulative Rewards: 311.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.3419307046061779\n",
            "Episode: 38 Time Step: 493 Cumulative Rewards: 494.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.3385284372490823\n",
            "Episode: 39 Time Step: 204 Cumulative Rewards: 205.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.33516002301781966\n",
            "Episode: 40 Time Step: 208 Cumulative Rewards: 209.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.3318251250681597\n",
            "Episode: 41 Time Step: 401 Cumulative Rewards: 402.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.3285234099075284\n",
            "Episode: 42 Time Step: 254 Cumulative Rewards: 255.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.32525454736165826\n",
            "Episode: 43 Time Step: 157 Cumulative Rewards: 158.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.3220182105415707\n",
            "Episode: 44 Time Step: 433 Cumulative Rewards: 434.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.31881407581088667\n",
            "Episode: 45 Time Step: 225 Cumulative Rewards: 226.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.315641822753463\n",
            "Episode: 46 Time Step: 161 Cumulative Rewards: 162.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.3125011341413504\n",
            "Episode: 47 Time Step: 161 Cumulative Rewards: 162.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.3093916959030704\n",
            "Episode: 48 Time Step: 150 Cumulative Rewards: 151.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.30631319709220806\n",
            "Episode: 49 Time Step: 147 Cumulative Rewards: 148.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.3032653298563167\n",
            "Episode: 50 Time Step: 156 Cumulative Rewards: 157.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.30024778940613295\n",
            "Episode: 51 Time Step: 173 Cumulative Rewards: 174.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2972602739850972\n",
            "Episode: 52 Time Step: 179 Cumulative Rewards: 180.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2943024848391776\n",
            "Episode: 53 Time Step: 150 Cumulative Rewards: 151.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2913741261869948\n",
            "Episode: 54 Time Step: 177 Cumulative Rewards: 178.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2884749051902433\n",
            "Episode: 55 Time Step: 251 Cumulative Rewards: 252.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.28560453192440743\n",
            "Episode: 56 Time Step: 160 Cumulative Rewards: 161.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.28276271934976854\n",
            "Episode: 57 Time Step: 210 Cumulative Rewards: 211.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.279949183282701\n",
            "Episode: 58 Time Step: 230 Cumulative Rewards: 231.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.27716364236725355\n",
            "Episode: 59 Time Step: 177 Cumulative Rewards: 178.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.27440581804701325\n",
            "Episode: 60 Time Step: 194 Cumulative Rewards: 195.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2716754345372499\n",
            "Episode: 61 Time Step: 157 Cumulative Rewards: 158.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.26897221879733724\n",
            "Episode: 62 Time Step: 184 Cumulative Rewards: 185.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2662959005034486\n",
            "Episode: 63 Time Step: 181 Cumulative Rewards: 182.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2636462120215243\n",
            "Episode: 64 Time Step: 160 Cumulative Rewards: 161.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.261022888380508\n",
            "Episode: 65 Time Step: 199 Cumulative Rewards: 200.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2584256672458496\n",
            "Episode: 66 Time Step: 162 Cumulative Rewards: 163.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2558542888932712\n",
            "Episode: 67 Time Step: 272 Cumulative Rewards: 273.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2533084961827948\n",
            "Episode: 68 Time Step: 293 Cumulative Rewards: 294.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2507880345330277\n",
            "Episode: 69 Time Step: 180 Cumulative Rewards: 181.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24829265189570474\n",
            "Episode: 70 Time Step: 200 Cumulative Rewards: 201.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24582209873048255\n",
            "Episode: 71 Time Step: 225 Cumulative Rewards: 226.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24337612797998584\n",
            "Episode: 72 Time Step: 189 Cumulative Rewards: 190.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24095449504510122\n",
            "Episode: 73 Time Step: 278 Cumulative Rewards: 279.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2385569577605172\n",
            "Episode: 74 Time Step: 323 Cumulative Rewards: 324.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23618327637050734\n",
            "Episode: 75 Time Step: 276 Cumulative Rewards: 277.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23383321350495462\n",
            "Episode: 76 Time Step: 201 Cumulative Rewards: 202.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.23150653415561404\n",
            "Episode: 77 Time Step: 388 Cumulative Rewards: 389.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22920300565261176\n",
            "Episode: 78 Time Step: 229 Cumulative Rewards: 230.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22692239764117791\n",
            "Episode: 79 Time Step: 332 Cumulative Rewards: 333.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22466448205861078\n",
            "Episode: 80 Time Step: 220 Cumulative Rewards: 221.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22242903311147055\n",
            "Episode: 81 Time Step: 260 Cumulative Rewards: 261.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22021582725299962\n",
            "Episode: 82 Time Step: 345 Cumulative Rewards: 346.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.21802464316076778\n",
            "Episode: 83 Time Step: 264 Cumulative Rewards: 265.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.21585526171453986\n",
            "Episode: 84 Time Step: 280 Cumulative Rewards: 281.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.21370746597436335\n",
            "Episode: 85 Time Step: 328 Cumulative Rewards: 329.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2115810411588744\n",
            "Episode: 86 Time Step: 425 Cumulative Rewards: 426.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2094757746238195\n",
            "Episode: 87 Time Step: 236 Cumulative Rewards: 237.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2073914558407907\n",
            "Episode: 88 Time Step: 137 Cumulative Rewards: 138.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.20532787637617275\n",
            "Episode: 89 Time Step: 61 Cumulative Rewards: 62.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.20328482987029955\n",
            "Episode: 90 Time Step: 407 Cumulative Rewards: 408.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.20126211201681798\n",
            "Episode: 91 Time Step: 379 Cumulative Rewards: 380.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.19925952054225707\n",
            "Episode: 92 Time Step: 397 Cumulative Rewards: 398.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.19727685518580054\n",
            "Episode: 93 Time Step: 409 Cumulative Rewards: 410.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.19531391767926054\n",
            "Episode: 94 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.1933705117272506\n",
            "Episode: 95 Time Step: 374 Cumulative Rewards: 375.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.19144644298755603\n",
            "Episode: 96 Time Step: 432 Cumulative Rewards: 433.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.18954151905169941\n",
            "Episode: 97 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.18765554942569979\n",
            "Episode: 98 Time Step: 488 Cumulative Rewards: 489.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.18578834551102286\n",
            "Episode: 99 Time Step: 264 Cumulative Rewards: 265.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "528e67742d5443b989b39506b837797f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>▂▁▁▁▁▁▁▁▁▁▃▂▃▂██▄▃▄▃▃▃▄▄▃▃▃▅▄▅▄▄▄▅▇▃▆▇▇▅</td></tr><tr><td>time_step</td><td>▂▁▁▁▁▁▁▁▁▁▃▂▃▂██▄▃▄▃▃▃▄▄▃▃▃▅▄▅▄▄▄▅▇▃▆▇▇▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>265.0</td></tr><tr><td>time_step</td><td>264</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">cartpole_exponential_0.5_0.01_1</strong>: <a href=\"https://wandb.ai/irl_team7/DQfD/runs/1mcx9oe6\" target=\"_blank\">https://wandb.ai/irl_team7/DQfD/runs/1mcx9oe6</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221210_164559-1mcx9oe6/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA...\n",
            "Number of <state,action> pairs in the demonstrator data = 10000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221210_165622-3dbg2bon</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/irl_team7/DQfD/runs/3dbg2bon\" target=\"_blank\">cartpole_exponential_0.5_0.1_1</a></strong> to <a href=\"https://wandb.ai/irl_team7/DQfD\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-training ...\n",
            "All pre-train finish.\n",
            "Annealing: exponential, Demo Sampling Percent: 0.5\n",
            "Episode: 0 Time Step: 40 Cumulative Rewards: 41.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.45241870901797976\n",
            "Episode: 1 Time Step: 36 Cumulative Rewards: 37.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.4093653765389909\n",
            "Episode: 2 Time Step: 59 Cumulative Rewards: 60.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.37040911034085894\n",
            "Episode: 3 Time Step: 55 Cumulative Rewards: 56.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.33516002301781966\n",
            "Episode: 4 Time Step: 34 Cumulative Rewards: 35.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.3032653298563167\n",
            "Episode: 5 Time Step: 26 Cumulative Rewards: 27.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.2744058180470132\n",
            "Episode: 6 Time Step: 21 Cumulative Rewards: 22.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.24829265189570474\n",
            "Episode: 7 Time Step: 32 Cumulative Rewards: 33.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.22466448205861078\n",
            "Episode: 8 Time Step: 34 Cumulative Rewards: 35.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.20328482987029955\n",
            "Episode: 9 Time Step: 36 Cumulative Rewards: 37.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.18393972058572117\n",
            "Episode: 10 Time Step: 22 Cumulative Rewards: 23.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.16643554184903978\n",
            "Episode: 11 Time Step: 22 Cumulative Rewards: 23.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.150597105956101\n",
            "Episode: 12 Time Step: 26 Cumulative Rewards: 27.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.1362658965170063\n",
            "Episode: 13 Time Step: 20 Cumulative Rewards: 21.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.12329848197080322\n",
            "Episode: 14 Time Step: 27 Cumulative Rewards: 28.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.11156508007421491\n",
            "Episode: 15 Time Step: 18 Cumulative Rewards: 19.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.10094825899732769\n",
            "Episode: 16 Time Step: 17 Cumulative Rewards: 18.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0913417620263673\n",
            "Episode: 17 Time Step: 27 Cumulative Rewards: 28.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.08264944411079327\n",
            "Episode: 18 Time Step: 23 Cumulative Rewards: 24.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.07478430961131752\n",
            "Episode: 19 Time Step: 27 Cumulative Rewards: 28.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.06766764161830635\n",
            "Episode: 20 Time Step: 34 Cumulative Rewards: 35.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.06122821412649095\n",
            "Episode: 21 Time Step: 39 Cumulative Rewards: 40.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.055401579181166935\n",
            "Episode: 22 Time Step: 44 Cumulative Rewards: 45.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.05012942186140185\n",
            "Episode: 23 Time Step: 33 Cumulative Rewards: 34.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.045358976644706235\n",
            "Episode: 24 Time Step: 28 Cumulative Rewards: 29.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0410424993119494\n",
            "Episode: 25 Time Step: 38 Cumulative Rewards: 39.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.03713678910716694\n",
            "Episode: 26 Time Step: 91 Cumulative Rewards: 92.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.03360275636987488\n",
            "Episode: 27 Time Step: 33 Cumulative Rewards: 34.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.030405031312608976\n",
            "Episode: 28 Time Step: 29 Cumulative Rewards: 30.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.027511610028203605\n",
            "Episode: 29 Time Step: 34 Cumulative Rewards: 35.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.024893534183931972\n",
            "Episode: 30 Time Step: 73 Cumulative Rewards: 74.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0225246011967789\n",
            "Episode: 31 Time Step: 76 Cumulative Rewards: 77.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.020381101989183106\n",
            "Episode: 32 Time Step: 46 Cumulative Rewards: 47.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.018441583700619997\n",
            "Episode: 33 Time Step: 41 Cumulative Rewards: 42.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.016686634980163033\n",
            "Episode: 34 Time Step: 31 Cumulative Rewards: 32.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.01509869171115925\n",
            "Episode: 35 Time Step: 63 Cumulative Rewards: 64.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.01366186122364628\n",
            "Episode: 36 Time Step: 51 Cumulative Rewards: 52.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.012361763235169694\n",
            "Episode: 37 Time Step: 46 Cumulative Rewards: 47.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.011185385928082795\n",
            "Episode: 38 Time Step: 56 Cumulative Rewards: 57.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.01012095572290219\n",
            "Episode: 39 Time Step: 58 Cumulative Rewards: 59.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.00915781944436709\n",
            "Episode: 40 Time Step: 44 Cumulative Rewards: 45.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.008286337700880619\n",
            "Episode: 41 Time Step: 84 Cumulative Rewards: 85.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.007497788410238852\n",
            "Episode: 42 Time Step: 128 Cumulative Rewards: 129.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.006784279506100467\n",
            "Episode: 43 Time Step: 277 Cumulative Rewards: 278.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.006138669951534218\n",
            "Episode: 44 Time Step: 50 Cumulative Rewards: 51.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.005554498269121153\n",
            "Episode: 45 Time Step: 51 Cumulative Rewards: 52.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.005025917872316788\n",
            "Episode: 46 Time Step: 24 Cumulative Rewards: 25.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.004547638550847908\n",
            "Episode: 47 Time Step: 18 Cumulative Rewards: 19.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.004114873524510012\n",
            "Episode: 48 Time Step: 12 Cumulative Rewards: 13.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.003723291535462169\n",
            "Episode: 49 Time Step: 9 Cumulative Rewards: 10.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0033689734995427335\n",
            "Episode: 50 Time Step: 9 Cumulative Rewards: 10.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0030483732827578163\n",
            "Episode: 51 Time Step: 84 Cumulative Rewards: 85.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0027582822103803858\n",
            "Episode: 52 Time Step: 8 Cumulative Rewards: 9.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0024957969534551063\n",
            "Episode: 53 Time Step: 20 Cumulative Rewards: 21.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.002258290471306333\n",
            "Episode: 54 Time Step: 8 Cumulative Rewards: 9.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0020433857192320333\n",
            "Episode: 55 Time Step: 7 Cumulative Rewards: 8.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0018489318582414645\n",
            "Episode: 56 Time Step: 9 Cumulative Rewards: 10.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.001672982728735636\n",
            "Episode: 57 Time Step: 9 Cumulative Rewards: 10.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0015137773726879064\n",
            "Episode: 58 Time Step: 25 Cumulative Rewards: 26.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0013697224093841842\n",
            "Episode: 59 Time Step: 8 Cumulative Rewards: 9.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0012393760883331792\n",
            "Episode: 60 Time Step: 53 Cumulative Rewards: 54.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0011214338597429006\n",
            "Episode: 61 Time Step: 458 Cumulative Rewards: 459.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.001014715318147867\n",
            "Episode: 62 Time Step: 494 Cumulative Rewards: 495.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0009181523885144528\n",
            "Episode: 63 Time Step: 297 Cumulative Rewards: 298.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.000830778636586967\n",
            "Episode: 64 Time Step: 245 Cumulative Rewards: 246.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0007517195964887862\n",
            "Episode: 65 Time Step: 240 Cumulative Rewards: 241.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0006801840187739464\n",
            "Episode: 66 Time Step: 239 Cumulative Rewards: 240.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0006154559513367405\n",
            "Episode: 67 Time Step: 195 Cumulative Rewards: 196.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0005568875739224012\n",
            "Episode: 68 Time Step: 183 Cumulative Rewards: 184.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0005038927145242552\n",
            "Episode: 69 Time Step: 206 Cumulative Rewards: 207.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0004559409827772581\n",
            "Episode: 70 Time Step: 171 Cumulative Rewards: 172.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0004125524616329519\n",
            "Episode: 71 Time Step: 195 Cumulative Rewards: 196.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0003732929041883396\n",
            "Episode: 72 Time Step: 185 Cumulative Rewards: 186.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.00033776938759692187\n",
            "Episode: 73 Time Step: 182 Cumulative Rewards: 183.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.00030562638056478615\n",
            "Episode: 74 Time Step: 164 Cumulative Rewards: 165.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0002765421850739168\n",
            "Episode: 75 Time Step: 148 Cumulative Rewards: 149.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0002502257167203052\n",
            "Episode: 76 Time Step: 197 Cumulative Rewards: 198.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.00022641359144339848\n",
            "Episode: 77 Time Step: 178 Cumulative Rewards: 179.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.00020486748948989321\n",
            "Episode: 78 Time Step: 156 Cumulative Rewards: 157.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0001853717702295441\n",
            "Episode: 79 Time Step: 163 Cumulative Rewards: 164.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.00016773131395125593\n",
            "Episode: 80 Time Step: 152 Cumulative Rewards: 153.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0001517695690394334\n",
            "Episode: 81 Time Step: 163 Cumulative Rewards: 164.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.00013732678498607102\n",
            "Episode: 82 Time Step: 146 Cumulative Rewards: 147.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.00012425841355397592\n",
            "Episode: 83 Time Step: 150 Cumulative Rewards: 151.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.0001124336620894241\n",
            "Episode: 84 Time Step: 162 Cumulative Rewards: 163.0\n",
            "Annealing: exponential, Demo Sampling Percent: 0.00010173418450532208\n",
            "Episode: 85 Time Step: 148 Cumulative Rewards: 149.0\n",
            "Annealing: exponential, Demo Sampling Percent: 9.20528968337896e-05\n",
            "Episode: 86 Time Step: 159 Cumulative Rewards: 160.0\n",
            "Annealing: exponential, Demo Sampling Percent: 8.329290549381662e-05\n",
            "Episode: 87 Time Step: 162 Cumulative Rewards: 163.0\n",
            "Annealing: exponential, Demo Sampling Percent: 7.536653754773825e-05\n",
            "Episode: 88 Time Step: 140 Cumulative Rewards: 141.0\n",
            "Annealing: exponential, Demo Sampling Percent: 6.81944632410057e-05\n",
            "Episode: 89 Time Step: 187 Cumulative Rewards: 188.0\n",
            "Annealing: exponential, Demo Sampling Percent: 6.170490204333978e-05\n",
            "Episode: 90 Time Step: 164 Cumulative Rewards: 165.0\n",
            "Annealing: exponential, Demo Sampling Percent: 5.583290424505739e-05\n",
            "Episode: 91 Time Step: 145 Cumulative Rewards: 146.0\n",
            "Annealing: exponential, Demo Sampling Percent: 5.051970091854662e-05\n",
            "Episode: 92 Time Step: 149 Cumulative Rewards: 150.0\n",
            "Annealing: exponential, Demo Sampling Percent: 4.5712115739086635e-05\n",
            "Episode: 93 Time Step: 162 Cumulative Rewards: 163.0\n",
            "Annealing: exponential, Demo Sampling Percent: 4.1362032778316114e-05\n",
            "Episode: 94 Time Step: 171 Cumulative Rewards: 172.0\n",
            "Annealing: exponential, Demo Sampling Percent: 3.74259149438503e-05\n",
            "Episode: 95 Time Step: 303 Cumulative Rewards: 304.0\n",
            "Annealing: exponential, Demo Sampling Percent: 3.386436824542689e-05\n",
            "Episode: 96 Time Step: 173 Cumulative Rewards: 174.0\n",
            "Annealing: exponential, Demo Sampling Percent: 3.064174752661101e-05\n",
            "Episode: 97 Time Step: 164 Cumulative Rewards: 165.0\n",
            "Annealing: exponential, Demo Sampling Percent: 2.7725799716088473e-05\n",
            "Episode: 98 Time Step: 200 Cumulative Rewards: 201.0\n",
            "Annealing: exponential, Demo Sampling Percent: 2.508734102808764e-05\n",
            "Episode: 99 Time Step: 138 Cumulative Rewards: 139.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3db9fdbc3f2642e19a2873b532c38b97"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>▂▂▁▂▁▁▁▁▂▂▂▂▃▂▂▂▂█▂▁▁▁▁▁▂█▇▅▆▅▆▅▅▄▅▄▄▅▅▄</td></tr><tr><td>time_step</td><td>▂▂▁▂▁▁▁▁▂▂▂▂▃▂▂▂▂█▂▁▁▁▁▁▂█▇▅▆▅▆▅▅▄▅▄▄▅▅▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>139.0</td></tr><tr><td>time_step</td><td>138</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">cartpole_exponential_0.5_0.1_1</strong>: <a href=\"https://wandb.ai/irl_team7/DQfD/runs/3dbg2bon\" target=\"_blank\">https://wandb.ai/irl_team7/DQfD/runs/3dbg2bon</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221210_165622-3dbg2bon/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA...\n",
            "Number of <state,action> pairs in the demonstrator data = 10000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221210_170108-61p581lr</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/irl_team7/DQfD/runs/61p581lr\" target=\"_blank\">cartpole_constant_0.0_1</a></strong> to <a href=\"https://wandb.ai/irl_team7/DQfD\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-training ...\n",
            "All pre-train finish.\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 0 Time Step: 64 Cumulative Rewards: 65.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 1 Time Step: 47 Cumulative Rewards: 48.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 2 Time Step: 40 Cumulative Rewards: 41.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 3 Time Step: 29 Cumulative Rewards: 30.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 4 Time Step: 48 Cumulative Rewards: 49.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 5 Time Step: 31 Cumulative Rewards: 32.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 6 Time Step: 24 Cumulative Rewards: 25.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 7 Time Step: 17 Cumulative Rewards: 18.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 8 Time Step: 17 Cumulative Rewards: 18.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 9 Time Step: 12 Cumulative Rewards: 13.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 10 Time Step: 16 Cumulative Rewards: 17.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 11 Time Step: 16 Cumulative Rewards: 17.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 12 Time Step: 12 Cumulative Rewards: 13.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 13 Time Step: 12 Cumulative Rewards: 13.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 14 Time Step: 17 Cumulative Rewards: 18.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 15 Time Step: 12 Cumulative Rewards: 13.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 16 Time Step: 18 Cumulative Rewards: 19.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 17 Time Step: 17 Cumulative Rewards: 18.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 18 Time Step: 11 Cumulative Rewards: 12.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 19 Time Step: 19 Cumulative Rewards: 20.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 20 Time Step: 26 Cumulative Rewards: 27.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 21 Time Step: 28 Cumulative Rewards: 29.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 22 Time Step: 45 Cumulative Rewards: 46.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 23 Time Step: 64 Cumulative Rewards: 65.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 24 Time Step: 37 Cumulative Rewards: 38.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 25 Time Step: 44 Cumulative Rewards: 45.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 26 Time Step: 42 Cumulative Rewards: 43.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 27 Time Step: 52 Cumulative Rewards: 53.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 28 Time Step: 61 Cumulative Rewards: 62.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 29 Time Step: 101 Cumulative Rewards: 102.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 30 Time Step: 102 Cumulative Rewards: 103.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 31 Time Step: 59 Cumulative Rewards: 60.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 32 Time Step: 41 Cumulative Rewards: 42.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 33 Time Step: 41 Cumulative Rewards: 42.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 34 Time Step: 89 Cumulative Rewards: 90.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 35 Time Step: 95 Cumulative Rewards: 96.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 36 Time Step: 60 Cumulative Rewards: 61.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 37 Time Step: 54 Cumulative Rewards: 55.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 38 Time Step: 106 Cumulative Rewards: 107.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 39 Time Step: 53 Cumulative Rewards: 54.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 40 Time Step: 33 Cumulative Rewards: 34.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 41 Time Step: 138 Cumulative Rewards: 139.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 42 Time Step: 79 Cumulative Rewards: 80.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 43 Time Step: 47 Cumulative Rewards: 48.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 44 Time Step: 63 Cumulative Rewards: 64.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 45 Time Step: 47 Cumulative Rewards: 48.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 46 Time Step: 64 Cumulative Rewards: 65.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 47 Time Step: 58 Cumulative Rewards: 59.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 48 Time Step: 72 Cumulative Rewards: 73.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 49 Time Step: 203 Cumulative Rewards: 204.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 50 Time Step: 171 Cumulative Rewards: 172.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 51 Time Step: 142 Cumulative Rewards: 143.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 52 Time Step: 302 Cumulative Rewards: 303.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 53 Time Step: 190 Cumulative Rewards: 191.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 54 Time Step: 128 Cumulative Rewards: 129.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 55 Time Step: 225 Cumulative Rewards: 226.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 56 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 57 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 58 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 59 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 60 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 61 Time Step: 355 Cumulative Rewards: 356.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 62 Time Step: 292 Cumulative Rewards: 293.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 63 Time Step: 296 Cumulative Rewards: 297.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 64 Time Step: 298 Cumulative Rewards: 299.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 65 Time Step: 317 Cumulative Rewards: 318.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 66 Time Step: 279 Cumulative Rewards: 280.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 67 Time Step: 263 Cumulative Rewards: 264.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 68 Time Step: 261 Cumulative Rewards: 262.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 69 Time Step: 240 Cumulative Rewards: 241.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 70 Time Step: 282 Cumulative Rewards: 283.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 71 Time Step: 228 Cumulative Rewards: 229.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 72 Time Step: 233 Cumulative Rewards: 234.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 73 Time Step: 208 Cumulative Rewards: 209.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 74 Time Step: 180 Cumulative Rewards: 181.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 75 Time Step: 241 Cumulative Rewards: 242.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 76 Time Step: 238 Cumulative Rewards: 239.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 77 Time Step: 230 Cumulative Rewards: 231.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 78 Time Step: 194 Cumulative Rewards: 195.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 79 Time Step: 196 Cumulative Rewards: 197.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 80 Time Step: 232 Cumulative Rewards: 233.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 81 Time Step: 411 Cumulative Rewards: 412.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 82 Time Step: 356 Cumulative Rewards: 357.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 83 Time Step: 248 Cumulative Rewards: 249.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 84 Time Step: 295 Cumulative Rewards: 296.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 85 Time Step: 325 Cumulative Rewards: 326.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 86 Time Step: 300 Cumulative Rewards: 301.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 87 Time Step: 189 Cumulative Rewards: 190.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 88 Time Step: 334 Cumulative Rewards: 335.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 89 Time Step: 237 Cumulative Rewards: 238.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 90 Time Step: 194 Cumulative Rewards: 195.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 91 Time Step: 310 Cumulative Rewards: 311.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 92 Time Step: 187 Cumulative Rewards: 188.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 93 Time Step: 158 Cumulative Rewards: 159.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 94 Time Step: 463 Cumulative Rewards: 464.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 95 Time Step: 154 Cumulative Rewards: 155.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 96 Time Step: 120 Cumulative Rewards: 121.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 97 Time Step: 187 Cumulative Rewards: 188.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 98 Time Step: 229 Cumulative Rewards: 230.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.0\n",
            "Episode: 99 Time Step: 246 Cumulative Rewards: 247.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.018 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.038816…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd6fe0822e3140619ab0b764e3036f90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>▂▁▁▁▁▁▁▁▁▁▁▂▂▁▂▂▁▂▂▂▃▄▄██▅▅▅▄▄▄▄▇▄▅▆▅▃▃▄</td></tr><tr><td>time_step</td><td>▂▁▁▁▁▁▁▁▁▁▁▂▂▁▂▂▁▂▂▂▃▄▄██▅▅▅▄▄▄▄▇▄▅▆▅▃▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>247.0</td></tr><tr><td>time_step</td><td>246</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">cartpole_constant_0.0_1</strong>: <a href=\"https://wandb.ai/irl_team7/DQfD/runs/61p581lr\" target=\"_blank\">https://wandb.ai/irl_team7/DQfD/runs/61p581lr</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221210_170108-61p581lr/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA...\n",
            "Number of <state,action> pairs in the demonstrator data = 10000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221210_170833-12hh90rq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/irl_team7/DQfD/runs/12hh90rq\" target=\"_blank\">cartpole_constant_0.25_1</a></strong> to <a href=\"https://wandb.ai/irl_team7/DQfD\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-training ...\n",
            "All pre-train finish.\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 0 Time Step: 38 Cumulative Rewards: 39.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 1 Time Step: 31 Cumulative Rewards: 32.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 2 Time Step: 57 Cumulative Rewards: 58.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 3 Time Step: 39 Cumulative Rewards: 40.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 4 Time Step: 35 Cumulative Rewards: 36.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 5 Time Step: 31 Cumulative Rewards: 32.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 6 Time Step: 26 Cumulative Rewards: 27.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 7 Time Step: 32 Cumulative Rewards: 33.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 8 Time Step: 28 Cumulative Rewards: 29.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 9 Time Step: 23 Cumulative Rewards: 24.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 10 Time Step: 18 Cumulative Rewards: 19.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 11 Time Step: 29 Cumulative Rewards: 30.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 12 Time Step: 19 Cumulative Rewards: 20.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 13 Time Step: 39 Cumulative Rewards: 40.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 14 Time Step: 22 Cumulative Rewards: 23.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 15 Time Step: 21 Cumulative Rewards: 22.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 16 Time Step: 25 Cumulative Rewards: 26.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 17 Time Step: 24 Cumulative Rewards: 25.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 18 Time Step: 45 Cumulative Rewards: 46.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 19 Time Step: 31 Cumulative Rewards: 32.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 20 Time Step: 34 Cumulative Rewards: 35.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 21 Time Step: 29 Cumulative Rewards: 30.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 22 Time Step: 39 Cumulative Rewards: 40.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 23 Time Step: 63 Cumulative Rewards: 64.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 24 Time Step: 36 Cumulative Rewards: 37.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 25 Time Step: 54 Cumulative Rewards: 55.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 26 Time Step: 36 Cumulative Rewards: 37.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 27 Time Step: 87 Cumulative Rewards: 88.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 28 Time Step: 40 Cumulative Rewards: 41.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 29 Time Step: 59 Cumulative Rewards: 60.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 30 Time Step: 79 Cumulative Rewards: 80.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 31 Time Step: 51 Cumulative Rewards: 52.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 32 Time Step: 132 Cumulative Rewards: 133.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 33 Time Step: 55 Cumulative Rewards: 56.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 34 Time Step: 149 Cumulative Rewards: 150.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 35 Time Step: 71 Cumulative Rewards: 72.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 36 Time Step: 144 Cumulative Rewards: 145.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 37 Time Step: 209 Cumulative Rewards: 210.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 38 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 39 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 40 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 41 Time Step: 383 Cumulative Rewards: 384.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 42 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 43 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 44 Time Step: 277 Cumulative Rewards: 278.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 45 Time Step: 437 Cumulative Rewards: 438.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 46 Time Step: 336 Cumulative Rewards: 337.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 47 Time Step: 302 Cumulative Rewards: 303.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 48 Time Step: 290 Cumulative Rewards: 291.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 49 Time Step: 289 Cumulative Rewards: 290.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 50 Time Step: 253 Cumulative Rewards: 254.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 51 Time Step: 262 Cumulative Rewards: 263.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 52 Time Step: 283 Cumulative Rewards: 284.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 53 Time Step: 281 Cumulative Rewards: 282.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 54 Time Step: 246 Cumulative Rewards: 247.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 55 Time Step: 254 Cumulative Rewards: 255.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 56 Time Step: 268 Cumulative Rewards: 269.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 57 Time Step: 263 Cumulative Rewards: 264.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 58 Time Step: 233 Cumulative Rewards: 234.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 59 Time Step: 219 Cumulative Rewards: 220.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 60 Time Step: 223 Cumulative Rewards: 224.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 61 Time Step: 259 Cumulative Rewards: 260.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 62 Time Step: 244 Cumulative Rewards: 245.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 63 Time Step: 256 Cumulative Rewards: 257.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 64 Time Step: 233 Cumulative Rewards: 234.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 65 Time Step: 277 Cumulative Rewards: 278.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 66 Time Step: 222 Cumulative Rewards: 223.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 67 Time Step: 268 Cumulative Rewards: 269.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 68 Time Step: 224 Cumulative Rewards: 225.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 69 Time Step: 222 Cumulative Rewards: 223.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 70 Time Step: 203 Cumulative Rewards: 204.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 71 Time Step: 265 Cumulative Rewards: 266.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 72 Time Step: 266 Cumulative Rewards: 267.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 73 Time Step: 253 Cumulative Rewards: 254.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 74 Time Step: 262 Cumulative Rewards: 263.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 75 Time Step: 217 Cumulative Rewards: 218.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 76 Time Step: 231 Cumulative Rewards: 232.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 77 Time Step: 238 Cumulative Rewards: 239.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 78 Time Step: 185 Cumulative Rewards: 186.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 79 Time Step: 253 Cumulative Rewards: 254.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 80 Time Step: 138 Cumulative Rewards: 139.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 81 Time Step: 270 Cumulative Rewards: 271.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 82 Time Step: 365 Cumulative Rewards: 366.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 83 Time Step: 54 Cumulative Rewards: 55.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 84 Time Step: 77 Cumulative Rewards: 78.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 85 Time Step: 243 Cumulative Rewards: 244.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 86 Time Step: 32 Cumulative Rewards: 33.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 87 Time Step: 80 Cumulative Rewards: 81.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 88 Time Step: 92 Cumulative Rewards: 93.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 89 Time Step: 251 Cumulative Rewards: 252.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 90 Time Step: 271 Cumulative Rewards: 272.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 91 Time Step: 225 Cumulative Rewards: 226.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 92 Time Step: 282 Cumulative Rewards: 283.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 93 Time Step: 269 Cumulative Rewards: 270.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 94 Time Step: 225 Cumulative Rewards: 226.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 95 Time Step: 214 Cumulative Rewards: 215.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 96 Time Step: 227 Cumulative Rewards: 228.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 97 Time Step: 261 Cumulative Rewards: 262.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 98 Time Step: 273 Cumulative Rewards: 274.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.25\n",
            "Episode: 99 Time Step: 182 Cumulative Rewards: 183.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91a7fd04776047fdadaf43a2035d41bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>▁▂▁▁▁▁▁▁▁▁▂▂▂▂▂███▇▅▄▅▄▄▄▄▄▄▅▄▄▃▅▂▁▂▄▅▄▃</td></tr><tr><td>time_step</td><td>▁▂▁▁▁▁▁▁▁▁▂▂▂▂▂███▇▅▄▅▄▄▄▄▄▄▅▄▄▃▅▂▁▂▄▅▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>183.0</td></tr><tr><td>time_step</td><td>182</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">cartpole_constant_0.25_1</strong>: <a href=\"https://wandb.ai/irl_team7/DQfD/runs/12hh90rq\" target=\"_blank\">https://wandb.ai/irl_team7/DQfD/runs/12hh90rq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221210_170833-12hh90rq/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA...\n",
            "Number of <state,action> pairs in the demonstrator data = 10000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221210_171824-vzfdhhpg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/irl_team7/DQfD/runs/vzfdhhpg\" target=\"_blank\">cartpole_constant_0.5_1</a></strong> to <a href=\"https://wandb.ai/irl_team7/DQfD\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-training ...\n",
            "All pre-train finish.\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 0 Time Step: 113 Cumulative Rewards: 114.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 1 Time Step: 36 Cumulative Rewards: 37.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 2 Time Step: 50 Cumulative Rewards: 51.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 3 Time Step: 30 Cumulative Rewards: 31.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 4 Time Step: 32 Cumulative Rewards: 33.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 5 Time Step: 41 Cumulative Rewards: 42.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 6 Time Step: 55 Cumulative Rewards: 56.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 7 Time Step: 64 Cumulative Rewards: 65.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 8 Time Step: 30 Cumulative Rewards: 31.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 9 Time Step: 30 Cumulative Rewards: 31.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 10 Time Step: 51 Cumulative Rewards: 52.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 11 Time Step: 64 Cumulative Rewards: 65.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 12 Time Step: 37 Cumulative Rewards: 38.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 13 Time Step: 39 Cumulative Rewards: 40.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 14 Time Step: 55 Cumulative Rewards: 56.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 15 Time Step: 63 Cumulative Rewards: 64.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 16 Time Step: 50 Cumulative Rewards: 51.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 17 Time Step: 98 Cumulative Rewards: 99.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 18 Time Step: 47 Cumulative Rewards: 48.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 19 Time Step: 83 Cumulative Rewards: 84.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 20 Time Step: 42 Cumulative Rewards: 43.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 21 Time Step: 93 Cumulative Rewards: 94.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 22 Time Step: 90 Cumulative Rewards: 91.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 23 Time Step: 85 Cumulative Rewards: 86.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 24 Time Step: 59 Cumulative Rewards: 60.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 25 Time Step: 40 Cumulative Rewards: 41.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 26 Time Step: 38 Cumulative Rewards: 39.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 27 Time Step: 98 Cumulative Rewards: 99.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 28 Time Step: 40 Cumulative Rewards: 41.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 29 Time Step: 42 Cumulative Rewards: 43.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 30 Time Step: 63 Cumulative Rewards: 64.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 31 Time Step: 103 Cumulative Rewards: 104.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 32 Time Step: 111 Cumulative Rewards: 112.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 33 Time Step: 57 Cumulative Rewards: 58.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 34 Time Step: 42 Cumulative Rewards: 43.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 35 Time Step: 132 Cumulative Rewards: 133.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 36 Time Step: 57 Cumulative Rewards: 58.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 37 Time Step: 109 Cumulative Rewards: 110.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 38 Time Step: 106 Cumulative Rewards: 107.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 39 Time Step: 110 Cumulative Rewards: 111.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 40 Time Step: 77 Cumulative Rewards: 78.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 41 Time Step: 271 Cumulative Rewards: 272.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 42 Time Step: 124 Cumulative Rewards: 125.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 43 Time Step: 61 Cumulative Rewards: 62.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 44 Time Step: 74 Cumulative Rewards: 75.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 45 Time Step: 101 Cumulative Rewards: 102.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 46 Time Step: 76 Cumulative Rewards: 77.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 47 Time Step: 81 Cumulative Rewards: 82.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 48 Time Step: 96 Cumulative Rewards: 97.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 49 Time Step: 96 Cumulative Rewards: 97.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 50 Time Step: 84 Cumulative Rewards: 85.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 51 Time Step: 72 Cumulative Rewards: 73.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 52 Time Step: 75 Cumulative Rewards: 76.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 53 Time Step: 121 Cumulative Rewards: 122.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 54 Time Step: 109 Cumulative Rewards: 110.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 55 Time Step: 112 Cumulative Rewards: 113.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 56 Time Step: 126 Cumulative Rewards: 127.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 57 Time Step: 143 Cumulative Rewards: 144.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 58 Time Step: 162 Cumulative Rewards: 163.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 59 Time Step: 103 Cumulative Rewards: 104.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 60 Time Step: 185 Cumulative Rewards: 186.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 61 Time Step: 219 Cumulative Rewards: 220.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 62 Time Step: 262 Cumulative Rewards: 263.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 63 Time Step: 221 Cumulative Rewards: 222.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 64 Time Step: 139 Cumulative Rewards: 140.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 65 Time Step: 139 Cumulative Rewards: 140.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 66 Time Step: 198 Cumulative Rewards: 199.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 67 Time Step: 199 Cumulative Rewards: 200.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 68 Time Step: 200 Cumulative Rewards: 201.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 69 Time Step: 133 Cumulative Rewards: 134.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 70 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 71 Time Step: 167 Cumulative Rewards: 168.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 72 Time Step: 204 Cumulative Rewards: 205.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 73 Time Step: 156 Cumulative Rewards: 157.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 74 Time Step: 195 Cumulative Rewards: 196.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 75 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 76 Time Step: 276 Cumulative Rewards: 277.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 77 Time Step: 450 Cumulative Rewards: 451.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 78 Time Step: 201 Cumulative Rewards: 202.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 79 Time Step: 122 Cumulative Rewards: 123.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 80 Time Step: 167 Cumulative Rewards: 168.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 81 Time Step: 308 Cumulative Rewards: 309.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 82 Time Step: 217 Cumulative Rewards: 218.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 83 Time Step: 239 Cumulative Rewards: 240.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 84 Time Step: 209 Cumulative Rewards: 210.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 85 Time Step: 167 Cumulative Rewards: 168.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 86 Time Step: 216 Cumulative Rewards: 217.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 87 Time Step: 344 Cumulative Rewards: 345.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 88 Time Step: 194 Cumulative Rewards: 195.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 89 Time Step: 225 Cumulative Rewards: 226.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 90 Time Step: 323 Cumulative Rewards: 324.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 91 Time Step: 255 Cumulative Rewards: 256.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 92 Time Step: 279 Cumulative Rewards: 280.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 93 Time Step: 234 Cumulative Rewards: 235.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 94 Time Step: 191 Cumulative Rewards: 192.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 95 Time Step: 384 Cumulative Rewards: 385.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 96 Time Step: 240 Cumulative Rewards: 241.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 97 Time Step: 395 Cumulative Rewards: 396.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 98 Time Step: 215 Cumulative Rewards: 216.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.5\n",
            "Episode: 99 Time Step: 499 Cumulative Rewards: 500.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2d07d782b0946e8899663778ffdfa86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>▂▁▁▁▁▁▁▂▁▂▁▂▁▁▂▂▂▁▂▂▂▂▂▃▃▄▃▃▃▃▅▃▅▄▄▃▄▄▄█</td></tr><tr><td>time_step</td><td>▂▁▁▁▁▁▁▂▁▂▁▂▁▁▂▂▂▁▂▂▂▂▂▃▃▄▃▃▃▃▅▃▅▄▄▃▄▄▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>500.0</td></tr><tr><td>time_step</td><td>499</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">cartpole_constant_0.5_1</strong>: <a href=\"https://wandb.ai/irl_team7/DQfD/runs/vzfdhhpg\" target=\"_blank\">https://wandb.ai/irl_team7/DQfD/runs/vzfdhhpg</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221210_171824-vzfdhhpg/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA...\n",
            "Number of <state,action> pairs in the demonstrator data = 10000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221210_172717-2asalgzp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/irl_team7/DQfD/runs/2asalgzp\" target=\"_blank\">cartpole_constant_0.75_1</a></strong> to <a href=\"https://wandb.ai/irl_team7/DQfD\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-training ...\n",
            "All pre-train finish.\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 0 Time Step: 36 Cumulative Rewards: 37.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 1 Time Step: 41 Cumulative Rewards: 42.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 2 Time Step: 31 Cumulative Rewards: 32.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 3 Time Step: 38 Cumulative Rewards: 39.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 4 Time Step: 32 Cumulative Rewards: 33.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 5 Time Step: 29 Cumulative Rewards: 30.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 6 Time Step: 36 Cumulative Rewards: 37.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 7 Time Step: 25 Cumulative Rewards: 26.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 8 Time Step: 29 Cumulative Rewards: 30.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 9 Time Step: 24 Cumulative Rewards: 25.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 10 Time Step: 23 Cumulative Rewards: 24.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 11 Time Step: 36 Cumulative Rewards: 37.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 12 Time Step: 37 Cumulative Rewards: 38.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 13 Time Step: 57 Cumulative Rewards: 58.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 14 Time Step: 28 Cumulative Rewards: 29.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 15 Time Step: 31 Cumulative Rewards: 32.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 16 Time Step: 57 Cumulative Rewards: 58.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 17 Time Step: 31 Cumulative Rewards: 32.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 18 Time Step: 41 Cumulative Rewards: 42.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 19 Time Step: 35 Cumulative Rewards: 36.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 20 Time Step: 48 Cumulative Rewards: 49.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 21 Time Step: 46 Cumulative Rewards: 47.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 22 Time Step: 59 Cumulative Rewards: 60.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 23 Time Step: 54 Cumulative Rewards: 55.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 24 Time Step: 48 Cumulative Rewards: 49.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 25 Time Step: 49 Cumulative Rewards: 50.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 26 Time Step: 51 Cumulative Rewards: 52.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 27 Time Step: 102 Cumulative Rewards: 103.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 28 Time Step: 51 Cumulative Rewards: 52.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 29 Time Step: 35 Cumulative Rewards: 36.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 30 Time Step: 73 Cumulative Rewards: 74.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 31 Time Step: 38 Cumulative Rewards: 39.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 32 Time Step: 38 Cumulative Rewards: 39.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 33 Time Step: 9 Cumulative Rewards: 10.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 34 Time Step: 62 Cumulative Rewards: 63.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 35 Time Step: 28 Cumulative Rewards: 29.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 36 Time Step: 45 Cumulative Rewards: 46.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 37 Time Step: 38 Cumulative Rewards: 39.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 38 Time Step: 56 Cumulative Rewards: 57.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 39 Time Step: 25 Cumulative Rewards: 26.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 40 Time Step: 24 Cumulative Rewards: 25.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 41 Time Step: 23 Cumulative Rewards: 24.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 42 Time Step: 25 Cumulative Rewards: 26.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 43 Time Step: 30 Cumulative Rewards: 31.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 44 Time Step: 77 Cumulative Rewards: 78.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 45 Time Step: 83 Cumulative Rewards: 84.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 46 Time Step: 67 Cumulative Rewards: 68.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 47 Time Step: 69 Cumulative Rewards: 70.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 48 Time Step: 61 Cumulative Rewards: 62.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 49 Time Step: 36 Cumulative Rewards: 37.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 50 Time Step: 63 Cumulative Rewards: 64.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 51 Time Step: 40 Cumulative Rewards: 41.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 52 Time Step: 34 Cumulative Rewards: 35.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 53 Time Step: 77 Cumulative Rewards: 78.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 54 Time Step: 57 Cumulative Rewards: 58.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 55 Time Step: 53 Cumulative Rewards: 54.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 56 Time Step: 47 Cumulative Rewards: 48.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 57 Time Step: 70 Cumulative Rewards: 71.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 58 Time Step: 92 Cumulative Rewards: 93.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 59 Time Step: 118 Cumulative Rewards: 119.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 60 Time Step: 90 Cumulative Rewards: 91.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 61 Time Step: 164 Cumulative Rewards: 165.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 62 Time Step: 105 Cumulative Rewards: 106.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 63 Time Step: 106 Cumulative Rewards: 107.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 64 Time Step: 56 Cumulative Rewards: 57.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 65 Time Step: 75 Cumulative Rewards: 76.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 66 Time Step: 81 Cumulative Rewards: 82.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 67 Time Step: 90 Cumulative Rewards: 91.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 68 Time Step: 61 Cumulative Rewards: 62.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 69 Time Step: 70 Cumulative Rewards: 71.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 70 Time Step: 49 Cumulative Rewards: 50.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 71 Time Step: 89 Cumulative Rewards: 90.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 72 Time Step: 69 Cumulative Rewards: 70.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 73 Time Step: 67 Cumulative Rewards: 68.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 74 Time Step: 79 Cumulative Rewards: 80.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 75 Time Step: 157 Cumulative Rewards: 158.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 76 Time Step: 121 Cumulative Rewards: 122.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 77 Time Step: 180 Cumulative Rewards: 181.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 78 Time Step: 139 Cumulative Rewards: 140.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 79 Time Step: 89 Cumulative Rewards: 90.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 80 Time Step: 66 Cumulative Rewards: 67.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 81 Time Step: 85 Cumulative Rewards: 86.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 82 Time Step: 88 Cumulative Rewards: 89.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 83 Time Step: 72 Cumulative Rewards: 73.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 84 Time Step: 113 Cumulative Rewards: 114.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 85 Time Step: 154 Cumulative Rewards: 155.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 86 Time Step: 117 Cumulative Rewards: 118.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 87 Time Step: 164 Cumulative Rewards: 165.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 88 Time Step: 175 Cumulative Rewards: 176.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 89 Time Step: 84 Cumulative Rewards: 85.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 90 Time Step: 155 Cumulative Rewards: 156.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 91 Time Step: 92 Cumulative Rewards: 93.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 92 Time Step: 126 Cumulative Rewards: 127.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 93 Time Step: 121 Cumulative Rewards: 122.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 94 Time Step: 123 Cumulative Rewards: 124.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 95 Time Step: 186 Cumulative Rewards: 187.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 96 Time Step: 184 Cumulative Rewards: 185.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 97 Time Step: 165 Cumulative Rewards: 166.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 98 Time Step: 176 Cumulative Rewards: 177.0\n",
            "Annealing: constant, Demo Sampling Percent: 0.75\n",
            "Episode: 99 Time Step: 174 Cumulative Rewards: 175.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.018 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.038740…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "571f1088186d4c91a12d03fdad4897a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>▂▂▂▂▂▂▂▂▃▃▃▅▄▁▂▃▂▂▄▃▃▄▃▄▄▅▄▃▄▃▅▆▄▄▅█▄▅██</td></tr><tr><td>time_step</td><td>▂▂▂▂▂▂▂▂▃▃▃▅▄▁▂▃▂▂▄▃▃▄▃▄▄▅▄▃▄▃▅▆▄▄▅█▄▅██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>175.0</td></tr><tr><td>time_step</td><td>174</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">cartpole_constant_0.75_1</strong>: <a href=\"https://wandb.ai/irl_team7/DQfD/runs/2asalgzp\" target=\"_blank\">https://wandb.ai/irl_team7/DQfD/runs/2asalgzp</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221210_172717-2asalgzp/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA...\n",
            "Number of <state,action> pairs in the demonstrator data = 10000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221210_173221-3lyuozx5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/irl_team7/DQfD/runs/3lyuozx5\" target=\"_blank\">cartpole_constant_1.0_1</a></strong> to <a href=\"https://wandb.ai/irl_team7/DQfD\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-training ...\n",
            "All pre-train finish.\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 0 Time Step: 57 Cumulative Rewards: 58.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 1 Time Step: 34 Cumulative Rewards: 35.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 2 Time Step: 35 Cumulative Rewards: 36.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 3 Time Step: 72 Cumulative Rewards: 73.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 4 Time Step: 52 Cumulative Rewards: 53.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 5 Time Step: 40 Cumulative Rewards: 41.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 6 Time Step: 67 Cumulative Rewards: 68.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 7 Time Step: 39 Cumulative Rewards: 40.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 8 Time Step: 69 Cumulative Rewards: 70.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 9 Time Step: 43 Cumulative Rewards: 44.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 10 Time Step: 55 Cumulative Rewards: 56.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 11 Time Step: 62 Cumulative Rewards: 63.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 12 Time Step: 69 Cumulative Rewards: 70.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 13 Time Step: 143 Cumulative Rewards: 144.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 14 Time Step: 143 Cumulative Rewards: 144.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 15 Time Step: 122 Cumulative Rewards: 123.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 16 Time Step: 84 Cumulative Rewards: 85.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 17 Time Step: 37 Cumulative Rewards: 38.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 18 Time Step: 80 Cumulative Rewards: 81.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 19 Time Step: 37 Cumulative Rewards: 38.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 20 Time Step: 120 Cumulative Rewards: 121.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 21 Time Step: 37 Cumulative Rewards: 38.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 22 Time Step: 204 Cumulative Rewards: 205.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 23 Time Step: 139 Cumulative Rewards: 140.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 24 Time Step: 18 Cumulative Rewards: 19.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 25 Time Step: 157 Cumulative Rewards: 158.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 26 Time Step: 201 Cumulative Rewards: 202.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 27 Time Step: 147 Cumulative Rewards: 148.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 28 Time Step: 11 Cumulative Rewards: 12.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 29 Time Step: 144 Cumulative Rewards: 145.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 30 Time Step: 142 Cumulative Rewards: 143.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 31 Time Step: 33 Cumulative Rewards: 34.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 32 Time Step: 157 Cumulative Rewards: 158.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 33 Time Step: 159 Cumulative Rewards: 160.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 34 Time Step: 41 Cumulative Rewards: 42.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 35 Time Step: 36 Cumulative Rewards: 37.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 36 Time Step: 36 Cumulative Rewards: 37.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 37 Time Step: 128 Cumulative Rewards: 129.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 38 Time Step: 34 Cumulative Rewards: 35.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 39 Time Step: 162 Cumulative Rewards: 163.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 40 Time Step: 49 Cumulative Rewards: 50.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 41 Time Step: 49 Cumulative Rewards: 50.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 42 Time Step: 35 Cumulative Rewards: 36.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 43 Time Step: 43 Cumulative Rewards: 44.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 44 Time Step: 32 Cumulative Rewards: 33.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 45 Time Step: 50 Cumulative Rewards: 51.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 46 Time Step: 30 Cumulative Rewards: 31.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 47 Time Step: 39 Cumulative Rewards: 40.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 48 Time Step: 42 Cumulative Rewards: 43.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 49 Time Step: 47 Cumulative Rewards: 48.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 50 Time Step: 29 Cumulative Rewards: 30.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 51 Time Step: 41 Cumulative Rewards: 42.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 52 Time Step: 149 Cumulative Rewards: 150.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 53 Time Step: 44 Cumulative Rewards: 45.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 54 Time Step: 39 Cumulative Rewards: 40.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 55 Time Step: 9 Cumulative Rewards: 10.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 56 Time Step: 26 Cumulative Rewards: 27.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 57 Time Step: 8 Cumulative Rewards: 9.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 58 Time Step: 32 Cumulative Rewards: 33.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 59 Time Step: 156 Cumulative Rewards: 157.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 60 Time Step: 7 Cumulative Rewards: 8.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 61 Time Step: 9 Cumulative Rewards: 10.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 62 Time Step: 26 Cumulative Rewards: 27.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 63 Time Step: 8 Cumulative Rewards: 9.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 64 Time Step: 26 Cumulative Rewards: 27.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 65 Time Step: 8 Cumulative Rewards: 9.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 66 Time Step: 8 Cumulative Rewards: 9.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 67 Time Step: 23 Cumulative Rewards: 24.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 68 Time Step: 8 Cumulative Rewards: 9.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 69 Time Step: 31 Cumulative Rewards: 32.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 70 Time Step: 9 Cumulative Rewards: 10.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 71 Time Step: 27 Cumulative Rewards: 28.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 72 Time Step: 9 Cumulative Rewards: 10.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 73 Time Step: 146 Cumulative Rewards: 147.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 74 Time Step: 48 Cumulative Rewards: 49.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 75 Time Step: 10 Cumulative Rewards: 11.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 76 Time Step: 22 Cumulative Rewards: 23.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 77 Time Step: 9 Cumulative Rewards: 10.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 78 Time Step: 28 Cumulative Rewards: 29.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 79 Time Step: 9 Cumulative Rewards: 10.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 80 Time Step: 155 Cumulative Rewards: 156.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 81 Time Step: 7 Cumulative Rewards: 8.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 82 Time Step: 33 Cumulative Rewards: 34.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 83 Time Step: 9 Cumulative Rewards: 10.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 84 Time Step: 171 Cumulative Rewards: 172.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 85 Time Step: 50 Cumulative Rewards: 51.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 86 Time Step: 9 Cumulative Rewards: 10.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 87 Time Step: 32 Cumulative Rewards: 33.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 88 Time Step: 9 Cumulative Rewards: 10.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 89 Time Step: 26 Cumulative Rewards: 27.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 90 Time Step: 8 Cumulative Rewards: 9.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 91 Time Step: 9 Cumulative Rewards: 10.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 92 Time Step: 23 Cumulative Rewards: 24.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 93 Time Step: 120 Cumulative Rewards: 121.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 94 Time Step: 9 Cumulative Rewards: 10.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 95 Time Step: 195 Cumulative Rewards: 196.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 96 Time Step: 190 Cumulative Rewards: 191.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 97 Time Step: 9 Cumulative Rewards: 10.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 98 Time Step: 28 Cumulative Rewards: 29.0\n",
            "Annealing: constant, Demo Sampling Percent: 1.0\n",
            "Episode: 99 Time Step: 9 Cumulative Rewards: 10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.018 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.039058…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55fbe2834d3047c19951ab154bd640d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>▃▂▂▂▃▃▅▂▅█▆▆▆▆▂▂▂▂▃▂▂▂▁▂▁▁▁▁▂▆▂▂▁▁▁▁▁▅█▁</td></tr><tr><td>time_step</td><td>▃▂▂▂▃▃▅▂▅█▆▆▆▆▂▂▂▂▃▂▂▂▁▂▁▁▁▁▂▆▂▂▁▁▁▁▁▅█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>10.0</td></tr><tr><td>time_step</td><td>9</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">cartpole_constant_1.0_1</strong>: <a href=\"https://wandb.ai/irl_team7/DQfD/runs/3lyuozx5\" target=\"_blank\">https://wandb.ai/irl_team7/DQfD/runs/3lyuozx5</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221210_173221-3lyuozx5/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA...\n",
            "Number of <state,action> pairs in the demonstrator data = 10000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221210_173653-1hofnwt4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/irl_team7/DQfD/runs/1hofnwt4\" target=\"_blank\">cartpole_linear_0.25_0.01_1</a></strong> to <a href=\"https://wandb.ai/irl_team7/DQfD\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-training ...\n",
            "All pre-train finish.\n",
            "Annealing: linear, Demo Sampling Percent: 0.25\n",
            "Episode: 0 Time Step: 70 Cumulative Rewards: 71.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.24\n",
            "Episode: 1 Time Step: 37 Cumulative Rewards: 38.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.23\n",
            "Episode: 2 Time Step: 33 Cumulative Rewards: 34.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.22\n",
            "Episode: 3 Time Step: 35 Cumulative Rewards: 36.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.21\n",
            "Episode: 4 Time Step: 22 Cumulative Rewards: 23.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.2\n",
            "Episode: 5 Time Step: 24 Cumulative Rewards: 25.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.19\n",
            "Episode: 6 Time Step: 46 Cumulative Rewards: 47.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.18\n",
            "Episode: 7 Time Step: 32 Cumulative Rewards: 33.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.16999999999999998\n",
            "Episode: 8 Time Step: 32 Cumulative Rewards: 33.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.16\n",
            "Episode: 9 Time Step: 21 Cumulative Rewards: 22.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.15\n",
            "Episode: 10 Time Step: 16 Cumulative Rewards: 17.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.14\n",
            "Episode: 11 Time Step: 19 Cumulative Rewards: 20.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.13\n",
            "Episode: 12 Time Step: 24 Cumulative Rewards: 25.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.12\n",
            "Episode: 13 Time Step: 14 Cumulative Rewards: 15.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.10999999999999999\n",
            "Episode: 14 Time Step: 16 Cumulative Rewards: 17.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.1\n",
            "Episode: 15 Time Step: 30 Cumulative Rewards: 31.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.09\n",
            "Episode: 16 Time Step: 32 Cumulative Rewards: 33.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.07999999999999999\n",
            "Episode: 17 Time Step: 23 Cumulative Rewards: 24.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.07\n",
            "Episode: 18 Time Step: 31 Cumulative Rewards: 32.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.06\n",
            "Episode: 19 Time Step: 24 Cumulative Rewards: 25.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.04999999999999999\n",
            "Episode: 20 Time Step: 39 Cumulative Rewards: 40.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.04000000000000001\n",
            "Episode: 21 Time Step: 20 Cumulative Rewards: 21.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.03\n",
            "Episode: 22 Time Step: 26 Cumulative Rewards: 27.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.01999999999999999\n",
            "Episode: 23 Time Step: 23 Cumulative Rewards: 24.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.010000000000000009\n",
            "Episode: 24 Time Step: 37 Cumulative Rewards: 38.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 25 Time Step: 38 Cumulative Rewards: 39.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 26 Time Step: 37 Cumulative Rewards: 38.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 27 Time Step: 27 Cumulative Rewards: 28.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 28 Time Step: 51 Cumulative Rewards: 52.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 29 Time Step: 32 Cumulative Rewards: 33.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 30 Time Step: 31 Cumulative Rewards: 32.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 31 Time Step: 41 Cumulative Rewards: 42.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 32 Time Step: 38 Cumulative Rewards: 39.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 33 Time Step: 44 Cumulative Rewards: 45.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 34 Time Step: 49 Cumulative Rewards: 50.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 35 Time Step: 42 Cumulative Rewards: 43.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 36 Time Step: 60 Cumulative Rewards: 61.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 37 Time Step: 45 Cumulative Rewards: 46.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 38 Time Step: 50 Cumulative Rewards: 51.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 39 Time Step: 55 Cumulative Rewards: 56.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 40 Time Step: 66 Cumulative Rewards: 67.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 41 Time Step: 62 Cumulative Rewards: 63.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 42 Time Step: 140 Cumulative Rewards: 141.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 43 Time Step: 177 Cumulative Rewards: 178.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 44 Time Step: 52 Cumulative Rewards: 53.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 45 Time Step: 81 Cumulative Rewards: 82.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 46 Time Step: 89 Cumulative Rewards: 90.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 47 Time Step: 144 Cumulative Rewards: 145.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 48 Time Step: 171 Cumulative Rewards: 172.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 49 Time Step: 152 Cumulative Rewards: 153.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 50 Time Step: 344 Cumulative Rewards: 345.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 51 Time Step: 185 Cumulative Rewards: 186.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 52 Time Step: 255 Cumulative Rewards: 256.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 53 Time Step: 310 Cumulative Rewards: 311.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 54 Time Step: 232 Cumulative Rewards: 233.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 55 Time Step: 215 Cumulative Rewards: 216.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 56 Time Step: 225 Cumulative Rewards: 226.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 57 Time Step: 215 Cumulative Rewards: 216.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 58 Time Step: 263 Cumulative Rewards: 264.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 59 Time Step: 241 Cumulative Rewards: 242.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 60 Time Step: 239 Cumulative Rewards: 240.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 61 Time Step: 232 Cumulative Rewards: 233.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 62 Time Step: 217 Cumulative Rewards: 218.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 63 Time Step: 232 Cumulative Rewards: 233.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 64 Time Step: 228 Cumulative Rewards: 229.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 65 Time Step: 237 Cumulative Rewards: 238.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 66 Time Step: 209 Cumulative Rewards: 210.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 67 Time Step: 229 Cumulative Rewards: 230.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 68 Time Step: 237 Cumulative Rewards: 238.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 69 Time Step: 266 Cumulative Rewards: 267.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 70 Time Step: 235 Cumulative Rewards: 236.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 71 Time Step: 212 Cumulative Rewards: 213.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 72 Time Step: 228 Cumulative Rewards: 229.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 73 Time Step: 208 Cumulative Rewards: 209.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 74 Time Step: 193 Cumulative Rewards: 194.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 75 Time Step: 240 Cumulative Rewards: 241.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 76 Time Step: 221 Cumulative Rewards: 222.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 77 Time Step: 240 Cumulative Rewards: 241.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 78 Time Step: 285 Cumulative Rewards: 286.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 79 Time Step: 187 Cumulative Rewards: 188.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 80 Time Step: 178 Cumulative Rewards: 179.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 81 Time Step: 191 Cumulative Rewards: 192.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 82 Time Step: 192 Cumulative Rewards: 193.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 83 Time Step: 168 Cumulative Rewards: 169.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 84 Time Step: 168 Cumulative Rewards: 169.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 85 Time Step: 155 Cumulative Rewards: 156.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 86 Time Step: 133 Cumulative Rewards: 134.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 87 Time Step: 243 Cumulative Rewards: 244.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 88 Time Step: 136 Cumulative Rewards: 137.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 89 Time Step: 142 Cumulative Rewards: 143.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 90 Time Step: 122 Cumulative Rewards: 123.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 91 Time Step: 222 Cumulative Rewards: 223.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 92 Time Step: 181 Cumulative Rewards: 182.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 93 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 94 Time Step: 113 Cumulative Rewards: 114.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 95 Time Step: 176 Cumulative Rewards: 177.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 96 Time Step: 22 Cumulative Rewards: 23.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 97 Time Step: 122 Cumulative Rewards: 123.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 98 Time Step: 195 Cumulative Rewards: 196.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 99 Time Step: 107 Cumulative Rewards: 108.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e802090af4c464998709f9dadc1af9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▂▃▆▅▄▅▄▄▄▄▄▄▄▅▄▃▃▃▄█▁▂</td></tr><tr><td>time_step</td><td>▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▂▃▆▅▄▅▄▄▄▄▄▄▄▅▄▃▃▃▄█▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>108.0</td></tr><tr><td>time_step</td><td>107</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">cartpole_linear_0.25_0.01_1</strong>: <a href=\"https://wandb.ai/irl_team7/DQfD/runs/1hofnwt4\" target=\"_blank\">https://wandb.ai/irl_team7/DQfD/runs/1hofnwt4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221210_173653-1hofnwt4/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA...\n",
            "Number of <state,action> pairs in the demonstrator data = 10000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221210_174301-17960kui</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/irl_team7/DQfD/runs/17960kui\" target=\"_blank\">cartpole_linear_0.25_0.005_1</a></strong> to <a href=\"https://wandb.ai/irl_team7/DQfD\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-training ...\n",
            "All pre-train finish.\n",
            "Annealing: linear, Demo Sampling Percent: 0.25\n",
            "Episode: 0 Time Step: 74 Cumulative Rewards: 75.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.245\n",
            "Episode: 1 Time Step: 31 Cumulative Rewards: 32.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.24\n",
            "Episode: 2 Time Step: 62 Cumulative Rewards: 63.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.235\n",
            "Episode: 3 Time Step: 67 Cumulative Rewards: 68.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.23\n",
            "Episode: 4 Time Step: 24 Cumulative Rewards: 25.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.225\n",
            "Episode: 5 Time Step: 34 Cumulative Rewards: 35.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.22\n",
            "Episode: 6 Time Step: 47 Cumulative Rewards: 48.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.215\n",
            "Episode: 7 Time Step: 39 Cumulative Rewards: 40.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.21\n",
            "Episode: 8 Time Step: 29 Cumulative Rewards: 30.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.20500000000000002\n",
            "Episode: 9 Time Step: 26 Cumulative Rewards: 27.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.2\n",
            "Episode: 10 Time Step: 48 Cumulative Rewards: 49.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.195\n",
            "Episode: 11 Time Step: 27 Cumulative Rewards: 28.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.19\n",
            "Episode: 12 Time Step: 44 Cumulative Rewards: 45.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.185\n",
            "Episode: 13 Time Step: 32 Cumulative Rewards: 33.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.18\n",
            "Episode: 14 Time Step: 59 Cumulative Rewards: 60.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.175\n",
            "Episode: 15 Time Step: 33 Cumulative Rewards: 34.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.16999999999999998\n",
            "Episode: 16 Time Step: 89 Cumulative Rewards: 90.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.16499999999999998\n",
            "Episode: 17 Time Step: 34 Cumulative Rewards: 35.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.16\n",
            "Episode: 18 Time Step: 44 Cumulative Rewards: 45.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.155\n",
            "Episode: 19 Time Step: 43 Cumulative Rewards: 44.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.15\n",
            "Episode: 20 Time Step: 41 Cumulative Rewards: 42.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.14500000000000002\n",
            "Episode: 21 Time Step: 46 Cumulative Rewards: 47.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.14\n",
            "Episode: 22 Time Step: 69 Cumulative Rewards: 70.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.135\n",
            "Episode: 23 Time Step: 86 Cumulative Rewards: 87.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.13\n",
            "Episode: 24 Time Step: 57 Cumulative Rewards: 58.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.125\n",
            "Episode: 25 Time Step: 45 Cumulative Rewards: 46.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.12\n",
            "Episode: 26 Time Step: 70 Cumulative Rewards: 71.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.11499999999999999\n",
            "Episode: 27 Time Step: 59 Cumulative Rewards: 60.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.10999999999999999\n",
            "Episode: 28 Time Step: 71 Cumulative Rewards: 72.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.10500000000000001\n",
            "Episode: 29 Time Step: 93 Cumulative Rewards: 94.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.1\n",
            "Episode: 30 Time Step: 199 Cumulative Rewards: 200.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.095\n",
            "Episode: 31 Time Step: 187 Cumulative Rewards: 188.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.09\n",
            "Episode: 32 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.08499999999999999\n",
            "Episode: 33 Time Step: 41 Cumulative Rewards: 42.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.07999999999999999\n",
            "Episode: 34 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.07499999999999998\n",
            "Episode: 35 Time Step: 444 Cumulative Rewards: 445.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.07\n",
            "Episode: 36 Time Step: 204 Cumulative Rewards: 205.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.065\n",
            "Episode: 37 Time Step: 208 Cumulative Rewards: 209.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.06\n",
            "Episode: 38 Time Step: 206 Cumulative Rewards: 207.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.05499999999999999\n",
            "Episode: 39 Time Step: 94 Cumulative Rewards: 95.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.04999999999999999\n",
            "Episode: 40 Time Step: 72 Cumulative Rewards: 73.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.044999999999999984\n",
            "Episode: 41 Time Step: 221 Cumulative Rewards: 222.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.04000000000000001\n",
            "Episode: 42 Time Step: 217 Cumulative Rewards: 218.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.035\n",
            "Episode: 43 Time Step: 187 Cumulative Rewards: 188.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.03\n",
            "Episode: 44 Time Step: 173 Cumulative Rewards: 174.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.024999999999999994\n",
            "Episode: 45 Time Step: 160 Cumulative Rewards: 161.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.01999999999999999\n",
            "Episode: 46 Time Step: 184 Cumulative Rewards: 185.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.014999999999999986\n",
            "Episode: 47 Time Step: 173 Cumulative Rewards: 174.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.010000000000000009\n",
            "Episode: 48 Time Step: 167 Cumulative Rewards: 168.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.0050000000000000044\n",
            "Episode: 49 Time Step: 199 Cumulative Rewards: 200.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 50 Time Step: 172 Cumulative Rewards: 173.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 51 Time Step: 209 Cumulative Rewards: 210.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 52 Time Step: 201 Cumulative Rewards: 202.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 53 Time Step: 177 Cumulative Rewards: 178.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 54 Time Step: 168 Cumulative Rewards: 169.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 55 Time Step: 98 Cumulative Rewards: 99.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 56 Time Step: 153 Cumulative Rewards: 154.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 57 Time Step: 172 Cumulative Rewards: 173.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 58 Time Step: 164 Cumulative Rewards: 165.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 59 Time Step: 199 Cumulative Rewards: 200.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 60 Time Step: 192 Cumulative Rewards: 193.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 61 Time Step: 164 Cumulative Rewards: 165.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 62 Time Step: 169 Cumulative Rewards: 170.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 63 Time Step: 186 Cumulative Rewards: 187.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 64 Time Step: 163 Cumulative Rewards: 164.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 65 Time Step: 143 Cumulative Rewards: 144.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 66 Time Step: 170 Cumulative Rewards: 171.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 67 Time Step: 188 Cumulative Rewards: 189.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 68 Time Step: 165 Cumulative Rewards: 166.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 69 Time Step: 40 Cumulative Rewards: 41.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 70 Time Step: 155 Cumulative Rewards: 156.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 71 Time Step: 168 Cumulative Rewards: 169.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 72 Time Step: 157 Cumulative Rewards: 158.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 73 Time Step: 177 Cumulative Rewards: 178.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 74 Time Step: 153 Cumulative Rewards: 154.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 75 Time Step: 168 Cumulative Rewards: 169.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 76 Time Step: 155 Cumulative Rewards: 156.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 77 Time Step: 152 Cumulative Rewards: 153.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 78 Time Step: 204 Cumulative Rewards: 205.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 79 Time Step: 169 Cumulative Rewards: 170.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 80 Time Step: 156 Cumulative Rewards: 157.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 81 Time Step: 156 Cumulative Rewards: 157.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 82 Time Step: 180 Cumulative Rewards: 181.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 83 Time Step: 57 Cumulative Rewards: 58.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 84 Time Step: 152 Cumulative Rewards: 153.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 85 Time Step: 158 Cumulative Rewards: 159.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 86 Time Step: 186 Cumulative Rewards: 187.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 87 Time Step: 157 Cumulative Rewards: 158.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 88 Time Step: 158 Cumulative Rewards: 159.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 89 Time Step: 155 Cumulative Rewards: 156.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 90 Time Step: 189 Cumulative Rewards: 190.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 91 Time Step: 190 Cumulative Rewards: 191.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 92 Time Step: 152 Cumulative Rewards: 153.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 93 Time Step: 163 Cumulative Rewards: 164.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 94 Time Step: 147 Cumulative Rewards: 148.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 95 Time Step: 172 Cumulative Rewards: 173.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 96 Time Step: 166 Cumulative Rewards: 167.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 97 Time Step: 136 Cumulative Rewards: 137.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 98 Time Step: 164 Cumulative Rewards: 165.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 99 Time Step: 162 Cumulative Rewards: 163.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b595e40c328c4b9394e564d1cdd59d26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>▂▁▁▁▁▁▁▁▁▂▁▁▄▁█▄▂▄▃▃▃▃▂▃▄▄▃▃▃▃▃▄▃▁▄▃▄▃▃▃</td></tr><tr><td>time_step</td><td>▂▁▁▁▁▁▁▁▁▂▁▁▄▁█▄▂▄▃▃▃▃▂▃▄▄▃▃▃▃▃▄▃▁▄▃▄▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>163.0</td></tr><tr><td>time_step</td><td>162</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">cartpole_linear_0.25_0.005_1</strong>: <a href=\"https://wandb.ai/irl_team7/DQfD/runs/17960kui\" target=\"_blank\">https://wandb.ai/irl_team7/DQfD/runs/17960kui</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221210_174301-17960kui/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA...\n",
            "Number of <state,action> pairs in the demonstrator data = 10000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221210_174941-2mgyqbwj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/irl_team7/DQfD/runs/2mgyqbwj\" target=\"_blank\">cartpole_linear_0.25_0.0025_1</a></strong> to <a href=\"https://wandb.ai/irl_team7/DQfD\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-training ...\n",
            "All pre-train finish.\n",
            "Annealing: linear, Demo Sampling Percent: 0.25\n",
            "Episode: 0 Time Step: 38 Cumulative Rewards: 39.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.2475\n",
            "Episode: 1 Time Step: 47 Cumulative Rewards: 48.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.245\n",
            "Episode: 2 Time Step: 42 Cumulative Rewards: 43.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.2425\n",
            "Episode: 3 Time Step: 52 Cumulative Rewards: 53.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.24\n",
            "Episode: 4 Time Step: 27 Cumulative Rewards: 28.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.2375\n",
            "Episode: 5 Time Step: 40 Cumulative Rewards: 41.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.235\n",
            "Episode: 6 Time Step: 36 Cumulative Rewards: 37.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.23249999999999998\n",
            "Episode: 7 Time Step: 25 Cumulative Rewards: 26.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.23\n",
            "Episode: 8 Time Step: 38 Cumulative Rewards: 39.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.2275\n",
            "Episode: 9 Time Step: 29 Cumulative Rewards: 30.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.225\n",
            "Episode: 10 Time Step: 24 Cumulative Rewards: 25.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.2225\n",
            "Episode: 11 Time Step: 19 Cumulative Rewards: 20.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.22\n",
            "Episode: 12 Time Step: 19 Cumulative Rewards: 20.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.2175\n",
            "Episode: 13 Time Step: 24 Cumulative Rewards: 25.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.215\n",
            "Episode: 14 Time Step: 19 Cumulative Rewards: 20.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.2125\n",
            "Episode: 15 Time Step: 27 Cumulative Rewards: 28.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.21\n",
            "Episode: 16 Time Step: 28 Cumulative Rewards: 29.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.2075\n",
            "Episode: 17 Time Step: 24 Cumulative Rewards: 25.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.20500000000000002\n",
            "Episode: 18 Time Step: 34 Cumulative Rewards: 35.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.2025\n",
            "Episode: 19 Time Step: 36 Cumulative Rewards: 37.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.2\n",
            "Episode: 20 Time Step: 35 Cumulative Rewards: 36.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.1975\n",
            "Episode: 21 Time Step: 27 Cumulative Rewards: 28.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.195\n",
            "Episode: 22 Time Step: 38 Cumulative Rewards: 39.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.1925\n",
            "Episode: 23 Time Step: 47 Cumulative Rewards: 48.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.19\n",
            "Episode: 24 Time Step: 46 Cumulative Rewards: 47.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.1875\n",
            "Episode: 25 Time Step: 31 Cumulative Rewards: 32.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.185\n",
            "Episode: 26 Time Step: 51 Cumulative Rewards: 52.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.1825\n",
            "Episode: 27 Time Step: 68 Cumulative Rewards: 69.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.18\n",
            "Episode: 28 Time Step: 87 Cumulative Rewards: 88.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.1775\n",
            "Episode: 29 Time Step: 76 Cumulative Rewards: 77.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.175\n",
            "Episode: 30 Time Step: 58 Cumulative Rewards: 59.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.1725\n",
            "Episode: 31 Time Step: 36 Cumulative Rewards: 37.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.16999999999999998\n",
            "Episode: 32 Time Step: 34 Cumulative Rewards: 35.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.16749999999999998\n",
            "Episode: 33 Time Step: 41 Cumulative Rewards: 42.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.16499999999999998\n",
            "Episode: 34 Time Step: 63 Cumulative Rewards: 64.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.16249999999999998\n",
            "Episode: 35 Time Step: 46 Cumulative Rewards: 47.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.16\n",
            "Episode: 36 Time Step: 64 Cumulative Rewards: 65.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.1575\n",
            "Episode: 37 Time Step: 51 Cumulative Rewards: 52.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.155\n",
            "Episode: 38 Time Step: 37 Cumulative Rewards: 38.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.1525\n",
            "Episode: 39 Time Step: 61 Cumulative Rewards: 62.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.15\n",
            "Episode: 40 Time Step: 82 Cumulative Rewards: 83.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.1475\n",
            "Episode: 41 Time Step: 114 Cumulative Rewards: 115.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.14500000000000002\n",
            "Episode: 42 Time Step: 78 Cumulative Rewards: 79.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.14250000000000002\n",
            "Episode: 43 Time Step: 39 Cumulative Rewards: 40.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.14\n",
            "Episode: 44 Time Step: 95 Cumulative Rewards: 96.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.1375\n",
            "Episode: 45 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.135\n",
            "Episode: 46 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.1325\n",
            "Episode: 47 Time Step: 346 Cumulative Rewards: 347.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.13\n",
            "Episode: 48 Time Step: 266 Cumulative Rewards: 267.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.1275\n",
            "Episode: 49 Time Step: 273 Cumulative Rewards: 274.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.125\n",
            "Episode: 50 Time Step: 232 Cumulative Rewards: 233.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.1225\n",
            "Episode: 51 Time Step: 223 Cumulative Rewards: 224.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.12\n",
            "Episode: 52 Time Step: 215 Cumulative Rewards: 216.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.1175\n",
            "Episode: 53 Time Step: 223 Cumulative Rewards: 224.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.11499999999999999\n",
            "Episode: 54 Time Step: 223 Cumulative Rewards: 224.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.11249999999999999\n",
            "Episode: 55 Time Step: 242 Cumulative Rewards: 243.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.10999999999999999\n",
            "Episode: 56 Time Step: 196 Cumulative Rewards: 197.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.10749999999999998\n",
            "Episode: 57 Time Step: 200 Cumulative Rewards: 201.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.10500000000000001\n",
            "Episode: 58 Time Step: 185 Cumulative Rewards: 186.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.10250000000000001\n",
            "Episode: 59 Time Step: 200 Cumulative Rewards: 201.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.1\n",
            "Episode: 60 Time Step: 186 Cumulative Rewards: 187.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.0975\n",
            "Episode: 61 Time Step: 198 Cumulative Rewards: 199.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.095\n",
            "Episode: 62 Time Step: 215 Cumulative Rewards: 216.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.0925\n",
            "Episode: 63 Time Step: 204 Cumulative Rewards: 205.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.09\n",
            "Episode: 64 Time Step: 196 Cumulative Rewards: 197.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.0875\n",
            "Episode: 65 Time Step: 208 Cumulative Rewards: 209.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.08499999999999999\n",
            "Episode: 66 Time Step: 193 Cumulative Rewards: 194.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.08249999999999999\n",
            "Episode: 67 Time Step: 165 Cumulative Rewards: 166.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.07999999999999999\n",
            "Episode: 68 Time Step: 202 Cumulative Rewards: 203.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.07749999999999999\n",
            "Episode: 69 Time Step: 191 Cumulative Rewards: 192.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.07499999999999998\n",
            "Episode: 70 Time Step: 171 Cumulative Rewards: 172.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.07250000000000001\n",
            "Episode: 71 Time Step: 199 Cumulative Rewards: 200.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.07\n",
            "Episode: 72 Time Step: 199 Cumulative Rewards: 200.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.0675\n",
            "Episode: 73 Time Step: 183 Cumulative Rewards: 184.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.065\n",
            "Episode: 74 Time Step: 180 Cumulative Rewards: 181.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.0625\n",
            "Episode: 75 Time Step: 191 Cumulative Rewards: 192.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.06\n",
            "Episode: 76 Time Step: 198 Cumulative Rewards: 199.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.057499999999999996\n",
            "Episode: 77 Time Step: 217 Cumulative Rewards: 218.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.05499999999999999\n",
            "Episode: 78 Time Step: 194 Cumulative Rewards: 195.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.05249999999999999\n",
            "Episode: 79 Time Step: 179 Cumulative Rewards: 180.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.04999999999999999\n",
            "Episode: 80 Time Step: 187 Cumulative Rewards: 188.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.04749999999999999\n",
            "Episode: 81 Time Step: 198 Cumulative Rewards: 199.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.044999999999999984\n",
            "Episode: 82 Time Step: 208 Cumulative Rewards: 209.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.04249999999999998\n",
            "Episode: 83 Time Step: 218 Cumulative Rewards: 219.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.04000000000000001\n",
            "Episode: 84 Time Step: 208 Cumulative Rewards: 209.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.037500000000000006\n",
            "Episode: 85 Time Step: 192 Cumulative Rewards: 193.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.035\n",
            "Episode: 86 Time Step: 204 Cumulative Rewards: 205.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.0325\n",
            "Episode: 87 Time Step: 191 Cumulative Rewards: 192.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.03\n",
            "Episode: 88 Time Step: 172 Cumulative Rewards: 173.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.027499999999999997\n",
            "Episode: 89 Time Step: 202 Cumulative Rewards: 203.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.024999999999999994\n",
            "Episode: 90 Time Step: 206 Cumulative Rewards: 207.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.022499999999999992\n",
            "Episode: 91 Time Step: 232 Cumulative Rewards: 233.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.01999999999999999\n",
            "Episode: 92 Time Step: 278 Cumulative Rewards: 279.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.017499999999999988\n",
            "Episode: 93 Time Step: 204 Cumulative Rewards: 205.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.014999999999999986\n",
            "Episode: 94 Time Step: 218 Cumulative Rewards: 219.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.012499999999999983\n",
            "Episode: 95 Time Step: 295 Cumulative Rewards: 296.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.010000000000000009\n",
            "Episode: 96 Time Step: 216 Cumulative Rewards: 217.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.007500000000000007\n",
            "Episode: 97 Time Step: 182 Cumulative Rewards: 183.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.0050000000000000044\n",
            "Episode: 98 Time Step: 217 Cumulative Rewards: 218.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.0025000000000000022\n",
            "Episode: 99 Time Step: 214 Cumulative Rewards: 215.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▂▁█▅▄▄▄▃▃▄▄▄▄▃▄▄▄▄▄▃▄▄▄▄</td></tr><tr><td>time_step</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▂▁█▅▄▄▄▃▃▄▄▄▄▃▄▄▄▄▄▃▄▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>rewards</td><td>215.0</td></tr><tr><td>time_step</td><td>214</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">cartpole_linear_0.25_0.0025_1</strong>: <a href=\"https://wandb.ai/irl_team7/DQfD/runs/2mgyqbwj\" target=\"_blank\">https://wandb.ai/irl_team7/DQfD/runs/2mgyqbwj</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221210_174941-2mgyqbwj/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA...\n",
            "Number of <state,action> pairs in the demonstrator data = 10000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221210_175641-24h81qs7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/irl_team7/DQfD/runs/24h81qs7\" target=\"_blank\">cartpole_linear_0.5_0.01_1</a></strong> to <a href=\"https://wandb.ai/irl_team7/DQfD\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-training ...\n",
            "All pre-train finish.\n",
            "Annealing: linear, Demo Sampling Percent: 0.5\n",
            "Episode: 0 Time Step: 56 Cumulative Rewards: 57.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.49\n",
            "Episode: 1 Time Step: 27 Cumulative Rewards: 28.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.48\n",
            "Episode: 2 Time Step: 61 Cumulative Rewards: 62.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.47\n",
            "Episode: 3 Time Step: 35 Cumulative Rewards: 36.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.46\n",
            "Episode: 4 Time Step: 50 Cumulative Rewards: 51.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.45\n",
            "Episode: 5 Time Step: 24 Cumulative Rewards: 25.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.44\n",
            "Episode: 6 Time Step: 27 Cumulative Rewards: 28.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.43\n",
            "Episode: 7 Time Step: 23 Cumulative Rewards: 24.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.42\n",
            "Episode: 8 Time Step: 23 Cumulative Rewards: 24.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.41000000000000003\n",
            "Episode: 9 Time Step: 25 Cumulative Rewards: 26.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.4\n",
            "Episode: 10 Time Step: 23 Cumulative Rewards: 24.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.39\n",
            "Episode: 11 Time Step: 63 Cumulative Rewards: 64.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.38\n",
            "Episode: 12 Time Step: 35 Cumulative Rewards: 36.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.37\n",
            "Episode: 13 Time Step: 46 Cumulative Rewards: 47.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.36\n",
            "Episode: 14 Time Step: 32 Cumulative Rewards: 33.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.35\n",
            "Episode: 15 Time Step: 28 Cumulative Rewards: 29.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.33999999999999997\n",
            "Episode: 16 Time Step: 39 Cumulative Rewards: 40.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.32999999999999996\n",
            "Episode: 17 Time Step: 52 Cumulative Rewards: 53.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.32\n",
            "Episode: 18 Time Step: 52 Cumulative Rewards: 53.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.31\n",
            "Episode: 19 Time Step: 37 Cumulative Rewards: 38.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.3\n",
            "Episode: 20 Time Step: 56 Cumulative Rewards: 57.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.29000000000000004\n",
            "Episode: 21 Time Step: 33 Cumulative Rewards: 34.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.28\n",
            "Episode: 22 Time Step: 37 Cumulative Rewards: 38.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.27\n",
            "Episode: 23 Time Step: 61 Cumulative Rewards: 62.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.26\n",
            "Episode: 24 Time Step: 43 Cumulative Rewards: 44.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.25\n",
            "Episode: 25 Time Step: 62 Cumulative Rewards: 63.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.24\n",
            "Episode: 26 Time Step: 46 Cumulative Rewards: 47.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.22999999999999998\n",
            "Episode: 27 Time Step: 104 Cumulative Rewards: 105.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.21999999999999997\n",
            "Episode: 28 Time Step: 69 Cumulative Rewards: 70.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.21000000000000002\n",
            "Episode: 29 Time Step: 98 Cumulative Rewards: 99.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.2\n",
            "Episode: 30 Time Step: 64 Cumulative Rewards: 65.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.19\n",
            "Episode: 31 Time Step: 80 Cumulative Rewards: 81.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.18\n",
            "Episode: 32 Time Step: 52 Cumulative Rewards: 53.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.16999999999999998\n",
            "Episode: 33 Time Step: 69 Cumulative Rewards: 70.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.15999999999999998\n",
            "Episode: 34 Time Step: 64 Cumulative Rewards: 65.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.14999999999999997\n",
            "Episode: 35 Time Step: 149 Cumulative Rewards: 150.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.14\n",
            "Episode: 36 Time Step: 90 Cumulative Rewards: 91.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.13\n",
            "Episode: 37 Time Step: 86 Cumulative Rewards: 87.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.12\n",
            "Episode: 38 Time Step: 42 Cumulative Rewards: 43.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.10999999999999999\n",
            "Episode: 39 Time Step: 192 Cumulative Rewards: 193.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.09999999999999998\n",
            "Episode: 40 Time Step: 241 Cumulative Rewards: 242.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.08999999999999997\n",
            "Episode: 41 Time Step: 182 Cumulative Rewards: 183.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.08000000000000002\n",
            "Episode: 42 Time Step: 276 Cumulative Rewards: 277.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.07\n",
            "Episode: 43 Time Step: 115 Cumulative Rewards: 116.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.06\n",
            "Episode: 44 Time Step: 130 Cumulative Rewards: 131.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.04999999999999999\n",
            "Episode: 45 Time Step: 194 Cumulative Rewards: 195.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.03999999999999998\n",
            "Episode: 46 Time Step: 247 Cumulative Rewards: 248.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.02999999999999997\n",
            "Episode: 47 Time Step: 175 Cumulative Rewards: 176.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.020000000000000018\n",
            "Episode: 48 Time Step: 220 Cumulative Rewards: 221.0\n",
            "Annealing: linear, Demo Sampling Percent: 0.010000000000000009\n",
            "Episode: 49 Time Step: 492 Cumulative Rewards: 493.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 50 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 51 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 52 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 53 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 54 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 55 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 56 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 57 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 58 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 59 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 60 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 61 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 62 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 63 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 64 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 65 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 66 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 67 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 68 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 69 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n",
            "Episode: 70 Time Step: 499 Cumulative Rewards: 500.0\n",
            "Annealing: linear, Demo Sampling Percent: 0\n"
          ]
        }
      ]
    }
  ]
}